1
00:00:09,784 --> 00:00:11,867
- Okay let's get started.

2
00:00:13,038 --> 00:00:15,888
Alright, so welcome to lecture 14,

3
00:00:15,888 --> 00:00:20,884
and today we'll be talking
about reinforcement learning.

4
00:00:20,884 --> 00:00:23,222
So some administrative details first,

5
00:00:23,222 --> 00:00:30,346
update on grades. Midterm grades were released last night, so
see Piazza for more information and statistics about that.

6
00:00:30,346 --> 00:00:35,402
And we also have A2 and milestone grades
scheduled for later this week.

7
00:00:36,768 --> 00:00:40,682
Also, about your projects, all teams
must register your projects.

8
00:00:40,682 --> 00:00:47,580
So on Piazza we have a form posted, so you should go there and this is
required, every team should go and fill out this form with information

9
00:00:47,580 --> 00:00:53,214
about your project, that we'll use for
final grading and the poster session.

10
00:00:53,214 --> 00:01:01,779
And the Tiny ImageNet evaluation servers are also now online
for those of you who are doing the Tiny ImageNet challenge.

11
00:01:01,779 --> 00:01:06,193
We also have a link to a course survey on
Piazza that was released a few days ago,

12
00:01:06,193 --> 00:01:13,600
so, please fill it out if you guys haven't already. We'd love
to have your feedback and know how we can improve this class.

13
00:01:16,589 --> 00:01:19,650
Okay, so the topic of today,
reinforcement learning.

14
00:01:19,650 --> 00:01:22,544
Alright, so so far we've talked
about supervised learning,

15
00:01:22,544 --> 00:01:30,498
which is about a type of problem where we have data x and then we have
labels y and our goal is to learn a function that is mapping from x to y.

16
00:01:30,498 --> 00:01:35,067
So, for example, the classification
problem that we've been working with.

17
00:01:35,067 --> 00:01:37,753
We also talked last lecture
about unsupervised learning,

18
00:01:37,753 --> 00:01:45,362
which is the problem where we have just data and no labels, and our
goal is to learn some underlying, hidden structure of the data.

19
00:01:45,362 --> 00:01:50,528
So, an example of this is the generative
models that we talked about last lecture.

20
00:01:52,040 --> 00:01:57,370
And so today we're going to talk about a different kind
of problem set-up, the reinforcement learning problem.

21
00:01:57,370 --> 00:02:01,824
And so here we have an agent
that can take actions in its environment,

22
00:02:01,824 --> 00:02:04,352
and it can receive rewards
for for its action.

23
00:02:04,352 --> 00:02:09,959
And its goal is going to be to learn how to take
actions in a way that can maximize its reward.

24
00:02:09,959 --> 00:02:14,101
And so we'll talk about this
in a lot more detail today.

25
00:02:14,101 --> 00:02:16,020
So, the outline for today,
we're going to first

26
00:02:16,020 --> 00:02:18,116
talk about the reinforcement
learning problem,

27
00:02:18,116 --> 00:02:20,927
and then we'll talk about
Markov decision processes,

28
00:02:20,927 --> 00:02:24,747
which is a formalism of the
reinforcement learning problem,

29
00:02:24,747 --> 00:02:31,095
and then we'll talk about two major classes of
RL algorithms, Q-learning and policy gradients.

30
00:02:32,876 --> 00:02:38,936
So, in the reinforcement learning set up, what we
have is we have an agent and we have an environment.

31
00:02:38,936 --> 00:02:43,268
And so the environment
gives the agent a state.

32
00:02:43,268 --> 00:02:46,877
In turn, the agent is
going to take an action,

33
00:02:46,877 --> 00:02:52,609
and then the environment is going to give
back a reward, as well as the next state.

34
00:02:52,609 --> 00:03:00,918
And so this is going to keep going on in this loop, on and on, until the
environment gives back a terminal state, which then ends the episode.

35
00:03:00,918 --> 00:03:03,401
So, let's see some examples of this.

36
00:03:03,401 --> 00:03:05,536
First we have here the cart-pole problem,

37
00:03:05,536 --> 00:03:11,142
which is a classic problem that some of you
may have seen, in, for example, 229 before.

38
00:03:11,142 --> 00:03:16,252
And so this objective here is that you want
to balance a pole on top of a movable cart.

39
00:03:16,252 --> 00:03:20,280
Alright, so the state that you have here
is your current description of the system.

40
00:03:20,280 --> 00:03:28,206
So, for example, angular, angular speed of your pole,
your position, and the horizontal velocity of your cart.

41
00:03:28,206 --> 00:03:33,224
And the actions you can take are horizontal
forces that you apply onto the cart, right?

42
00:03:33,224 --> 00:03:38,387
So you're basically trying to move this cart
around to try and balance this pole on top of it.

43
00:03:38,387 --> 00:03:43,990
And the reward that you're getting from this environment
is one at each time step if your pole is upright.

44
00:03:43,990 --> 00:03:48,143
So you basically want to keep this pole
balanced for as long as you can.

45
00:03:49,286 --> 00:03:52,192
Okay, so here's another example
of a classic RL problem.

46
00:03:52,192 --> 00:03:53,998
Here is robot locomotion.

47
00:03:53,998 --> 00:03:59,670
So we have here an example of a humanoid
robot, as well as an ant robot model.

48
00:03:59,670 --> 00:04:03,128
And our objective here is to
make the robot move forward.

49
00:04:03,128 --> 00:04:10,807
And so the state that we have describing our system is the
angle and the positions of all the joints of our robots.

50
00:04:10,807 --> 00:04:15,887
And then the actions that we can take are
the torques applied onto these joints,

51
00:04:15,887 --> 00:04:21,228
right, and so these are trying to make the robot
move forward and then the reward that we get is

52
00:04:21,228 --> 00:04:31,701
our forward movement as well as, I think, in the time of, in the case of the humanoid, also,
you can have something like a reward of one for each time step that this robot is upright.

53
00:04:33,521 --> 00:04:38,384
So, games are also a big class of
problems that can be formulated with RL.

54
00:04:38,384 --> 00:04:40,700
So, for example, here we have Atari games

55
00:04:40,700 --> 00:04:44,280
which are a classic success
of deep reinforcement learning

56
00:04:44,280 --> 00:04:48,574
and so here the objective is to complete these
games with the highest possible score, right.

57
00:04:48,574 --> 00:04:52,753
So, your agent is basically a player
that's trying to play these games.

58
00:04:52,753 --> 00:04:57,506
And the state that you have is going to be
the raw pixels of the game state.

59
00:04:57,506 --> 00:05:02,882
Right, so these are just the pixels on the screen
that you would see as you're playing the game.

60
00:05:02,882 --> 00:05:09,912
And then the actions that you have are your game controls, so for
example, in some games maybe moving left to right, up or down.

61
00:05:09,912 --> 00:05:12,534
And then the score that you
have is your score increase

62
00:05:12,534 --> 00:05:15,667
or decrease at each time step,
and your goal is going to be

63
00:05:15,667 --> 00:05:19,834
to maximize your total score
over the course of the game.

64
00:05:21,312 --> 00:05:24,179
And, finally, here we have
another example of a game here.

65
00:05:24,179 --> 00:05:25,587
It's

66
00:05:25,587 --> 00:05:26,587
Go, which is

67
00:05:27,573 --> 00:05:28,893
something that was a

68
00:05:28,893 --> 00:05:31,697
huge achievement of deep
reinforcement learning last year,

69
00:05:31,697 --> 00:05:34,721
when Deep Minds AlphaGo beats Lee Sedol,

70
00:05:34,721 --> 00:05:36,867
which is one of the

71
00:05:36,867 --> 00:05:38,589
best Go players of the last few years,

72
00:05:38,589 --> 00:05:41,685
and this is actually in the news again

73
00:05:41,685 --> 00:05:45,667
for, as some of you may have
seen, there's another Go

74
00:05:45,667 --> 00:05:47,529
competition going on now with

75
00:05:47,529 --> 00:05:50,919
AlphaGo versus a top-ranked Go player.

76
00:05:50,919 --> 00:05:53,495
And so the objective here is to

77
00:05:53,495 --> 00:05:56,295
win the game, and our
state is the position

78
00:05:56,295 --> 00:05:58,349
of all the pieces, the action
is where to put the next

79
00:05:58,349 --> 00:06:02,062
piece down, and the reward
is, one, if you win at the end

80
00:06:02,062 --> 00:06:03,912
of the game, and zero otherwise.

81
00:06:03,912 --> 00:06:05,032
And we'll also talk about this one

82
00:06:05,032 --> 00:06:08,411
in a little bit more detail, later.

83
00:06:08,411 --> 00:06:09,891
Okay, so

84
00:06:09,891 --> 00:06:12,046
how can we mathematically formalize

85
00:06:12,046 --> 00:06:13,330
the RL problem, right?

86
00:06:13,330 --> 00:06:15,817
This loop that we talked about earlier,

87
00:06:15,817 --> 00:06:18,051
of environments giving agents states,

88
00:06:18,051 --> 00:06:20,634
and then agents taking actions.

89
00:06:22,394 --> 00:06:24,884
So, a Markov decision process is

90
00:06:24,884 --> 00:06:28,512
the mathematical formulation
of the RL problem,

91
00:06:28,512 --> 00:06:31,447
and an MDP satisfies the Markov property,

92
00:06:31,447 --> 00:06:33,054
which is that the current state completely

93
00:06:33,054 --> 00:06:36,107
characterizes the state of the world.

94
00:06:36,107 --> 00:06:40,164
And an MDP here is defined
by tuple of objects,

95
00:06:40,164 --> 00:06:43,170
consisting of S, which is
the set of possible states.

96
00:06:43,170 --> 00:06:45,762
We have A, our set of possible actions,

97
00:06:45,762 --> 00:06:50,018
we also have R, our
distribution of our reward,

98
00:06:50,018 --> 00:06:51,694
given a state, action pair,

99
00:06:51,694 --> 00:06:53,824
so it's a function
mapping from state action

100
00:06:53,824 --> 00:06:55,323
to your reward.

101
00:06:55,323 --> 00:06:57,430
You also have P, which is
a transition probability

102
00:06:57,430 --> 00:07:00,079
distribution over your
next state, that you're

103
00:07:00,079 --> 00:07:02,940
going to transition to given
your state, action pair.

104
00:07:02,940 --> 00:07:05,718
And then finally we have a
Gamma, a discount factor,

105
00:07:05,718 --> 00:07:09,720
which is basically
saying how much we value

106
00:07:09,720 --> 00:07:12,970
rewards coming up soon versus later on.

107
00:07:14,203 --> 00:07:17,395
So, the way the Markov
Decision Process works is that

108
00:07:17,395 --> 00:07:20,053
at our initial time step t equals zero,

109
00:07:20,053 --> 00:07:21,523
the environment is going to sample some

110
00:07:21,523 --> 00:07:24,615
initial state as zero, from
the initial state distribution,

111
00:07:24,615 --> 00:07:26,363
p of s zero.

112
00:07:26,363 --> 00:07:29,271
And then, once it has that,
then from time t equals zero

113
00:07:29,271 --> 00:07:32,253
until it's done, we're going
to iterate through this loop

114
00:07:32,253 --> 00:07:35,797
where the agent is going to
select an action, a sub t.

115
00:07:35,797 --> 00:07:38,885
The environment is going to
sample a reward from here,

116
00:07:38,885 --> 00:07:41,907
so reward given your state and the

117
00:07:41,907 --> 00:07:44,032
action that you just took.

118
00:07:44,032 --> 00:07:47,640
It's also going to sample the next state,

119
00:07:47,640 --> 00:07:51,534
at time t plus one, given
your probability distribution

120
00:07:51,534 --> 00:07:54,467
and then the agent is going to receive

121
00:07:54,467 --> 00:07:56,790
the reward, as well as the
next state, and then we're

122
00:07:56,790 --> 00:07:58,707
going to through this process again,

123
00:07:58,707 --> 00:08:01,769
and keep looping; agent
will select the next action,

124
00:08:01,769 --> 00:08:05,542
and so on until the episode is over.

125
00:08:05,542 --> 00:08:06,989
Okay, so

126
00:08:06,989 --> 00:08:10,724
now based on this, we
can define a policy pi,

127
00:08:10,724 --> 00:08:13,593
which is a function from
your states to your actions

128
00:08:13,593 --> 00:08:16,651
that specifies what action
to take in each state.

129
00:08:16,651 --> 00:08:19,748
And this can be either
deterministic or stochastic.

130
00:08:19,748 --> 00:08:22,447
And our objective now is
to going to be to find

131
00:08:22,447 --> 00:08:24,727
your optimal policy pi
star, that maximizes your

132
00:08:24,727 --> 00:08:27,205
cumulative discounted reward.

133
00:08:27,205 --> 00:08:29,059
So we can see here we have our

134
00:08:29,059 --> 00:08:31,813
some of our future
rewards, which can be also

135
00:08:31,813 --> 00:08:35,509
discounted by your discount factor.

136
00:08:35,509 --> 00:08:39,327
So, let's look at an
example of a simple MDP.

137
00:08:39,327 --> 00:08:42,034
And here we have Grid World, which is this

138
00:08:42,034 --> 00:08:44,533
task where we have this grid of states.

139
00:08:44,533 --> 00:08:46,950
So you can be in any of these

140
00:08:48,112 --> 00:08:50,295
cells of your grid, which are your states.

141
00:08:50,295 --> 00:08:52,613
And you can take actions from your states,

142
00:08:52,613 --> 00:08:54,713
and so these actions are going to be

143
00:08:54,713 --> 00:08:56,527
simple movements, moving to your right,

144
00:08:56,527 --> 00:08:59,299
to your left, up or down.

145
00:08:59,299 --> 00:09:02,683
And you're going to get a
negative reward for each

146
00:09:02,683 --> 00:09:07,163
transition or each time step,
basically, that happens.

147
00:09:07,163 --> 00:09:08,859
Each movement that you take,

148
00:09:08,859 --> 00:09:11,989
and this can be something
like R equals negative one.

149
00:09:11,989 --> 00:09:13,871
And so your objective is going to be

150
00:09:13,871 --> 00:09:15,588
to reach one of the terminal states,

151
00:09:15,588 --> 00:09:17,793
which are the gray states shown here,

152
00:09:17,793 --> 00:09:20,055
in the least number of actions.

153
00:09:20,055 --> 00:09:22,249
Right, so the longer
that you take to reach

154
00:09:22,249 --> 00:09:23,522
your terminal state, you're going to keep

155
00:09:23,522 --> 00:09:26,522
accumulating these negative rewards.

156
00:09:27,625 --> 00:09:30,540
Okay, so if you look at
a random policy here,

157
00:09:30,540 --> 00:09:33,141
a random policy would
consist of, basically,

158
00:09:33,141 --> 00:09:35,305
at any given state or cell that you're in

159
00:09:35,305 --> 00:09:37,770
just sampling randomly which direction

160
00:09:37,770 --> 00:09:39,090
that you're going to move in next.

161
00:09:39,090 --> 00:09:41,843
Right, so all of these
have equal probability.

162
00:09:41,843 --> 00:09:44,115
On the other hand, an optimal policy that

163
00:09:44,115 --> 00:09:46,518
we would like to have is

164
00:09:46,518 --> 00:09:48,672
basically taking the action, the direction

165
00:09:48,672 --> 00:09:51,866
that will move us closest
to a terminal state.

166
00:09:51,866 --> 00:09:53,164
So you can see here,

167
00:09:53,164 --> 00:09:54,808
if we're right next to one of the

168
00:09:54,808 --> 00:09:56,156
terminal states we should

169
00:09:56,156 --> 00:09:57,506
always move in the direction

170
00:09:57,506 --> 00:09:59,171
that gets us to this terminal state.

171
00:09:59,171 --> 00:10:01,385
And otherwise, if you're in
one of these other states,

172
00:10:01,385 --> 00:10:03,822
you want to take the
direction that will take you

173
00:10:03,822 --> 00:10:06,405
closest to one of these states.

174
00:10:09,119 --> 00:10:11,644
Okay, so now given this

175
00:10:11,644 --> 00:10:13,745
description of our MDP, what we want to do

176
00:10:13,745 --> 00:10:17,155
is we want to find our
optimal policy pi star.

177
00:10:17,155 --> 00:10:20,755
Right, our policy that's
maximizing the sum of the rewards.

178
00:10:20,755 --> 00:10:22,955
And so this optimal policy
is going to tell us,

179
00:10:22,955 --> 00:10:25,655
given any state that we're
in, what is the action that

180
00:10:25,655 --> 00:10:27,851
we should take in order
to maximize the sum

181
00:10:27,851 --> 00:10:29,731
of the rewards that we'll get.

182
00:10:29,731 --> 00:10:32,011
And so one question is how do we

183
00:10:32,011 --> 00:10:34,091
handle the randomness in the MDP, right?

184
00:10:34,091 --> 00:10:36,459
We have randomness in

185
00:10:36,459 --> 00:10:39,073
terms of our initial
state that we're sampling,

186
00:10:39,073 --> 00:10:40,727
in therms of this transition probability

187
00:10:40,727 --> 00:10:42,303
distribution that will give us

188
00:10:42,303 --> 00:10:46,341
distribution of our
next states, and so on.

189
00:10:46,341 --> 00:10:49,292
Also what we'll do is we'll
work, then, with maximizing

190
00:10:49,292 --> 00:10:51,947
our expected sum of the rewards.

191
00:10:51,947 --> 00:10:55,451
So, formally, we can write
our optimal policy pi star

192
00:10:55,451 --> 00:10:59,129
as maximizing this expected
sum of future rewards

193
00:10:59,129 --> 00:11:02,957
over policy's pi, where
we have our initial state

194
00:11:02,957 --> 00:11:05,103
sampled from our state distribution.

195
00:11:05,103 --> 00:11:07,388
We have our actions,

196
00:11:07,388 --> 00:11:09,127
sampled from our policy, given the state.

197
00:11:09,127 --> 00:11:11,929
And then we have our next states sampled

198
00:11:11,929 --> 00:11:16,423
from our transition
probability distributions.

199
00:11:16,423 --> 00:11:17,256
Okay, so

200
00:11:18,351 --> 00:11:19,668
before we talk about

201
00:11:19,668 --> 00:11:22,143
exactly how we're going
to find this policy,

202
00:11:22,143 --> 00:11:23,909
let's first talk about a few definitions

203
00:11:23,909 --> 00:11:26,787
that's going to be helpful
for us in doing so.

204
00:11:26,787 --> 00:11:29,115
So, specifically, the value function

205
00:11:29,115 --> 00:11:31,405
and the Q-value function.

206
00:11:31,405 --> 00:11:33,647
So, as we follow the policy,

207
00:11:33,647 --> 00:11:35,489
we're going to sample trajectories

208
00:11:35,489 --> 00:11:37,426
or paths, right, for every episode.

209
00:11:37,426 --> 00:11:40,287
And we're going to have
our initial state as zero,

210
00:11:40,287 --> 00:11:43,611
a-zero, r-zero, s-one,
a-one, r-one, and so on.

211
00:11:43,611 --> 00:11:44,905
We're going to have this trajectory

212
00:11:44,905 --> 00:11:49,331
of states, actions, and
rewards that we get.

213
00:11:49,331 --> 00:11:52,613
And so, how good is a state
that we're currently in?

214
00:11:52,613 --> 00:11:55,985
Well, the value function at any state s,

215
00:11:55,985 --> 00:11:58,513
is the expected cumulative reward

216
00:11:58,513 --> 00:12:01,770
following the policy from
state s, from here on out.

217
00:12:01,770 --> 00:12:05,258
Right, so it's going to be expected value

218
00:12:05,258 --> 00:12:07,635
of our expected cumulative reward,

219
00:12:07,635 --> 00:12:10,800
starting from our current state.

220
00:12:10,800 --> 00:12:13,286
And then how good is a state, action pair?

221
00:12:13,286 --> 00:12:17,370
So how good is taking action a in state s?

222
00:12:17,370 --> 00:12:20,468
And we define this using
a Q-value function,

223
00:12:20,468 --> 00:12:23,574
which is, the expected
cumulative reward from taking

224
00:12:23,574 --> 00:12:27,741
action a in state s and
then following the policy.

225
00:12:29,708 --> 00:12:32,708
Right, so then, the
optimal Q-value function

226
00:12:32,708 --> 00:12:36,404
that we can get is going to be
Q star, which is the maximum

227
00:12:36,404 --> 00:12:39,216
expected cumulative reward that we can get

228
00:12:39,216 --> 00:12:43,383
from a given state action
pair, defined here.

229
00:12:45,099 --> 00:12:48,592
So now we're going to
see one important thing

230
00:12:48,592 --> 00:12:50,018
in reinforcement learning,

231
00:12:50,018 --> 00:12:52,018
which is called the Bellman equation.

232
00:12:52,018 --> 00:12:54,485
So let's consider this a Q-value function

233
00:12:54,485 --> 00:12:57,697
from the optimal policy Q star,

234
00:12:57,697 --> 00:13:00,911
which is then going to
satisfy this Bellman equation,

235
00:13:00,911 --> 00:13:03,533
which is this identity shown here,

236
00:13:03,533 --> 00:13:05,194
and what this means is that

237
00:13:05,194 --> 00:13:08,873
given any state, action pair, s and a,

238
00:13:08,873 --> 00:13:11,748
the value of this pair
is going to be the reward

239
00:13:11,748 --> 00:13:15,092
that you're going to get, r,
plus the value of whatever

240
00:13:15,092 --> 00:13:16,517
state that you end up in.

241
00:13:16,517 --> 00:13:18,868
So, let's say, s prime.

242
00:13:18,868 --> 00:13:22,319
And since we know that we
have the optimal policy,

243
00:13:22,319 --> 00:13:24,150
then we also know that we're going to

244
00:13:24,150 --> 00:13:26,202
play the best action that we can,

245
00:13:26,202 --> 00:13:28,746
right, at our state s prime.

246
00:13:28,746 --> 00:13:31,413
And so then, the value at state s prime

247
00:13:31,413 --> 00:13:34,432
is just going to be the
maximum over our actions,

248
00:13:34,432 --> 00:13:38,626
a prime, of Q star at s prime, a prime.

249
00:13:38,626 --> 00:13:41,325
And so then we get this

250
00:13:41,325 --> 00:13:44,119
identity here, for optimal Q-value.

251
00:13:44,119 --> 00:13:46,753
Right, and then also, as always, we have

252
00:13:46,753 --> 00:13:48,075
this expectation here,

253
00:13:48,075 --> 00:13:49,880
because we have randomness over what state

254
00:13:49,880 --> 00:13:52,380
that we're going to end up in.

255
00:13:54,252 --> 00:13:56,782
And then we can also
infer, from here, that our

256
00:13:56,782 --> 00:13:58,928
optimal policy, right, is going to consist

257
00:13:58,928 --> 00:14:00,860
of taking the best action in any state,

258
00:14:00,860 --> 00:14:02,488
as specified by Q star.

259
00:14:02,488 --> 00:14:04,295
Q star is going to tell us

260
00:14:04,295 --> 00:14:05,462
of the maximum

261
00:14:06,540 --> 00:14:08,437
future reward that we can
get from any of our actions,

262
00:14:08,437 --> 00:14:09,456
so we should just

263
00:14:09,456 --> 00:14:11,356
take a policy that's following this

264
00:14:11,356 --> 00:14:13,615
and just taking the action that's

265
00:14:13,615 --> 00:14:16,863
going to lead to best reward.

266
00:14:16,863 --> 00:14:21,025
Okay, so how can we solve
for this optimal policy?

267
00:14:21,025 --> 00:14:23,381
So, one way we can solve for this is

268
00:14:23,381 --> 00:14:25,692
something called a value
iteration algorithm,

269
00:14:25,692 --> 00:14:28,046
where we're going to use
this Bellman equation

270
00:14:28,046 --> 00:14:29,527
as an iterative update.

271
00:14:29,527 --> 00:14:33,830
So at each step, we're going
to refine our approximation

272
00:14:33,830 --> 00:14:37,997
of Q star by trying to
enforce the Bellman equation.

273
00:14:39,347 --> 00:14:42,602
And so, under some
mathematical conditions,

274
00:14:42,602 --> 00:14:45,602
we also know that this sequence Q, i

275
00:14:47,008 --> 00:14:49,569
of our Q-function is going
to converge to our optimal

276
00:14:49,569 --> 00:14:52,236
Q star as i approaches infinity.

277
00:14:54,257 --> 00:14:55,579
And so this, this works well,

278
00:14:55,579 --> 00:14:58,329
but what's the problem with this?

279
00:14:59,184 --> 00:15:01,887
Well, an important problem
is that this is not scalable.

280
00:15:01,887 --> 00:15:02,720
Right?

281
00:15:02,720 --> 00:15:03,553
We have to compute

282
00:15:03,553 --> 00:15:05,793
Q of s, a here for
every state, action pair

283
00:15:05,793 --> 00:15:08,597
in order to make our iterative updates.

284
00:15:08,597 --> 00:15:10,382
Right, but then this is a problem if,

285
00:15:10,382 --> 00:15:13,049
for example, if we look at these

286
00:15:14,021 --> 00:15:15,865
the state of, for example, an Atari game

287
00:15:15,865 --> 00:15:17,519
that we had earlier, it's going to be

288
00:15:17,519 --> 00:15:18,933
your screen of pixels.

289
00:15:18,933 --> 00:15:22,229
And this is a huge state
space, and it's basically

290
00:15:22,229 --> 00:15:23,865
computationally infeasible

291
00:15:23,865 --> 00:15:27,448
to compute this for
the entire state space.

292
00:15:28,725 --> 00:15:31,200
Okay, so what's the solution to this?

293
00:15:31,200 --> 00:15:33,141
Well, we can use a function approximator

294
00:15:33,141 --> 00:15:35,908
to estimate Q of s, a

295
00:15:35,908 --> 00:15:37,620
so, for example, a neural network, right.

296
00:15:37,620 --> 00:15:40,400
So, we've seen before that
any time, if we have some

297
00:15:40,400 --> 00:15:42,367
really complex function that
don't know, that we want

298
00:15:42,367 --> 00:15:44,360
to estimate, a neural network is

299
00:15:44,360 --> 00:15:46,693
a good way to estimate this.

300
00:15:48,472 --> 00:15:51,458
Okay, so this is going to take us to our

301
00:15:51,458 --> 00:15:54,242
formulation of Q-learning
that we're going to look at.

302
00:15:54,242 --> 00:15:56,646
And so, what we're going
to do is we're going

303
00:15:56,646 --> 00:15:58,906
to use a function approximator

304
00:15:58,906 --> 00:16:02,118
in order to estimate our
action value function.

305
00:16:02,118 --> 00:16:02,951
Right?

306
00:16:02,951 --> 00:16:04,502
And if this function approximator

307
00:16:04,502 --> 00:16:06,013
is a deep neural network, which is

308
00:16:06,013 --> 00:16:08,142
what's been used recently,

309
00:16:08,142 --> 00:16:10,782
then this is going to be
called deep Q-learning.

310
00:16:10,782 --> 00:16:12,322
And so this is something that

311
00:16:12,322 --> 00:16:15,742
you'll hear around as one
of the common approaches

312
00:16:15,742 --> 00:16:20,150
to deep reinforcement
learning that's in use.

313
00:16:20,150 --> 00:16:21,259
Right, and so in this case,

314
00:16:21,259 --> 00:16:23,474
we also have our function parameters

315
00:16:23,474 --> 00:16:26,134
theta here, so our Q-value function

316
00:16:26,134 --> 00:16:28,348
is determined by these weights,

317
00:16:28,348 --> 00:16:30,765
theta, of our neural network.

318
00:16:33,050 --> 00:16:35,425
Okay, so given this
function approximation,

319
00:16:35,425 --> 00:16:37,970
how do we solve for our optimal policy?

320
00:16:37,970 --> 00:16:39,814
So remember that we want to find

321
00:16:39,814 --> 00:16:44,744
a Q-function that's satisfying
the Bellman equation.

322
00:16:44,744 --> 00:16:47,017
Right, and so we want to
enforce this Bellman equation

323
00:16:47,017 --> 00:16:50,452
to happen, so what we
can do when we have this

324
00:16:50,452 --> 00:16:54,713
neural network approximating
our Q-function is that

325
00:16:54,713 --> 00:16:56,811
we can train this where our loss function

326
00:16:56,811 --> 00:16:58,169
is going to try and minimize

327
00:16:58,169 --> 00:17:00,240
the error of our Bellman equation, right?

328
00:17:00,240 --> 00:17:03,689
Or how far q of s, a is from its target,

329
00:17:03,689 --> 00:17:06,454
which is the Y_i here,
the right hand side

330
00:17:06,454 --> 00:17:09,853
of the Bellman equation
that we saw earlier.

331
00:17:09,853 --> 00:17:12,103
So, we're basically going to take these

332
00:17:12,103 --> 00:17:13,994
forward passes of our

333
00:17:13,994 --> 00:17:16,928
loss function, trying
to minimize this error

334
00:17:16,929 --> 00:17:19,332
and then our backward
pass, our gradient update,

335
00:17:19,332 --> 00:17:20,863
is just going to be

336
00:17:20,863 --> 00:17:23,243
you just take the gradient of this

337
00:17:23,243 --> 00:17:28,182
loss, with respect to our
network parameter's theta.

338
00:17:28,183 --> 00:17:31,568
Right, and so our goal is again to

339
00:17:31,568 --> 00:17:33,752
have this effect as we're
taking gradient steps

340
00:17:33,752 --> 00:17:36,107
of iteratively trying
to make our Q-function

341
00:17:36,107 --> 00:17:38,436
closer to our target value.

342
00:17:38,436 --> 00:17:40,853
So, any questions about this?

343
00:17:42,691 --> 00:17:43,524
Okay.

344
00:17:44,537 --> 00:17:48,719
So let's look at a case
study of an example where

345
00:17:48,719 --> 00:17:50,824
one of the classic examples
of deep reinforcement learning

346
00:17:50,824 --> 00:17:53,370
where this approach was applied.

347
00:17:53,370 --> 00:17:56,174
And so we're going to look at
this problem that we saw earlier

348
00:17:56,174 --> 00:17:59,744
of playing Atari games,
where our objective was

349
00:17:59,744 --> 00:18:01,746
to complete the game
with the highest score

350
00:18:01,746 --> 00:18:04,150
and remember our state is
going to be the raw pixel

351
00:18:04,150 --> 00:18:05,460
inputs of the game state,

352
00:18:05,460 --> 00:18:07,064
and we can take these actions

353
00:18:07,064 --> 00:18:09,308
of moving left, right, up, down,

354
00:18:09,308 --> 00:18:12,964
or whatever actions of
the particular game.

355
00:18:12,964 --> 00:18:15,210
And our reward at each time
step, we're going to get

356
00:18:15,210 --> 00:18:18,509
a reward of our score
increase or decrease that we

357
00:18:18,509 --> 00:18:21,183
got at this time step, and
so our cumulative total

358
00:18:21,183 --> 00:18:24,435
reward is this total reward
that we'll usually see

359
00:18:24,435 --> 00:18:27,095
at the top of the screen.

360
00:18:27,095 --> 00:18:30,135
Okay, so the network that
we're going to use for our

361
00:18:30,135 --> 00:18:32,955
Q-function is going to
look something like this,

362
00:18:32,955 --> 00:18:37,355
right, where we have our
Q-network, with weight's theta.

363
00:18:37,355 --> 00:18:41,272
And then our input, our
state s, is going to be

364
00:18:42,259 --> 00:18:43,791
our current game screen.

365
00:18:43,791 --> 00:18:45,377
And in practice we're going to take

366
00:18:45,377 --> 00:18:49,509
a stack of the last four
frames, so we have some history.

367
00:18:49,509 --> 00:18:52,340
And so we'll take these raw pixel values,

368
00:18:52,340 --> 00:18:55,609
we'll do some, you know, RGB
to gray-scale conversions,

369
00:18:55,609 --> 00:18:57,053
some down-sampling, some cropping,

370
00:18:57,053 --> 00:18:58,609
so, some pre-processing.

371
00:18:58,609 --> 00:19:02,543
And what we'll get out of
this is this 84 by 84 by four

372
00:19:02,543 --> 00:19:04,631
stack of the last four frames.

373
00:19:04,631 --> 00:19:05,464
Yeah, question.

374
00:19:05,464 --> 00:19:09,631
[inaudible question from audience]

375
00:19:12,792 --> 00:19:14,768
Okay, so the question
is, are we saying here

376
00:19:14,768 --> 00:19:18,067
that our network is
going to approximate our

377
00:19:18,067 --> 00:19:20,809
Q-value function for
different state, action pairs,

378
00:19:20,809 --> 00:19:22,491
for example, four of these?

379
00:19:22,491 --> 00:19:24,765
Yeah, that's correct.

380
00:19:24,765 --> 00:19:25,598
We'll see,

381
00:19:25,598 --> 00:19:27,551
we'll talk about that in a few slides.

382
00:19:27,551 --> 00:19:29,935
[inaudible question from audience]

383
00:19:29,935 --> 00:19:30,768
So, no.

384
00:19:30,768 --> 00:19:32,883
So, we don't have a Softmax
layer after the connected,

385
00:19:32,883 --> 00:19:35,535
because here our goal
is to directly predict

386
00:19:35,535 --> 00:19:36,816
our Q-value functions.

387
00:19:36,816 --> 00:19:37,712
[inaudible question from audience]

388
00:19:37,712 --> 00:19:38,545
Q-values.

389
00:19:38,545 --> 00:19:40,583
[inaudible question from audience]

390
00:19:40,583 --> 00:19:44,014
Yes, so it's more doing
regression to our Q-values.

391
00:19:44,014 --> 00:19:47,549
Okay, so we have our input to this network

392
00:19:47,549 --> 00:19:51,007
and then on top of this,
we're going to have

393
00:19:51,007 --> 00:19:52,847
a couple of familiar convolutional layers,

394
00:19:52,847 --> 00:19:54,084
and a fully-connected layer,

395
00:19:54,084 --> 00:19:55,334
so here we have

396
00:19:56,191 --> 00:19:58,036
an eight-by-eight
convolutions and we have some

397
00:19:58,036 --> 00:19:59,611
four-by-four convolutions.

398
00:19:59,611 --> 00:20:01,861
Then we have a FC 256 layer,

399
00:20:01,861 --> 00:20:03,458
so this is just a standard kind of networK

400
00:20:03,458 --> 00:20:05,674
that you've seen before.

401
00:20:05,674 --> 00:20:10,382
And then, finally, our last
fully-connected layer has

402
00:20:10,382 --> 00:20:13,470
a vector of outputs, which
is corresponding to your

403
00:20:13,470 --> 00:20:16,074
Q-value for each action, right, given

404
00:20:16,074 --> 00:20:17,415
the state that you've input.

405
00:20:17,415 --> 00:20:19,565
And so, for example, if
you have four actions,

406
00:20:19,565 --> 00:20:21,770
then here we have this
four-dimensional output

407
00:20:21,770 --> 00:20:25,570
corresponding to Q of
current s, as well as a-one,

408
00:20:25,570 --> 00:20:28,685
and then a-two, a-three, and a-four.

409
00:20:28,685 --> 00:20:30,857
Right so this is going
to be one scalar value

410
00:20:30,857 --> 00:20:33,179
for each of our actions.

411
00:20:33,179 --> 00:20:35,610
And then the number of
actions that we have

412
00:20:35,610 --> 00:20:36,955
can vary between,

413
00:20:36,955 --> 00:20:41,122
for example, 4 to 18,
depending on the Atari game.

414
00:20:43,073 --> 00:20:44,839
And one nice thing here is that

415
00:20:44,839 --> 00:20:46,709
using this network structure,

416
00:20:46,709 --> 00:20:49,931
a single feedforward
pass is able to compute

417
00:20:49,931 --> 00:20:52,810
the Q-values for all functions

418
00:20:52,810 --> 00:20:54,651
from the current state.

419
00:20:54,651 --> 00:20:56,117
And so this is really efficient.

420
00:20:56,117 --> 00:20:59,158
Right, so basically we
take our current state

421
00:20:59,158 --> 00:21:03,121
in and then because we have
this output of an action

422
00:21:03,121 --> 00:21:05,946
for each, or Q-value for each
action, as our output layer,

423
00:21:05,946 --> 00:21:10,259
we're able to do one pass and
get all of these values out.

424
00:21:10,259 --> 00:21:12,235
And then in order to train this,

425
00:21:12,235 --> 00:21:15,078
we're just going to use our
loss function from before.

426
00:21:15,078 --> 00:21:17,661
Remember, we're trying to
enforce this Bellman equation

427
00:21:17,661 --> 00:21:21,329
and so, on our forward
pass, our loss function

428
00:21:21,329 --> 00:21:25,193
we're going to try and
iteratively make our Q-value

429
00:21:25,193 --> 00:21:27,987
close to our target value,

430
00:21:27,987 --> 00:21:29,315
that it should have.

431
00:21:29,315 --> 00:21:31,281
And then our backward pass is just

432
00:21:31,281 --> 00:21:34,235
directly taking the gradient of this

433
00:21:34,235 --> 00:21:37,277
loss function that we have and then taking

434
00:21:37,277 --> 00:21:39,777
a gradient step based on that.

435
00:21:40,706 --> 00:21:42,948
So one other thing that's used
here that I want to mention

436
00:21:42,948 --> 00:21:45,639
is something called experience replay.

437
00:21:45,639 --> 00:21:49,556
And so this addresses a
problem with just using

438
00:21:50,579 --> 00:21:53,440
the plain two network
that I just described,

439
00:21:53,440 --> 00:21:55,416
which is that learning from batches

440
00:21:55,416 --> 00:21:58,134
of consecutive samples is bad.

441
00:21:58,134 --> 00:21:58,967
And so the reason

442
00:21:58,967 --> 00:22:01,268
because of this, right, is so for just

443
00:22:01,268 --> 00:22:03,578
playing the game, taking samples

444
00:22:03,578 --> 00:22:06,074
of state action rewards that we have

445
00:22:06,074 --> 00:22:08,222
and just taking consecutive
samples of these

446
00:22:08,222 --> 00:22:09,410
and training with these,

447
00:22:09,410 --> 00:22:11,814
well all of these samples are correlated

448
00:22:11,814 --> 00:22:14,218
and so this leads to

449
00:22:14,218 --> 00:22:16,118
inefficient learning, first of all,

450
00:22:16,118 --> 00:22:19,014
and also, because of this,
our current Q-network

451
00:22:19,014 --> 00:22:21,456
parameters, right, this
determines the policy

452
00:22:21,456 --> 00:22:24,842
that we're going to follow,
it determines our next

453
00:22:24,842 --> 00:22:25,798
samples that we're going to get that

454
00:22:25,798 --> 00:22:27,394
we're going to use for training.

455
00:22:27,394 --> 00:22:29,578
And so this leads to problems where

456
00:22:29,578 --> 00:22:30,832
you can have bad feedback loops.

457
00:22:30,832 --> 00:22:33,920
So, for example, if
currently the maximizing

458
00:22:33,920 --> 00:22:35,468
action that's going to take left,

459
00:22:35,468 --> 00:22:37,588
well this is going to bias all of my

460
00:22:37,588 --> 00:22:39,380
upcoming training examples to be dominated

461
00:22:39,380 --> 00:22:42,297
by samples from the left-hand side.

462
00:22:43,306 --> 00:22:45,406
And so this is a problem, right?

463
00:22:45,406 --> 00:22:47,875
And so the way that we
are going to address these

464
00:22:47,875 --> 00:22:49,808
problems is by using something called

465
00:22:49,808 --> 00:22:53,098
experience replay, where
we're going to keep this

466
00:22:53,098 --> 00:22:56,469
replay memory table of
transitions of state,

467
00:22:56,469 --> 00:22:59,345
as state, action, reward, next state,

468
00:22:59,345 --> 00:23:01,353
transitions that we have, and we're going

469
00:23:01,353 --> 00:23:04,279
to continuously update this
table with new transitions

470
00:23:04,279 --> 00:23:07,185
that we're getting as
game episodes are played,

471
00:23:07,185 --> 00:23:08,773
as we're getting more experience.

472
00:23:08,773 --> 00:23:10,653
Right, and so now what we can do

473
00:23:10,653 --> 00:23:13,207
is that we can now train
our Q-network on random,

474
00:23:13,207 --> 00:23:16,335
mini-batches of transitions
from the replay memory.

475
00:23:16,335 --> 00:23:19,261
Right, so instead of
using consecutive samples,

476
00:23:19,261 --> 00:23:21,815
we're now going to sample across these

477
00:23:21,815 --> 00:23:24,827
transitions that we've accumulated
random samples of these,

478
00:23:24,827 --> 00:23:27,573
and this breaks all of the,

479
00:23:27,573 --> 00:23:31,007
these correlation problems
that we had earlier.

480
00:23:31,007 --> 00:23:33,425
And then also, as another

481
00:23:33,425 --> 00:23:36,370
side benefit is that
each of these transitions

482
00:23:36,370 --> 00:23:39,207
can also contribute to potentially
multiple weight updates.

483
00:23:39,207 --> 00:23:41,440
We're just sampling from this table and so

484
00:23:41,440 --> 00:23:43,652
we could sample one multiple times.

485
00:23:43,652 --> 00:23:44,918
And so, this is going to lead

486
00:23:44,918 --> 00:23:47,585
also to greater data efficiency.

487
00:23:50,580 --> 00:23:52,442
Okay, so let's put this all together

488
00:23:52,442 --> 00:23:54,000
and let's look at the full algorithm

489
00:23:54,000 --> 00:23:57,583
for deep Q-learning
with experience replay.

490
00:23:59,166 --> 00:24:03,940
So we're going to start off with
initializing our replay memory

491
00:24:03,940 --> 00:24:07,383
to some capacity that we
choose, N, and then we're also

492
00:24:07,383 --> 00:24:09,703
going to initialize our

493
00:24:09,703 --> 00:24:13,075
Q-network, just with our random weights

494
00:24:13,075 --> 00:24:14,830
or initial weights.

495
00:24:14,830 --> 00:24:18,688
And then we're going to play
M episodes, or full games.

496
00:24:18,688 --> 00:24:21,832
This is going to be our training episodes.

497
00:24:21,832 --> 00:24:22,998
And then what we're going to do

498
00:24:22,998 --> 00:24:26,574
is we're going to initialize our state,

499
00:24:26,574 --> 00:24:29,526
using the starting game screen pixels

500
00:24:29,526 --> 00:24:31,265
at the beginning of each episode.

501
00:24:31,265 --> 00:24:33,555
And remember, we go through
the pre-processing step

502
00:24:33,555 --> 00:24:37,814
to get to our actual input state.

503
00:24:37,814 --> 00:24:39,313
And then for each time step

504
00:24:39,313 --> 00:24:41,584
of a game that we're currently playing,

505
00:24:41,584 --> 00:24:44,236
we're going to, with a small probability,

506
00:24:44,236 --> 00:24:46,268
select a random action,

507
00:24:46,268 --> 00:24:49,819
so one thing that's
important in these algorithms

508
00:24:49,819 --> 00:24:53,141
is to have sufficient exploration,

509
00:24:53,141 --> 00:24:54,957
so we want to make sure that

510
00:24:54,957 --> 00:24:58,559
we are sampling different
parts of the state space.

511
00:24:58,559 --> 00:25:00,353
And then otherwise, we're going

512
00:25:00,353 --> 00:25:02,405
to select from the greedy action

513
00:25:02,405 --> 00:25:03,614
from the current policy.

514
00:25:03,614 --> 00:25:05,564
Right, so most of the time
we'll take the greedy action

515
00:25:05,564 --> 00:25:07,443
that we think is

516
00:25:07,443 --> 00:25:11,083
a good policy of the type of
actions that we want to take

517
00:25:11,083 --> 00:25:13,580
and states that we want to see,
and with a small probability

518
00:25:13,580 --> 00:25:16,300
we'll sample something random.

519
00:25:16,300 --> 00:25:18,429
Okay, so then we'll take this action,

520
00:25:18,429 --> 00:25:23,076
a, t, and we'll observe the
next reward and the next state.

521
00:25:23,076 --> 00:25:26,070
So r, t and s, t plus one.

522
00:25:26,070 --> 00:25:28,385
And then we'll take this and
we'll store this transition

523
00:25:28,385 --> 00:25:32,771
in our replay memory
that we're building up.

524
00:25:32,771 --> 00:25:34,354
And then we're going to take,

525
00:25:34,354 --> 00:25:35,577
we're going to train a
network a little bit.

526
00:25:35,577 --> 00:25:37,550
So we're going to do experience replay

527
00:25:37,550 --> 00:25:40,429
and we'll take a sample
of a random mini-batches

528
00:25:40,429 --> 00:25:41,901
of transitions that we have

529
00:25:41,901 --> 00:25:44,543
from the replay memory,
and then we'll perform

530
00:25:44,543 --> 00:25:47,214
a gradient descent step on this.

531
00:25:47,214 --> 00:25:49,635
Right, so this is going to
be our full training loop.

532
00:25:49,635 --> 00:25:52,561
We're going to be
continuously playing this game

533
00:25:52,561 --> 00:25:55,774
and then also sampling

534
00:25:55,774 --> 00:25:58,431
minibatches, using
experienced replay to update

535
00:25:58,431 --> 00:26:00,100
our weights of our Q-network and then

536
00:26:00,100 --> 00:26:02,350
continuing in this fashion.

537
00:26:03,887 --> 00:26:05,912
Okay, so let's see.

538
00:26:05,912 --> 00:26:07,524
Let's see if I can,

539
00:26:07,524 --> 00:26:09,030
is this playing?

540
00:26:09,030 --> 00:26:11,852
Okay, so let's take a look

541
00:26:11,852 --> 00:26:13,532
at this deep Q-learning algorithm

542
00:26:13,532 --> 00:26:17,699
from Google DeepMind, trained
on an Atari game of Breakout.

543
00:26:20,911 --> 00:26:22,316
Alright, so it's saying
here that our input

544
00:26:22,316 --> 00:26:26,185
is just going to be our
state are raw game pixels.

545
00:26:26,185 --> 00:26:28,385
And so here we're looking
at what's happening

546
00:26:28,385 --> 00:26:29,520
at the beginning of training.

547
00:26:29,520 --> 00:26:31,505
So we've just started training a bit.

548
00:26:31,505 --> 00:26:33,159
And

549
00:26:33,159 --> 00:26:34,650
right, so it's going to look to

550
00:26:34,650 --> 00:26:36,824
it's learned to kind of hit the ball,

551
00:26:36,824 --> 00:26:40,303
but it's not doing a very
good job of sustaining it.

552
00:26:40,303 --> 00:26:42,886
But it is looking for the ball.

553
00:26:50,969 --> 00:26:53,320
Okay, so now after some more training,

554
00:26:53,320 --> 00:26:55,737
it looks like a couple hours.

555
00:27:00,946 --> 00:27:05,113
Okay, so now it's learning
to do a pretty good job here.

556
00:27:06,190 --> 00:27:08,677
So it's able to continuously follow

557
00:27:08,677 --> 00:27:10,677
this ball and be able to

558
00:27:13,882 --> 00:27:16,593
to remove most of the blocks.

559
00:27:16,593 --> 00:27:18,926
Right, so after 240 minutes.

560
00:27:33,248 --> 00:27:36,203
Okay, so here it's found
the pro strategy, right?

561
00:27:36,203 --> 00:27:38,225
You want to get all the
way to the top and then

562
00:27:38,225 --> 00:27:39,975
have it go by itself.

563
00:27:41,197 --> 00:27:42,796
Okay, so

564
00:27:42,796 --> 00:27:44,450
this is an example of using

565
00:27:44,450 --> 00:27:46,815
deep Q-learning in order to

566
00:27:46,815 --> 00:27:49,501
train an agent to be
able to play Atari games.

567
00:27:49,501 --> 00:27:51,485
It's able to do this on many Atari games

568
00:27:51,485 --> 00:27:52,998
and so you can check out

569
00:27:52,998 --> 00:27:55,081
some more of this online.

570
00:27:56,419 --> 00:27:58,168
Okay, so we've talked about Q-learning.

571
00:27:58,168 --> 00:28:01,149
But there is a problem
with Q-learning, right?

572
00:28:01,149 --> 00:28:03,754
It can be challenging
and what's the problem?

573
00:28:03,754 --> 00:28:05,126
Well, the problem can be that

574
00:28:05,126 --> 00:28:07,226
the Q-function is very complicated.

575
00:28:07,226 --> 00:28:09,344
Right, so we have to, we're
saying that we want to learn

576
00:28:09,344 --> 00:28:12,335
the value of every state action pair.

577
00:28:12,335 --> 00:28:14,854
So, if, let's say you have
something, for example,

578
00:28:14,854 --> 00:28:17,275
a robot grasping, wanting
to grasp an object.

579
00:28:17,275 --> 00:28:19,576
Right, you're going to have a
really high dimensional state.

580
00:28:19,576 --> 00:28:23,033
You have, I mean, let's
say you have all of your

581
00:28:23,033 --> 00:28:26,225
even just joint, joint
positions, and angles.

582
00:28:26,225 --> 00:28:29,380
Right, and so learning the
exact value of every state

583
00:28:29,380 --> 00:28:31,421
action pair that you have, right,

584
00:28:31,421 --> 00:28:34,171
can be really, really hard to do.

585
00:28:35,493 --> 00:28:38,724
But on the other hand, your
policy can be much simpler.

586
00:28:38,724 --> 00:28:40,310
Right, like what you want this robot to do

587
00:28:40,310 --> 00:28:42,542
maybe just to have this simple motion

588
00:28:42,542 --> 00:28:44,556
of just closing your hand, right?

589
00:28:44,556 --> 00:28:45,952
Just, move your fingers in this

590
00:28:45,952 --> 00:28:48,252
particular direction and keep going.

591
00:28:48,252 --> 00:28:51,832
And so, that leads to the question of

592
00:28:51,832 --> 00:28:54,142
can we just learn this policy directly?

593
00:28:54,142 --> 00:28:55,872
Right, is it possible,
maybe, to just find the best

594
00:28:55,872 --> 00:28:58,306
policy from a collection of policies,

595
00:28:58,306 --> 00:28:59,988
without trying to go through this process

596
00:28:59,988 --> 00:29:02,078
of estimating your Q-value

597
00:29:02,078 --> 00:29:05,495
and then using that to infer your policy.

598
00:29:06,790 --> 00:29:09,288
So, this is an approach that

599
00:29:09,288 --> 00:29:10,257
oh,

600
00:29:10,257 --> 00:29:13,154
so, okay, this is an approach that

601
00:29:13,154 --> 00:29:15,938
we're going to call policy gradients.

602
00:29:15,938 --> 00:29:18,228
And so, formally, let's define a

603
00:29:18,228 --> 00:29:20,858
class of parametrized policies.

604
00:29:20,858 --> 00:29:24,146
Parametrized by weights theta,

605
00:29:24,146 --> 00:29:25,889
and so for each policy

606
00:29:25,889 --> 00:29:27,791
let's define the value of the policy.

607
00:29:27,791 --> 00:29:30,859
So, J, our value J,
given parameters theta,

608
00:29:30,859 --> 00:29:32,437
is going to be, or expected

609
00:29:32,437 --> 00:29:35,723
some cumulative sum of future
rewards that we care about.

610
00:29:35,723 --> 00:29:38,971
So, the same reward that we've been using.

611
00:29:38,971 --> 00:29:41,879
And so our goal then, under this setup

612
00:29:41,879 --> 00:29:44,719
is that we want to find an optimal policy,

613
00:29:44,719 --> 00:29:48,243
theta star, which is the maximum, right,

614
00:29:48,243 --> 00:29:51,548
arg max over theta of J of theta.

615
00:29:51,548 --> 00:29:53,946
So we want to find the
policy, the policy parameters

616
00:29:53,946 --> 00:29:56,917
that gives our best expected reward.

617
00:29:56,917 --> 00:29:58,834
So, how can we do this?

618
00:30:00,178 --> 00:30:01,011
Any ideas?

619
00:30:04,993 --> 00:30:06,843
Okay, well, what we can do

620
00:30:06,843 --> 00:30:10,155
is just a gradient assent on
our policy parameters, right?

621
00:30:10,155 --> 00:30:12,476
We've learned that given
some objective that we have,

622
00:30:12,476 --> 00:30:15,460
some parameters we can
just use gradient asscent

623
00:30:15,460 --> 00:30:17,512
and gradient assent in order

624
00:30:17,512 --> 00:30:20,762
to continuously improve our parameters.

625
00:30:23,202 --> 00:30:24,950
And so let's talk more
specifically about how

626
00:30:24,950 --> 00:30:27,174
we can do this, which we're going to call

627
00:30:27,174 --> 00:30:29,196
here the reinforce algorithm.

628
00:30:29,196 --> 00:30:31,068
So, mathematically, we can write

629
00:30:31,068 --> 00:30:34,375
out our expected future reward

630
00:30:34,375 --> 00:30:36,781
over trajectories, and
so we're going to sample

631
00:30:36,781 --> 00:30:38,611
these trajectories of experience, right,

632
00:30:38,611 --> 00:30:40,286
like for example episodes of game play

633
00:30:40,286 --> 00:30:41,902
that we talked about earlier.

634
00:30:41,902 --> 00:30:45,673
S-zero, a-zero, r-zero, s-one,

635
00:30:45,673 --> 00:30:47,411
a-one, r-one, and so on.

636
00:30:47,411 --> 00:30:51,723
Using some policy pi of theta.

637
00:30:51,723 --> 00:30:54,139
Right, and then so, for each trajectory

638
00:30:54,139 --> 00:30:57,739
we can compute a reward
for that trajectory.

639
00:30:57,739 --> 00:30:59,135
It's the cumulative reward that we

640
00:30:59,135 --> 00:31:01,245
got from following this trajectory.

641
00:31:01,245 --> 00:31:03,733
And then the value of a policy,

642
00:31:03,733 --> 00:31:05,968
pi sub theta, is going
to be just the expected

643
00:31:05,968 --> 00:31:07,933
reward of these
trajectories that we can get

644
00:31:07,933 --> 00:31:10,570
from the following pi sub theta.

645
00:31:10,570 --> 00:31:12,701
So that's here, this
expectation over trajectories

646
00:31:12,701 --> 00:31:16,868
that we can get, sampling
trajectories from our policy.

647
00:31:18,563 --> 00:31:19,424
Okay.

648
00:31:19,424 --> 00:31:21,288
So, we want to do gradient ascent, right?

649
00:31:21,288 --> 00:31:22,961
So let's differentiate this.

650
00:31:22,961 --> 00:31:25,023
Once we differentiate
this, then we can just take

651
00:31:25,023 --> 00:31:27,356
gradient steps, like normal.

652
00:31:28,535 --> 00:31:30,418
So, the problem is that
now if we try and just

653
00:31:30,418 --> 00:31:32,678
differentiate this exactly,

654
00:31:32,678 --> 00:31:34,300
this is intractable, right?

655
00:31:34,300 --> 00:31:37,388
So, the gradient of an
expectation is problematic

656
00:31:37,388 --> 00:31:41,319
when p is dependent on
theta here, because here

657
00:31:41,319 --> 00:31:43,513
we want to take this gradient

658
00:31:43,513 --> 00:31:47,661
of p of tau, given theta,

659
00:31:47,661 --> 00:31:48,766
but this is going to be,

660
00:31:48,766 --> 00:31:50,591
we want to take this integral over tau.

661
00:31:50,591 --> 00:31:53,033
Right, so this is intractable.

662
00:31:53,033 --> 00:31:57,327
However, we can use a trick
here to get around this.

663
00:31:57,327 --> 00:32:01,855
And this trick is taking this
gradient that we want, of p.

664
00:32:01,855 --> 00:32:03,203
We can rewrite this

665
00:32:03,203 --> 00:32:04,941
by just multiplying this by one,

666
00:32:04,941 --> 00:32:07,081
by multiplying top and bottom,

667
00:32:07,081 --> 00:32:10,286
both by p of tau given theta.

668
00:32:10,286 --> 00:32:12,052
Right, and then if we look at these terms

669
00:32:12,052 --> 00:32:14,248
that we have now here, in the
way that I've written this,

670
00:32:14,248 --> 00:32:15,958
on the left and the right, this is

671
00:32:15,958 --> 00:32:18,815
actually going to be equivalent to

672
00:32:18,815 --> 00:32:23,424
p of tau times our gradient

673
00:32:23,424 --> 00:32:26,170
with respect to theta, of log, of p.

674
00:32:26,170 --> 00:32:29,074
Right, because the gradient
of the log of p is just going

675
00:32:29,074 --> 00:32:32,741
to be one over p times gradient of p.

676
00:32:33,808 --> 00:32:36,934
Okay, so if we then inject this back

677
00:32:36,934 --> 00:32:41,385
into our expression that we
had earlier for this gradient,

678
00:32:41,385 --> 00:32:43,426
we can see that, what this
will actually look like,

679
00:32:43,426 --> 00:32:46,059
right, because now we
have a gradient of log p

680
00:32:46,059 --> 00:32:49,106
times our probabilities of
all of these trajectories

681
00:32:49,106 --> 00:32:52,187
and then taking this
integral here, over tau.

682
00:32:52,187 --> 00:32:54,495
This is now going to be an expectation

683
00:32:54,495 --> 00:32:58,586
over our trajectories tau,
and so what we've done here

684
00:32:58,586 --> 00:33:02,751
is that we've taken a
gradient of an expectation

685
00:33:02,751 --> 00:33:06,823
and we've transformed it into
an expectation of gradients.

686
00:33:06,823 --> 00:33:09,156
Right, and so now we can use

687
00:33:10,051 --> 00:33:12,404
sample trajectories that we can get

688
00:33:12,404 --> 00:33:14,712
in order to estimate our gradient.

689
00:33:14,712 --> 00:33:17,343
And so we do this using
Monte Carlo sampling,

690
00:33:17,343 --> 00:33:21,260
and this is one of the
core ideas of reinforce.

691
00:33:23,624 --> 00:33:25,846
Okay, so looking at this

692
00:33:25,846 --> 00:33:28,180
expression that we want to compute,

693
00:33:28,180 --> 00:33:30,421
can we compute these
quantities that we had here

694
00:33:30,421 --> 00:33:33,071
without knowing the
transition probabilities?

695
00:33:33,071 --> 00:33:36,643
Alright, so we have that
p of tau is going to be

696
00:33:36,643 --> 00:33:38,466
the probability of a trajectory.

697
00:33:38,466 --> 00:33:40,387
It's going to be the product of

698
00:33:40,387 --> 00:33:43,379
all of our transition
probabilities of the next state

699
00:33:43,379 --> 00:33:45,821
that we get, given our
current state and action

700
00:33:45,821 --> 00:33:49,051
as well as our probability
of the actions that

701
00:33:49,051 --> 00:33:52,232
we've taken under our policy pi.

702
00:33:52,232 --> 00:33:54,743
Right, so we're going to
multiply all of these together,

703
00:33:54,743 --> 00:33:58,441
and get our probability of our trajectory.

704
00:33:58,441 --> 00:34:03,059
So this log of p of tau
that we want to compute

705
00:34:03,059 --> 00:34:06,334
is going to be we just
take this log and this will

706
00:34:06,334 --> 00:34:08,326
separate this out into a sum

707
00:34:08,326 --> 00:34:10,389
of pushing the logs inside.

708
00:34:10,389 --> 00:34:12,383
And then here, when we differentiate this,

709
00:34:12,384 --> 00:34:14,237
we can see we want to
differentiate with respect

710
00:34:14,237 --> 00:34:18,162
to theta, but this first
term that we have here,

711
00:34:18,163 --> 00:34:20,911
log p of the state
transition probabilities

712
00:34:20,911 --> 00:34:22,850
there's no theta term here, and so

713
00:34:22,850 --> 00:34:25,292
the only place where we have
theta is the second term

714
00:34:25,292 --> 00:34:28,709
that we have, of log of pi sub theta,

715
00:34:29,675 --> 00:34:32,914
of our action, given our
state, and so this is the only

716
00:34:32,914 --> 00:34:34,139
term that we keep

717
00:34:34,139 --> 00:34:37,368
in our gradient estimate,
and so we can see here that

718
00:34:37,369 --> 00:34:39,670
this doesn't depend on the
transition probabilities,

719
00:34:39,670 --> 00:34:41,293
right, so we actually don't need to know

720
00:34:41,293 --> 00:34:44,588
our transition probabilities
in order to computer

721
00:34:44,589 --> 00:34:46,422
our gradient estimate.

722
00:34:47,257 --> 00:34:50,879
And then, so, therefore
when we're sampling these,

723
00:34:50,880 --> 00:34:55,047
for any given trajectory tau,
we can estimate J of theta

724
00:34:56,306 --> 00:34:58,524
using this gradient estimate.

725
00:34:58,524 --> 00:35:00,472
This is here shown for a single trajectory

726
00:35:00,472 --> 00:35:02,220
from what we had earlier,

727
00:35:02,220 --> 00:35:05,271
and then we can also sample
over multiple trajectories

728
00:35:05,271 --> 00:35:07,188
to get the expectation.

729
00:35:09,248 --> 00:35:12,974
Okay, so given this gradient
estimator that we've derived,

730
00:35:12,974 --> 00:35:17,141
the interpretation that we can
make from this here, is that

731
00:35:18,217 --> 00:35:21,931
if our reward for a trajectory
is high, if the reward that

732
00:35:21,931 --> 00:35:25,226
we got from taking the
sequence of actions was good,

733
00:35:25,226 --> 00:35:27,517
then let's push up the
probabilities of all

734
00:35:27,517 --> 00:35:29,434
the actions that we've seen.

735
00:35:29,434 --> 00:35:31,458
Right, we're just going to say that

736
00:35:31,458 --> 00:35:33,141
these were good actions that we took.

737
00:35:33,141 --> 00:35:35,287
And then if the reward is low,

738
00:35:35,287 --> 00:35:37,186
we want to push down these probabilities.

739
00:35:37,186 --> 00:35:38,629
We want to say these were bad actions,

740
00:35:38,629 --> 00:35:40,747
let's try and not sample these so much.

741
00:35:40,747 --> 00:35:43,568
Right and so we can see
that's what's happening here,

742
00:35:43,568 --> 00:35:47,392
where we have pi of a, given s.

743
00:35:47,392 --> 00:35:50,980
This is the likelihood of
the actions that we've taken

744
00:35:50,980 --> 00:35:53,163
and then we're going to scale
this, we're going to take the

745
00:35:53,163 --> 00:35:56,621
gradient and the gradient
is going to tell us how much

746
00:35:56,621 --> 00:36:00,555
should we change the
parameters in order to increase

747
00:36:00,555 --> 00:36:03,575
our likelihood of our action, a, right?

748
00:36:03,575 --> 00:36:06,501
And then we're going to
take this and scale it by

749
00:36:06,501 --> 00:36:09,019
how much reward we actually got from it,

750
00:36:09,019 --> 00:36:12,602
so how good were these
actions, in reality.

751
00:36:14,561 --> 00:36:16,209
Okay, so

752
00:36:16,209 --> 00:36:18,454
this might seem simplistic to say that,

753
00:36:18,454 --> 00:36:21,124
you know, if a trajectory
is good, then we're saying

754
00:36:21,124 --> 00:36:22,965
here that all of its actions were good.

755
00:36:22,965 --> 00:36:23,798
Right?

756
00:36:23,798 --> 00:36:26,356
But, in expectation, this
actually averages out.

757
00:36:26,356 --> 00:36:30,125
So we have an unbiased estimator here,

758
00:36:30,125 --> 00:36:32,580
and so if you have many samples of this,

759
00:36:32,580 --> 00:36:35,622
then we will get an accurate
estimate of our gradient.

760
00:36:35,622 --> 00:36:38,510
And this is nice because we can just take

761
00:36:38,510 --> 00:36:40,666
gradient steps and we know
that we're going to be

762
00:36:40,666 --> 00:36:42,994
improving our loss
function and getting closer

763
00:36:42,994 --> 00:36:45,976
to, at least some local optimum of our

764
00:36:45,976 --> 00:36:48,602
policy parameters theta.

765
00:36:48,602 --> 00:36:50,690
Alright, but there is a problem with this,

766
00:36:50,690 --> 00:36:52,789
and the problem is that this also suffers

767
00:36:52,789 --> 00:36:54,884
from high variance.

768
00:36:54,884 --> 00:36:57,201
Because this credit
assignment is really hard.

769
00:36:57,201 --> 00:36:58,902
Right, we're saying that

770
00:36:58,902 --> 00:37:02,283
given a reward that we
got, we're going to say

771
00:37:02,283 --> 00:37:04,412
all of the actions were good,
we're just going to hope

772
00:37:04,412 --> 00:37:06,537
that this assignment of
which actions were actually

773
00:37:06,537 --> 00:37:08,395
the best actions, that mattered,

774
00:37:08,395 --> 00:37:11,080
are going to average out over time.

775
00:37:11,080 --> 00:37:14,598
And so this is really hard
and we need a lot of samples

776
00:37:14,598 --> 00:37:17,190
in order to have a good estimate.

777
00:37:17,190 --> 00:37:19,406
Alright, so this leads to the
question of, is there anything

778
00:37:19,406 --> 00:37:21,684
that we can do to reduce the variance

779
00:37:21,684 --> 00:37:23,851
and improve the estimator?

780
00:37:26,540 --> 00:37:29,123
And so variance reduction is

781
00:37:30,164 --> 00:37:33,323
an important area of research
in policy gradients,

782
00:37:33,323 --> 00:37:36,467
and in coming up with
ways in order to improve

783
00:37:36,467 --> 00:37:39,756
the estimator and require fewer samples.

784
00:37:39,756 --> 00:37:41,445
Alright, so let's look
at a couple of ideas

785
00:37:41,445 --> 00:37:43,278
of how we can do this.

786
00:37:44,202 --> 00:37:46,764
So given our gradient estimator,

787
00:37:46,764 --> 00:37:49,017
so the first idea is that we can

788
00:37:49,017 --> 00:37:52,610
push up the probabilities of an action

789
00:37:52,610 --> 00:37:56,258
only by it's affect on future rewards

790
00:37:56,258 --> 00:37:57,091
from that state, right?

791
00:37:57,091 --> 00:37:59,312
So, now with instead of scaling

792
00:37:59,312 --> 00:38:02,066
this likelihood, or
pushing up this likelihood

793
00:38:02,066 --> 00:38:04,736
of this action by the total
reward of its trajectory,

794
00:38:04,736 --> 00:38:07,320
let's look more
specifically at just the sum

795
00:38:07,320 --> 00:38:09,876
of rewards coming from this time step

796
00:38:09,876 --> 00:38:12,108
on to the end, right?

797
00:38:12,108 --> 00:38:14,224
And so, this is basically saying that

798
00:38:14,224 --> 00:38:17,441
how good an action is, is
only specified by how much

799
00:38:17,441 --> 00:38:18,999
future reward it generates.

800
00:38:18,999 --> 00:38:20,499
Which makes sense.

801
00:38:21,811 --> 00:38:24,251
Okay, so a second idea
that we can also use

802
00:38:24,251 --> 00:38:26,931
is using a discount factor in order

803
00:38:26,931 --> 00:38:29,448
to ignore delayed effects.

804
00:38:29,448 --> 00:38:33,133
Alright so here we've added
back in this discount factor,

805
00:38:33,133 --> 00:38:36,774
that we've seen before,
which is saying that

806
00:38:36,774 --> 00:38:39,991
we are, you know, our discount
factor's going to tell us

807
00:38:39,991 --> 00:38:41,841
how much we care about just the

808
00:38:41,841 --> 00:38:44,510
rewards that are coming up soon,

809
00:38:44,510 --> 00:38:47,276
versus rewards that came much later on.

810
00:38:47,276 --> 00:38:49,462
Right, so we were going to now

811
00:38:49,462 --> 00:38:51,438
say how good or bad an action is,

812
00:38:51,438 --> 00:38:54,071
looking more at the local neighborhood

813
00:38:54,071 --> 00:38:57,489
of action set it generates
in the immediate near future

814
00:38:57,489 --> 00:39:00,880
and down weighting the the
ones that come later on.

815
00:39:00,880 --> 00:39:02,471
Okay so

816
00:39:02,471 --> 00:39:05,194
these are some straightforward ideas

817
00:39:05,194 --> 00:39:07,730
that are generally used in practice.

818
00:39:07,730 --> 00:39:11,529
So, a third idea is this idea of using

819
00:39:11,529 --> 00:39:14,597
a baseline in order to
reduce your variance.

820
00:39:14,597 --> 00:39:18,273
And so, a problem with
just using the raw value

821
00:39:18,273 --> 00:39:20,690
of your trajectories, is that

822
00:39:21,675 --> 00:39:23,869
this isn't necessarily meaningful, right?

823
00:39:23,869 --> 00:39:26,653
So, for example, if your
rewards are all positive,

824
00:39:26,653 --> 00:39:27,973
then you're just going to keep pushing

825
00:39:27,973 --> 00:39:29,835
up the probabilities of all your actions.

826
00:39:29,835 --> 00:39:32,039
And of course, you'll push
them up to various degrees,

827
00:39:32,039 --> 00:39:35,448
but what's really important
is whether a reward is better

828
00:39:35,448 --> 00:39:39,753
or worse than what you're
expecting to be getting.

829
00:39:39,753 --> 00:39:42,993
Alright, so in order to
address this, we can introduce

830
00:39:42,993 --> 00:39:46,071
a baseline function that's
dependent on the state.

831
00:39:46,071 --> 00:39:47,598
Right, so this baseline function tell us

832
00:39:47,598 --> 00:39:51,219
what's, how much we, what's
our guess and what we expect

833
00:39:51,219 --> 00:39:53,886
to get from this state, and then

834
00:39:55,515 --> 00:39:58,031
our reward or our scaling
factor that we're going to use

835
00:39:58,031 --> 00:39:59,837
to be pushing up or
down our probabilities,

836
00:39:59,837 --> 00:40:02,592
can now just be our expected
sum of future rewards,

837
00:40:02,592 --> 00:40:05,508
minus this baseline, so now
it's the relative of how

838
00:40:05,508 --> 00:40:08,939
much better or worse is
the reward that we got

839
00:40:08,939 --> 00:40:10,772
from what we expected.

840
00:40:11,870 --> 00:40:14,971
And so how can we choose this baseline?

841
00:40:14,971 --> 00:40:16,099
Well,

842
00:40:16,099 --> 00:40:19,168
a very simple baseline, the
most simple you can use,

843
00:40:19,168 --> 00:40:21,065
is just taking a moving average

844
00:40:21,065 --> 00:40:23,013
of rewards that you've experienced so far.

845
00:40:23,013 --> 00:40:25,027
So you can even do this
overall trajectories,

846
00:40:25,027 --> 00:40:28,863
and this is just an
average of what rewards

847
00:40:28,863 --> 00:40:31,431
have I been seeing as I've been training,

848
00:40:31,431 --> 00:40:34,765
and as I've been playing these episodes?

849
00:40:34,765 --> 00:40:37,549
Right, and so this gives
some idea of whether the

850
00:40:37,549 --> 00:40:41,716
reward that I currently get
was relatively better or worse.

851
00:40:42,821 --> 00:40:45,737
And so there's some variance
on this that you can use

852
00:40:45,737 --> 00:40:49,215
but so far the variance
reductions that we've seen so far

853
00:40:49,215 --> 00:40:51,588
are all used in what's typically

854
00:40:51,588 --> 00:40:54,452
called "vanilla REINFORCE" algorithm.

855
00:40:54,452 --> 00:40:56,787
Right, so looking at the
cumulative future reward,

856
00:40:56,787 --> 00:41:00,954
having a discount factor,
and some simple baselines.

857
00:41:02,601 --> 00:41:05,081
Now let's talk about how we can

858
00:41:05,081 --> 00:41:06,547
think about this idea of baseline

859
00:41:06,547 --> 00:41:08,769
and potentially choose better baselines.

860
00:41:08,769 --> 00:41:12,084
Right, so if we're going to
think about what's a better

861
00:41:12,084 --> 00:41:13,567
baseline that we can choose,

862
00:41:13,567 --> 00:41:16,569
what we want to do is we want
to push up the probability

863
00:41:16,569 --> 00:41:19,931
of an action from a state,
if the action was better than

864
00:41:19,931 --> 00:41:24,255
the expected value of what we
should get from that state.

865
00:41:24,255 --> 00:41:27,655
So, thinking about the value
of what we're going to expect

866
00:41:27,655 --> 00:41:30,163
from the state, what
does this remind you of?

867
00:41:30,163 --> 00:41:31,189
Does this remind you of anything

868
00:41:31,189 --> 00:41:34,939
that we talked about
earlier in this lecture?

869
00:41:37,023 --> 00:41:37,856
Yes.

870
00:41:37,856 --> 00:41:39,266
[inaudible from audience]

871
00:41:39,266 --> 00:41:41,297
Yeah, so the value functions, right?

872
00:41:41,297 --> 00:41:45,201
The value functions that we
talked about with Q-learning.

873
00:41:45,201 --> 00:41:46,034
So, exactly.

874
00:41:46,034 --> 00:41:47,871
So Q-functions and value functions

875
00:41:47,871 --> 00:41:50,895
and so, the intuition is that

876
00:41:50,895 --> 00:41:52,347
well,

877
00:41:52,347 --> 00:41:54,704
we're happy with an action,

878
00:41:54,704 --> 00:41:58,173
taking an action in a state s, if

879
00:41:58,173 --> 00:42:00,248
our Q-value of taking

880
00:42:00,248 --> 00:42:04,752
a specific action from
this state is larger than

881
00:42:04,752 --> 00:42:06,999
the value function or expected value

882
00:42:06,999 --> 00:42:08,406
of the cumulative future reward

883
00:42:08,406 --> 00:42:09,698
that we can get from this state.

884
00:42:09,698 --> 00:42:11,842
Right, so this means that
this action was better than

885
00:42:11,842 --> 00:42:14,416
other actions that we could've taken.

886
00:42:14,416 --> 00:42:17,896
And on the contrary, we're
unhappy if this action,

887
00:42:17,896 --> 00:42:22,063
if this value or this
difference is negative or small.

888
00:42:23,917 --> 00:42:27,299
Right, so now if we plug
this in, in order to,

889
00:42:27,299 --> 00:42:29,269
as our scaling factor of how much we want

890
00:42:29,269 --> 00:42:32,692
to push up or down, our
probabilities of our actions,

891
00:42:32,692 --> 00:42:34,868
then we can get this estimator here.

892
00:42:34,868 --> 00:42:37,452
Right, so, it's going to be

893
00:42:37,452 --> 00:42:40,168
exactly the same as before, but now where

894
00:42:40,168 --> 00:42:43,993
we've had before our
cumulative expected reward,

895
00:42:43,993 --> 00:42:46,708
with our various reduction,
variance reduction

896
00:42:46,708 --> 00:42:50,514
techniques and baselines in,
here we can just plug in now

897
00:42:50,514 --> 00:42:53,297
this difference of how much better our

898
00:42:53,297 --> 00:42:57,113
current action was,
based on our Q-function

899
00:42:57,113 --> 00:43:00,530
minus our value function from that state.

900
00:43:01,771 --> 00:43:04,148
Right, but what we talked
about so far with our

901
00:43:04,148 --> 00:43:06,993
REINFORCE algorithm, we don't know

902
00:43:06,993 --> 00:43:09,413
what Q and V actually are.

903
00:43:09,413 --> 00:43:11,313
So can we learn these?

904
00:43:11,313 --> 00:43:14,479
And the answer is yes, using Q-learning.

905
00:43:14,479 --> 00:43:16,465
What we've already talked about before.

906
00:43:16,465 --> 00:43:19,828
So we can combine policy gradients

907
00:43:19,828 --> 00:43:22,210
while we've just been talking
about, with Q-learning,

908
00:43:22,210 --> 00:43:25,982
by training both an actor,
which is the policy,

909
00:43:25,982 --> 00:43:28,784
as well as a critic, right, a Q-function,

910
00:43:28,784 --> 00:43:32,366
which is going to tell us
how good we think a state is,

911
00:43:32,366 --> 00:43:34,380
and an action in a state.

912
00:43:34,380 --> 00:43:36,964
Right, so using this in approach,

913
00:43:36,964 --> 00:43:40,633
an actor is going to
decide which action to take

914
00:43:40,633 --> 00:43:43,716
and then the critic, or
Q-function, is going to tell

915
00:43:43,716 --> 00:43:47,708
the actor how good its action
was and how it should adjust.

916
00:43:47,708 --> 00:43:51,072
And so, and this also alleviates
a little bit of the task

917
00:43:51,072 --> 00:43:53,636
of this critic compared
to the Q-learning problems

918
00:43:53,636 --> 00:43:56,694
that we talked about earlier
of having to have this

919
00:43:56,694 --> 00:43:59,958
learning a Q-value for
every state, action pair,

920
00:43:59,958 --> 00:44:01,784
because here it only has to learn this

921
00:44:01,784 --> 00:44:04,762
for the state-action pairs that
are generated by the policy.

922
00:44:04,762 --> 00:44:06,103
It only needs to know this

923
00:44:06,103 --> 00:44:10,512
where it matters for
computing this scaling factor.

924
00:44:10,512 --> 00:44:12,830
Right, and then we can also,
as we're learning this,

925
00:44:12,830 --> 00:44:15,196
incorporate all of the
Q-learning tricks that we saw

926
00:44:15,196 --> 00:44:18,188
earlier, such as experience replay.

927
00:44:18,188 --> 00:44:20,972
And so, now I'm also going to just

928
00:44:20,972 --> 00:44:24,610
define this term that we saw earlier,

929
00:44:24,610 --> 00:44:28,248
Q of s of a, how much,
how good was an action

930
00:44:28,248 --> 00:44:30,831
in a given state, minus V of s?

931
00:44:32,199 --> 00:44:35,533
Our expected value of
how good the state is

932
00:44:35,533 --> 00:44:38,172
by this term advantage function.

933
00:44:38,172 --> 00:44:41,498
Right, so the advantage
function is how much advantage

934
00:44:41,498 --> 00:44:43,568
did we get from playing this action?

935
00:44:43,568 --> 00:44:48,100
How much better the
action was than expected.

936
00:44:48,100 --> 00:44:51,709
So, using this, we can
put together our full

937
00:44:51,709 --> 00:44:53,457
actor-critic algorithm.

938
00:44:53,457 --> 00:44:56,279
And so what this looks like,
is that we're going to start

939
00:44:56,279 --> 00:45:00,326
off with by initializing
our policy parameters theta,

940
00:45:00,326 --> 00:45:03,689
and our critic parameters
that we'll call phi.

941
00:45:03,689 --> 00:45:07,522
And then for each, for
iterations of training,

942
00:45:08,401 --> 00:45:11,149
we're going to sample M trajectories,

943
00:45:11,149 --> 00:45:12,185
under the current policy.

944
00:45:12,185 --> 00:45:13,734
Right, we're going to play
our policy and get these

945
00:45:13,734 --> 00:45:18,725
trajectories as s-zero, a-zero,
r-zero, s-one and so on.

946
00:45:18,725 --> 00:45:20,359
Okay, and then we're going to compute

947
00:45:20,359 --> 00:45:21,671
the gradients that we want.

948
00:45:21,671 --> 00:45:24,977
Right, so for each of these trajectories

949
00:45:24,977 --> 00:45:26,017
and in each time step, we're going

950
00:45:26,017 --> 00:45:28,901
to compute this advantage function,

951
00:45:28,901 --> 00:45:30,818
and then we're going to

952
00:45:31,701 --> 00:45:33,465
use this advantage function, right?

953
00:45:33,465 --> 00:45:37,131
And then we're going to use
that in our gradient estimator

954
00:45:37,131 --> 00:45:40,533
that we showed earlier, and accumulate our

955
00:45:40,533 --> 00:45:42,894
gradient estimate that we have for here.

956
00:45:42,894 --> 00:45:46,017
And then we're also going to train our

957
00:45:46,017 --> 00:45:50,837
critic parameters phi
by exactly the same way,

958
00:45:50,837 --> 00:45:54,193
so as we saw earlier,
basically trying to enforce

959
00:45:54,193 --> 00:45:57,557
this value function, right,
to learn our value function,

960
00:45:57,557 --> 00:46:01,640
which is going to be pulled
into, just minimizing

961
00:46:02,638 --> 00:46:05,467
this advantage function and this will

962
00:46:05,467 --> 00:46:08,324
encourage it to be closer
to this Bellman equation

963
00:46:08,324 --> 00:46:10,347
that we saw earlier, right?

964
00:46:10,347 --> 00:46:14,347
And so, this is basically
just iterating between

965
00:46:15,197 --> 00:46:17,733
learning and optimizing
our policy function,

966
00:46:17,733 --> 00:46:20,211
as well as our critic function.

967
00:46:20,211 --> 00:46:22,311
And so then we're going to update the

968
00:46:22,311 --> 00:46:23,977
gradients and then we're
going to go through and just

969
00:46:23,977 --> 00:46:26,727
continuously repeat this process.

970
00:46:29,271 --> 00:46:31,827
Okay, so now let's look at
some examples of REINFORCE

971
00:46:31,827 --> 00:46:36,027
in action, and let's look
first here at something called

972
00:46:36,027 --> 00:46:39,464
the Recurrent Attention Model,
which is something that,

973
00:46:39,464 --> 00:46:42,805
which is a model also
referred to as hard attention,

974
00:46:42,805 --> 00:46:46,876
but you'll see a lot in,
recently, in computer vision

975
00:46:46,876 --> 00:46:49,146
tasks for various purposes.

976
00:46:49,146 --> 00:46:51,806
Right, and so the idea behind this is

977
00:46:51,806 --> 00:46:55,122
here, I've talked about the
original work on hard attention,

978
00:46:55,122 --> 00:46:59,167
which is on image
classification, and your goal is

979
00:46:59,167 --> 00:47:02,504
to still predict the image class,

980
00:47:02,504 --> 00:47:04,822
but now you're going to do
this by taking a sequence

981
00:47:04,822 --> 00:47:06,494
of glimpses around the image.

982
00:47:06,494 --> 00:47:10,300
You're going to look at local
regions around the image

983
00:47:10,300 --> 00:47:12,754
and you're basically going
to selectively focus on these

984
00:47:12,754 --> 00:47:17,141
parts and build up information
as you're looking around.

985
00:47:17,141 --> 00:47:19,382
Right, and so the reason
that we want to do this

986
00:47:19,382 --> 00:47:21,638
is, well, first of all it
has some nice inspiration

987
00:47:21,638 --> 00:47:24,551
from human perception in eye movement.

988
00:47:24,551 --> 00:47:26,869
Let's say we're looking at a complex image

989
00:47:26,869 --> 00:47:29,225
and we want to determine
what's in the image.

990
00:47:29,225 --> 00:47:31,594
Well, you know, we might,
maybe look at a low-resolution

991
00:47:31,594 --> 00:47:34,013
of it first, and then
look specifically at parts

992
00:47:34,013 --> 00:47:36,913
of the image that will give us clues about

993
00:47:36,913 --> 00:47:39,168
what's in this image.

994
00:47:39,168 --> 00:47:40,001
And then,

995
00:47:41,160 --> 00:47:45,703
this approach of just looking
at, looking around at an image

996
00:47:45,703 --> 00:47:48,533
at local regions, is also
going to help you save

997
00:47:48,533 --> 00:47:50,435
computational resources, right?

998
00:47:50,435 --> 00:47:53,293
You don't need to process the full image.

999
00:47:53,293 --> 00:47:55,366
In practice, what usually
happens is you look at a

1000
00:47:55,366 --> 00:47:57,511
low-resolution image
first, of a full image,

1001
00:47:57,511 --> 00:48:01,678
to decide how to get started,
and then you look at high-res

1002
00:48:02,773 --> 00:48:04,671
portions of the image after that.

1003
00:48:04,671 --> 00:48:06,979
And so this saves a lot
of computational resources

1004
00:48:06,979 --> 00:48:09,725
and you can think about,
then, benefits of this

1005
00:48:09,725 --> 00:48:11,927
to scalability, right,
being able to, let's say

1006
00:48:11,927 --> 00:48:15,177
process larger images more efficiently.

1007
00:48:16,164 --> 00:48:17,780
And then, finally, this
could also actually help

1008
00:48:17,780 --> 00:48:20,099
with actual classification performance,

1009
00:48:20,099 --> 00:48:21,855
because now you're able to

1010
00:48:21,855 --> 00:48:24,760
ignore clutter and irrelevant
parts of the image.

1011
00:48:24,760 --> 00:48:25,593
Right?

1012
00:48:25,593 --> 00:48:27,678
Like, you know, instead
of always putting through

1013
00:48:27,678 --> 00:48:29,931
your ConvNet, all the parts of your image,

1014
00:48:29,931 --> 00:48:32,846
you can use this to, maybe,
first prune out what are the

1015
00:48:32,846 --> 00:48:34,936
relevant parts that I
actually want to process,

1016
00:48:34,936 --> 00:48:36,353
using my ConvNet.

1017
00:48:37,237 --> 00:48:39,849
Okay, so what's the reinforcement learning

1018
00:48:39,849 --> 00:48:41,531
formulation of this problem?

1019
00:48:41,531 --> 00:48:44,711
Well, our state is going to be

1020
00:48:44,711 --> 00:48:46,889
the glimpses that we've
seen so far, right?

1021
00:48:46,889 --> 00:48:47,722
Our

1022
00:48:48,881 --> 00:48:51,117
what's the information that we've seen?

1023
00:48:51,117 --> 00:48:53,643
Our action is then going to be where

1024
00:48:53,643 --> 00:48:55,228
to look next in the image.

1025
00:48:55,228 --> 00:48:57,090
Right, so in practice,
this can be something like

1026
00:48:57,090 --> 00:48:59,113
the x, y-coordinates,
maybe centered around some

1027
00:48:59,113 --> 00:49:02,842
fixed-sized glimpse that
you want to look at next.

1028
00:49:02,842 --> 00:49:05,664
And then the reward for
the classification problem

1029
00:49:05,664 --> 00:49:08,256
is going to be one, at
the final time step,

1030
00:49:08,256 --> 00:49:12,423
if our image is correctly
classified, and zero otherwise.

1031
00:49:14,495 --> 00:49:16,162
And so, because this

1032
00:49:17,373 --> 00:49:20,016
glimpsing, taking these
glimpses around the image

1033
00:49:20,016 --> 00:49:21,932
is a non-differentiable operation,

1034
00:49:21,932 --> 00:49:24,006
this is why we need to use

1035
00:49:24,006 --> 00:49:25,761
reinforcement learning formulation,

1036
00:49:25,761 --> 00:49:29,088
and learn policies for how
to take these glimpse actions

1037
00:49:29,088 --> 00:49:31,792
and we can train this using REINFORCE.

1038
00:49:31,792 --> 00:49:35,105
So, given the state of glimpses so far,

1039
00:49:35,105 --> 00:49:37,537
the core of our model is going to be

1040
00:49:37,537 --> 00:49:40,891
this RNN that we're going
to use to model the state,

1041
00:49:40,891 --> 00:49:44,501
and then we're going to
use our policy parameters

1042
00:49:44,501 --> 00:49:47,418
in order to output the next action.

1043
00:49:49,354 --> 00:49:53,248
Okay, so what this model looks
like is we're going to take

1044
00:49:53,248 --> 00:49:54,571
an input image.

1045
00:49:54,571 --> 00:49:57,655
Right, and then we're going to
take a glimpse at this image.

1046
00:49:57,655 --> 00:50:00,068
So here, this glimpse is the red box here,

1047
00:50:00,068 --> 00:50:03,184
and this is all blank, zeroes.

1048
00:50:03,184 --> 00:50:06,966
And so we'll pass what
we see so far into some

1049
00:50:06,966 --> 00:50:09,388
neural network, and this can be any

1050
00:50:09,388 --> 00:50:12,193
kind of network depending on your task.

1051
00:50:12,193 --> 00:50:14,276
In the original experiments
that I'm showing here,

1052
00:50:14,276 --> 00:50:16,138
on MNIST, this is very
simple, so you can just

1053
00:50:16,138 --> 00:50:18,758
use a couple of small,
fully-connected layers,

1054
00:50:18,758 --> 00:50:21,724
but you can imagine
for more complex images

1055
00:50:21,724 --> 00:50:26,105
and other tasks you may want
to use fancier ConvNets.

1056
00:50:26,105 --> 00:50:28,775
Right, so you've passed this
into some neural network,

1057
00:50:28,775 --> 00:50:31,065
and then, remember I said
we're also going to be

1058
00:50:31,065 --> 00:50:34,102
integrating our state of,
glimpses that we've seen

1059
00:50:34,102 --> 00:50:36,115
so far, using a recurrent network.

1060
00:50:36,115 --> 00:50:38,057
So, I'm just going to

1061
00:50:38,057 --> 00:50:40,265
we'll see that later on, but
this is going to go through that,

1062
00:50:40,265 --> 00:50:42,646
and then it's going to output my

1063
00:50:42,646 --> 00:50:46,094
x, y-coordinates, of where
I'm going to see next.

1064
00:50:46,094 --> 00:50:48,435
And in practice, this is going to be

1065
00:50:48,435 --> 00:50:50,766
We want to output a
distribution over actions,

1066
00:50:50,766 --> 00:50:53,385
right, and so, what this is
going to be it's going to be

1067
00:50:53,385 --> 00:50:57,282
a gaussian distribution and
we're going to output the mean.

1068
00:50:57,282 --> 00:50:59,084
You can also output a mean and variance

1069
00:50:59,084 --> 00:51:00,545
of this distribution in practice.

1070
00:51:00,545 --> 00:51:03,944
The variance can also be fixed.

1071
00:51:03,944 --> 00:51:07,172
Okay, so we're going to take this

1072
00:51:07,172 --> 00:51:08,496
action that we're now going to sample

1073
00:51:08,496 --> 00:51:11,854
a specific x, y location
from our action distribution

1074
00:51:11,854 --> 00:51:15,457
and then we're going to put
this in to get the next,

1075
00:51:15,457 --> 00:51:17,777
extract the next glimpse from our image.

1076
00:51:17,777 --> 00:51:19,297
Right, so here we've moved

1077
00:51:19,297 --> 00:51:23,385
to the end of the two,
this tail part of the two.

1078
00:51:23,385 --> 00:51:25,465
And so now we're actually
starting to get some signal

1079
00:51:25,465 --> 00:51:26,745
of what we want to see, right?

1080
00:51:26,745 --> 00:51:29,065
Like, what we want to do is we
want to look at the relevant

1081
00:51:29,065 --> 00:51:32,724
parts of the image that are
useful for classification.

1082
00:51:32,724 --> 00:51:35,354
So we pass this through, again,
our neural network layers,

1083
00:51:35,354 --> 00:51:37,104
and then also through

1084
00:51:38,153 --> 00:51:40,362
our recurrent network, right,
that's taking this input

1085
00:51:40,362 --> 00:51:43,642
as well as this previous hidden
state, and we're going to

1086
00:51:43,642 --> 00:51:45,524
use this to get a,

1087
00:51:45,524 --> 00:51:47,343
so this is representing our policy,

1088
00:51:47,343 --> 00:51:49,565
and then we're going to use this to output

1089
00:51:49,565 --> 00:51:51,354
our distribution for the next

1090
00:51:51,354 --> 00:51:54,095
location that we want to glimpse at.

1091
00:51:54,095 --> 00:51:55,874
So we can continue doing this,

1092
00:51:55,874 --> 00:51:57,303
you can see in this next glimpse here,

1093
00:51:57,303 --> 00:51:59,903
we've moved a little bit more
toward the center of the two.

1094
00:51:59,903 --> 00:52:01,723
Alright, so it's probably learning that,

1095
00:52:01,723 --> 00:52:05,005
you know, once I've seen
this tail part of the two,

1096
00:52:05,005 --> 00:52:08,093
that looks like this,
maybe moving in this upper

1097
00:52:08,093 --> 00:52:10,794
left-hand direction will
get you more towards

1098
00:52:10,794 --> 00:52:12,631
a center, which will also have a value,

1099
00:52:12,631 --> 00:52:14,543
valuable information.

1100
00:52:14,543 --> 00:52:17,473
And then we can keep doing this.

1101
00:52:17,473 --> 00:52:20,612
And then finally, at the
end, at our last time step,

1102
00:52:20,612 --> 00:52:23,412
so we can have a fixed
number of time steps here,

1103
00:52:23,412 --> 00:52:26,795
in practice something like six or eight.

1104
00:52:26,795 --> 00:52:29,359
And then at the final time
step, since we want to do

1105
00:52:29,359 --> 00:52:33,350
classification, we'll have our standard

1106
00:52:33,350 --> 00:52:36,100
Softmax layer that will produce a

1107
00:52:37,376 --> 00:52:39,363
distribution of
probabilities for each class.

1108
00:52:39,363 --> 00:52:42,111
And then here the max class was a two,

1109
00:52:42,111 --> 00:52:44,108
so we can predict that this was a two.

1110
00:52:44,108 --> 00:52:46,558
Right, and so this is going
to be the set up of our

1111
00:52:46,558 --> 00:52:50,428
model and our policy, and then we have our

1112
00:52:50,428 --> 00:52:53,079
estimate for the gradient
of this policy that we've

1113
00:52:53,079 --> 00:52:54,420
said earlier we could compute by taking

1114
00:52:54,420 --> 00:52:56,695
trajectories from here

1115
00:52:56,695 --> 00:52:59,569
and using those to do back prop.

1116
00:52:59,569 --> 00:53:02,819
And so we can just do this
in order to train this model

1117
00:53:02,819 --> 00:53:05,281
and learn the parameters
of our policy, right?

1118
00:53:05,281 --> 00:53:08,698
All of the weights that you can see here.

1119
00:53:09,953 --> 00:53:10,786
Okay, so

1120
00:53:12,239 --> 00:53:14,270
here's an example of a

1121
00:53:14,270 --> 00:53:16,710
policies trained on MNIST,

1122
00:53:16,710 --> 00:53:19,016
and so you can see that, in general,

1123
00:53:19,016 --> 00:53:20,808
from wherever it's
starting, usually learns

1124
00:53:20,808 --> 00:53:22,942
to go closer to where the digit is,

1125
00:53:22,942 --> 00:53:25,260
and then looking at the relevant
parts of the digit, right?

1126
00:53:25,260 --> 00:53:27,685
So this is pretty cool and

1127
00:53:27,685 --> 00:53:28,744
this

1128
00:53:28,744 --> 00:53:30,460
you know, follows kind of
what you would expect, right,

1129
00:53:30,460 --> 00:53:31,627
if you were to

1130
00:53:33,335 --> 00:53:34,967
choose places to look next

1131
00:53:34,967 --> 00:53:38,186
in order to most efficiently determine

1132
00:53:38,186 --> 00:53:40,108
what digit this is.

1133
00:53:40,108 --> 00:53:43,491
Right, and so this idea of hard attention,

1134
00:53:43,491 --> 00:53:45,862
of recurrent attention
models, has also been used

1135
00:53:45,862 --> 00:53:49,758
in a lot of tasks in
computer vision in the last

1136
00:53:49,758 --> 00:53:52,687
couple of years, so you'll
see this, used, for example,

1137
00:53:52,687 --> 00:53:54,790
fine-grained image recognition.

1138
00:53:54,790 --> 00:53:57,869
So, I mentioned earlier that

1139
00:53:57,869 --> 00:54:00,596
one of the useful benefits of this

1140
00:54:00,596 --> 00:54:01,763
can be also to

1141
00:54:02,975 --> 00:54:05,198
both save on computational efficiency

1142
00:54:05,198 --> 00:54:08,009
as well as to ignore
clutter and irrelevant

1143
00:54:08,009 --> 00:54:10,180
parts of the image, and
when you have fine-grained

1144
00:54:10,180 --> 00:54:11,750
image classification problems,

1145
00:54:11,750 --> 00:54:13,092
you usually want both of these.

1146
00:54:13,092 --> 00:54:17,307
You want to keep high-resolution,
so that you can look

1147
00:54:17,307 --> 00:54:19,447
at, you know, important differences.

1148
00:54:19,447 --> 00:54:23,327
And then you also want to
focus on these differences

1149
00:54:23,327 --> 00:54:25,777
and ignore irrelevant parts.

1150
00:54:25,777 --> 00:54:27,359
Yeah, question.

1151
00:54:27,359 --> 00:54:31,526
[inaudible question from audience]

1152
00:54:35,061 --> 00:54:36,789
Okay, so yeah, so the question is

1153
00:54:36,789 --> 00:54:39,061
how is there is
computational efficiency,

1154
00:54:39,061 --> 00:54:41,482
because we also have this
recurrent neural network in place.

1155
00:54:41,482 --> 00:54:45,842
So that's true, it depends
on exactly what's your,

1156
00:54:45,842 --> 00:54:47,761
what is your problem, what
is your network, and so on,

1157
00:54:47,761 --> 00:54:50,151
but you can imagine that
if you had some really

1158
00:54:50,151 --> 00:54:52,512
hi- resolution image

1159
00:54:52,512 --> 00:54:54,773
and you don't want to process
the entire parts of this

1160
00:54:54,773 --> 00:54:58,477
image with some huge ConvNet
or some huge, you know,

1161
00:54:58,477 --> 00:55:01,900
network, now you can
get some savings by just

1162
00:55:01,900 --> 00:55:04,669
focusing on specific
smaller parts of the image.

1163
00:55:04,669 --> 00:55:06,589
You only process those parts of the image.

1164
00:55:06,589 --> 00:55:08,507
But, you're right, that
it depends on exactly

1165
00:55:08,507 --> 00:55:10,924
what problem set-up you have.

1166
00:55:12,210 --> 00:55:14,530
This has also been used
in image captioning,

1167
00:55:14,530 --> 00:55:17,138
so if we're going to produce
an caption for an image,

1168
00:55:17,138 --> 00:55:20,421
we can choose, you know,
we can have the image

1169
00:55:20,421 --> 00:55:23,197
use this attention model
to generate this caption

1170
00:55:23,197 --> 00:55:26,120
and what it usually ends up
learning is these policies

1171
00:55:26,120 --> 00:55:28,999
where it'll focus on
specific parts of the image,

1172
00:55:28,999 --> 00:55:31,850
in sequence, and as it
focuses on each part,

1173
00:55:31,850 --> 00:55:34,629
it'll generate some words
or the part of the caption

1174
00:55:34,629 --> 00:55:38,341
referring to that part of the image.

1175
00:55:38,341 --> 00:55:40,170
And then it's also been used,

1176
00:55:40,170 --> 00:55:42,948
also tasks such as visual
question answering,

1177
00:55:42,948 --> 00:55:45,509
where we ask a question about the image

1178
00:55:45,509 --> 00:55:48,981
and you want the model
to output some answer

1179
00:55:48,981 --> 00:55:51,786
to your question, for
example, I don't know,

1180
00:55:51,786 --> 00:55:53,840
how many chairs are around the table?

1181
00:55:53,840 --> 00:55:58,229
And so you can see how
this attention mechanism

1182
00:55:58,229 --> 00:55:59,457
might be a good type of model

1183
00:55:59,457 --> 00:56:03,040
for learning how to
answer these questions.

1184
00:56:05,707 --> 00:56:08,475
Okay, so that was an
example of policy gradients

1185
00:56:08,475 --> 00:56:10,564
in these hard attention models.

1186
00:56:10,564 --> 00:56:13,524
And so, now I'm going to
talk about one more example,

1187
00:56:13,524 --> 00:56:16,430
that also uses policy gradients,

1188
00:56:16,430 --> 00:56:18,465
which is learning how to play Go.

1189
00:56:18,465 --> 00:56:22,006
Right, so DeepMind had this agent

1190
00:56:22,006 --> 00:56:24,497
for playing Go, called AlphGo,

1191
00:56:24,497 --> 00:56:27,297
that's been in the news a lot

1192
00:56:27,297 --> 00:56:30,708
in the past, last year and this year.

1193
00:56:30,708 --> 00:56:31,541
So, sorry?

1194
00:56:31,541 --> 00:56:32,374
[inaudible comment from audience]

1195
00:56:32,374 --> 00:56:35,291
And yesterday, yes, that's correct.

1196
00:56:36,172 --> 00:56:39,258
So this is very exciting,
recent news as well.

1197
00:56:39,258 --> 00:56:40,987
So last year,

1198
00:56:40,987 --> 00:56:43,234
a first version of AlphaGo

1199
00:56:43,234 --> 00:56:44,817
was put into a

1200
00:56:46,678 --> 00:56:49,539
competition against one
of the best Go players

1201
00:56:49,539 --> 00:56:52,609
of recent years, Lee Sedol, and the agent

1202
00:56:52,609 --> 00:56:54,927
was able to beat him

1203
00:56:54,927 --> 00:56:57,886
four to one, in a game of five matches.

1204
00:56:57,886 --> 00:57:00,541
And actually, right now, just

1205
00:57:00,541 --> 00:57:03,788
there's another match with
Ke Jie, which is current

1206
00:57:03,788 --> 00:57:07,855
world number one, and
so it's best of three

1207
00:57:07,855 --> 00:57:09,348
in China right now.

1208
00:57:09,348 --> 00:57:12,236
And so the first game was yesterday.

1209
00:57:12,236 --> 00:57:13,436
AlphaGo won.

1210
00:57:13,436 --> 00:57:16,596
I think it was by just
half a point, and so,

1211
00:57:16,596 --> 00:57:18,606
so there's two more games to watch.

1212
00:57:18,606 --> 00:57:20,657
These are all live-stream, so

1213
00:57:20,657 --> 00:57:24,276
you guys, should also go
online and watch these games.

1214
00:57:24,276 --> 00:57:28,193
It's pretty interesting
to hear the commentary.

1215
00:57:29,225 --> 00:57:32,577
But, so what is this AlphaGo
agent, right, from DeepMind?

1216
00:57:32,577 --> 00:57:34,868
And it's based on a lot
of what we've talked

1217
00:57:34,868 --> 00:57:36,466
about so far in this lecture.

1218
00:57:36,466 --> 00:57:39,687
And what it is it's a mixed
of supervised learning

1219
00:57:39,687 --> 00:57:42,045
and reinforcement learning,

1220
00:57:42,045 --> 00:57:44,657
as well as a mix of some older

1221
00:57:44,657 --> 00:57:48,573
methods for Go, Monte Carlo Tree Search,

1222
00:57:48,573 --> 00:57:51,656
as well as recent deep RL approaches.

1223
00:57:52,579 --> 00:57:56,986
So, okay, so how does AlphaGo
beat the Go world champion?

1224
00:57:56,986 --> 00:57:59,363
Well, what it first does is

1225
00:57:59,363 --> 00:58:02,449
to train AlphaGo, what it
takes as input is going to be

1226
00:58:02,449 --> 00:58:04,089
a few featurization of the board.

1227
00:58:04,089 --> 00:58:06,470
So it's basically, right,
your board and the positions

1228
00:58:06,470 --> 00:58:08,739
of the pieces on the board.

1229
00:58:08,739 --> 00:58:10,739
That's your natural state representation.

1230
00:58:10,739 --> 00:58:13,819
And what they do in order
to improve performance

1231
00:58:13,819 --> 00:58:16,638
a little bit is that
they featurize this into

1232
00:58:16,638 --> 00:58:17,958
some

1233
00:58:17,958 --> 00:58:20,510
more channels of one is all
the different stone colors,

1234
00:58:20,510 --> 00:58:21,956
so this is kind of like your

1235
00:58:21,956 --> 00:58:23,790
configuration of your board.

1236
00:58:23,790 --> 00:58:27,270
Also some channels, for
example, where, which moves

1237
00:58:27,270 --> 00:58:31,138
are legal, some bias
channels, some various things

1238
00:58:31,138 --> 00:58:33,125
and then, given this state, right,

1239
00:58:33,125 --> 00:58:35,117
it's going to first

1240
00:58:35,117 --> 00:58:36,518
train a network

1241
00:58:36,518 --> 00:58:38,867
that's initialized with
supervised training

1242
00:58:38,867 --> 00:58:40,897
from professional Go games.

1243
00:58:40,897 --> 00:58:43,147
So, given the current board configuration

1244
00:58:43,147 --> 00:58:45,627
or features, featurization of this,

1245
00:58:45,627 --> 00:58:48,495
what's the correct next action to take?

1246
00:58:48,495 --> 00:58:50,678
Alright, so given

1247
00:58:50,678 --> 00:58:52,787
examples of professional games played,

1248
00:58:52,787 --> 00:58:55,608
you know, just collected over time,

1249
00:58:55,608 --> 00:58:57,635
we can just take all of
these professional Go moves,

1250
00:58:57,635 --> 00:58:59,815
train a standard, supervised mapping,

1251
00:58:59,815 --> 00:59:02,605
from board state to action to take.

1252
00:59:02,605 --> 00:59:05,365
Alright, so they take this,
which is a pretty good start,

1253
00:59:05,365 --> 00:59:07,637
and then they're going
to use this to initialize

1254
00:59:07,637 --> 00:59:09,227
a policy network.

1255
00:59:09,227 --> 00:59:10,844
Right, so policy network,
it's just going to take

1256
00:59:10,844 --> 00:59:14,985
the exact same structure of input is your

1257
00:59:14,985 --> 00:59:16,389
board state and your output is the

1258
00:59:16,389 --> 00:59:17,778
actions that you're going to take.

1259
00:59:17,778 --> 00:59:20,887
And this was the set-up
for the policy gradients

1260
00:59:20,887 --> 00:59:21,978
that we just saw, right?

1261
00:59:21,978 --> 00:59:25,156
So now we're going to just
continue training this

1262
00:59:25,156 --> 00:59:27,130
using policy gradients.

1263
00:59:27,130 --> 00:59:30,831
And it's going to do this
reinforcement learning training

1264
00:59:30,831 --> 00:59:35,123
by playing against itself for
random, previous iterations.

1265
00:59:35,123 --> 00:59:37,384
So self play, and the
reward it's going to get

1266
00:59:37,384 --> 00:59:42,243
is one, if it wins, and a
negative one if it loses.

1267
00:59:42,243 --> 00:59:44,624
And what we're also going to
do is we're also going to learn

1268
00:59:44,624 --> 00:59:47,573
a value network, so,
something like a critic.

1269
00:59:47,573 --> 00:59:51,235
And then, the final AlphaGo
is going to be combining

1270
00:59:51,235 --> 00:59:53,982
all of these together, so
policy and value networks

1271
00:59:53,982 --> 00:59:56,075
as well as with

1272
00:59:56,075 --> 00:59:59,043
a Monte Carlo Tree Search
algorithm, in order to select

1273
00:59:59,043 --> 01:00:01,475
actions by look ahead search.

1274
01:00:01,475 --> 01:00:04,743
Right, so after putting all this together,

1275
01:00:04,743 --> 01:00:08,853
a value of a node, of
where you are in play,

1276
01:00:08,853 --> 01:00:11,590
and what you do next, is
going to be a combination

1277
01:00:11,590 --> 01:00:13,811
of your value function, as well as

1278
01:00:13,811 --> 01:00:16,552
roll at outcome that you're
computing from standard

1279
01:00:16,552 --> 01:00:19,891
Monte Carlo Tree Search roll outs.

1280
01:00:19,891 --> 01:00:22,891
Okay, so, yeah, so this is basically

1281
01:00:24,203 --> 01:00:27,453
the various, the components of AlphaGo.

1282
01:00:28,397 --> 01:00:30,314
If you're interested in
reading more about this,

1283
01:00:30,314 --> 01:00:33,814
there's a nature paper about this in 2016,

1284
01:00:34,664 --> 01:00:37,656
and they trained this, I think, over,

1285
01:00:37,656 --> 01:00:40,765
the version of AlphaGo
that's being used in these

1286
01:00:40,765 --> 01:00:45,016
matches is, like, I think
a couple thousand CPUs

1287
01:00:45,016 --> 01:00:47,953
plus a couple hundred GPUs,
putting all of this together,

1288
01:00:47,953 --> 01:00:52,120
so it's a huge amount of
training that's going on, right.

1289
01:00:55,659 --> 01:00:57,514
And yeah, so you guys should,

1290
01:00:57,514 --> 01:00:59,681
follow the game this week.

1291
01:01:01,643 --> 01:01:03,491
It's pretty exciting.

1292
01:01:03,491 --> 01:01:07,524
Okay, so in summary,
today we've talked about

1293
01:01:07,524 --> 01:01:10,858
policy gradients, right,
which are general.

1294
01:01:10,858 --> 01:01:13,025
They, you're just directly

1295
01:01:14,456 --> 01:01:18,855
taking gradient descent or
ascent on your policy parameters,

1296
01:01:18,855 --> 01:01:21,942
so this works well for a
large class of problems,

1297
01:01:21,942 --> 01:01:23,947
but it also suffers from high variance,

1298
01:01:23,947 --> 01:01:25,938
so it requires a lot of samples,

1299
01:01:25,938 --> 01:01:28,669
and your challenge here
is sample efficiency.

1300
01:01:28,669 --> 01:01:32,608
We also talked about
Q-learning, which doesn't always

1301
01:01:32,608 --> 01:01:35,349
work, it's harder to
sometimes get it to work

1302
01:01:35,349 --> 01:01:37,536
because of this problem
that we talked earlier where

1303
01:01:37,536 --> 01:01:39,702
you are trying to compute this

1304
01:01:39,702 --> 01:01:42,285
exact state, action value

1305
01:01:43,324 --> 01:01:47,769
for many, for very high
dimensions, but when it does work,

1306
01:01:47,769 --> 01:01:51,342
for problems, for example,
the Atari we saw earlier,

1307
01:01:51,342 --> 01:01:53,092
then it's usually more sample efficient

1308
01:01:53,092 --> 01:01:54,322
than policy gradients.

1309
01:01:54,322 --> 01:01:56,540
Right, and one of the
challenges in Q-learning is that

1310
01:01:56,540 --> 01:01:57,484
you want to make sure that you're

1311
01:01:57,484 --> 01:01:59,902
doing sufficient exploration.

1312
01:01:59,902 --> 01:02:00,735
Yeah?

1313
01:02:00,735 --> 01:02:04,902
[inaudible question from audience]

1314
01:02:14,313 --> 01:02:17,764
Oh, so for Q-learning can
you do this process where

1315
01:02:17,764 --> 01:02:20,684
you're, okay, where you're
trying to start this off by

1316
01:02:20,684 --> 01:02:21,753
some supervised training?

1317
01:02:21,753 --> 01:02:24,924
So, I guess the direct
approach for Q-learning doesn't

1318
01:02:24,924 --> 01:02:27,532
do that because you're
trying to regress to these

1319
01:02:27,532 --> 01:02:29,972
Q-values, right, instead of
policy gradients over this

1320
01:02:29,972 --> 01:02:32,732
distribution, but I think there
are ways in which you can,

1321
01:02:32,732 --> 01:02:34,232
like, massage this

1322
01:02:35,938 --> 01:02:37,985
type of thing to also bootstrap.

1323
01:02:37,985 --> 01:02:40,393
Because I think bootstrapping
in general or like

1324
01:02:40,393 --> 01:02:43,854
behavior cloning is a good way to

1325
01:02:43,854 --> 01:02:46,021
warm start these policies.

1326
01:02:47,454 --> 01:02:50,284
Okay, so, right, so we've
talked about policy gradients

1327
01:02:50,284 --> 01:02:54,213
and Q-learning, and just
another look at some of these,

1328
01:02:54,213 --> 01:02:55,413
some of the guarantees that you have,

1329
01:02:55,413 --> 01:02:56,752
right, with policy gradients.

1330
01:02:56,752 --> 01:02:58,622
One thing we do know
that's really nice is that

1331
01:02:58,622 --> 01:03:02,789
this will always converge to
a local minimum of J of theta,

1332
01:03:04,339 --> 01:03:06,592
because we're just directly
doing gradient ascent,

1333
01:03:06,592 --> 01:03:09,043
and so this is often,

1334
01:03:09,043 --> 01:03:12,131
and this local minimum is
often just pretty good, right.

1335
01:03:12,131 --> 01:03:14,931
And in Q-learning, on the
other hand, we don't have any

1336
01:03:14,931 --> 01:03:17,041
guarantees because here
we're trying to approximate

1337
01:03:17,041 --> 01:03:20,277
this Bellman equation with
a complicated function

1338
01:03:20,277 --> 01:03:23,358
approximator and so, in this
case, this is the problem

1339
01:03:23,358 --> 01:03:25,787
with Q-learning being a
little bit trickier to train

1340
01:03:25,787 --> 01:03:29,954
in terms of applicability
to a wide range of problems.

1341
01:03:31,849 --> 01:03:34,737
Alright, so today you got basically very,

1342
01:03:34,737 --> 01:03:37,907
brief, kind of high-level
overview of reinforcement learning

1343
01:03:37,907 --> 01:03:41,546
and some major classes
of algorithms in RL.

1344
01:03:41,546 --> 01:03:44,419
And next time we're going to have a

1345
01:03:44,419 --> 01:03:47,577
guest lecturer from, Song
Han, who's done a lot

1346
01:03:47,577 --> 01:03:51,276
of pioneering work in model compression

1347
01:03:51,276 --> 01:03:52,569
and energy efficient deep learning,

1348
01:03:52,569 --> 01:03:56,459
and so he's going to talk some
of this, about some of this.

1349
01:03:56,459 --> 01:03:58,459
Thank you.
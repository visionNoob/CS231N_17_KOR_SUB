1
00:00:00,000 --> 00:00:06,000
Translated by visionNoob, KNU
https://github.com/insurgent92/CS231N_17_KOR_SUB

2
00:00:06,971 --> 00:00:10,106
CS231n 두 번째 시간입니다.

3
00:00:10,106 --> 00:00:18,101
화요일의 내용을 복습해보면 컴퓨터비전과, 컴퓨터비전의 역사,
그리고 이 수업의 전반적인 내용에 대해서 다뤘습니다.

4
00:00:18,101 --> 00:00:21,451
그리고 오늘부터 본격적인 수업을 진행하겠습니다.

5
00:00:21,451 --> 00:00:27,864
그리고 앞으로 배우게 될 알고리즘들이 실생활에서
어떻게 적용되는지 심도깊게 살펴 볼 것입니다.

6
00:00:27,864 --> 00:00:35,574
초반 강의들에서는 주로 큰 개념들을 다룰 것이고
나머지 강의들에서는 조금 더 깊은 내용들과

7
00:00:35,574 --> 00:00:39,617
다양한 알고리즘들의 세부 매커니즘을 위주로 다룰 것입니다.

8
00:00:39,617 --> 00:00:43,419
오늘은 이제 첫 학습 알고리즘을 살펴볼 것인데요,
아마도 아주 재밌을 것입니다

9
00:00:43,419 --> 00:00:47,535
일단 진행에 앞서 몇가지 공지사항을 알려드리겠습니다.

10
00:00:47,535 --> 00:00:55,357
첫 번째는 Piazza에 관한 것인데요 어제 확인했을때
Piazza를 가입한 인원이 500명쯤 되었던 것으로 기억합니다.

11
00:00:55,357 --> 00:00:58,493
아직 100여명 가량이 가입을 하지 않았습니다.

12
00:00:58,493 --> 00:01:04,221
Piazza가 학생-조교간의 주요 의사소통 수단이 되었으면 좋겠습니다.

13
00:01:04,221 --> 00:01:12,841
프로젝트 아이디어나, 중간고사, 컨퍼런스와 관련된 질문이
조교들의 메일로 너무 많이 오고 있습니다.

14
00:01:12,841 --> 00:01:16,138
그리고 이런 종류의 질문은 Piazza에서
이루어져야만 합니다.

15
00:01:16,138 --> 00:01:21,568
모든 조교들이 Piazza를  수시로 확인하고 있기 때문에,
Piazza로 질문을 하는것이 훨씬 더 빠를 것입니다.

16
00:01:21,568 --> 00:01:26,234
이메일로 보내면 여러 메일 리스트랑 섞여서
답장을 못 할 수도 있습니다.

17
00:01:26,234 --> 00:01:33,629
또한 SCPD 학생들 중 일부는
Piazza 가입에 어려움을 겪고 있습니다.

18
00:01:33,629 --> 00:01:40,733
SCPD 학생들은 @ stanford.edu 이메일을 부여 받아야 합니다.
이메일 주소를 부여받으면

19
00:01:40,733 --> 00:01:44,276
그때 Stanford 이메일로 Piazaa에
가입할 수 있습니다.

20
00:01:44,276 --> 00:01:50,408
아마 이 문제는 지금 여기 앉아있는 분들은 상관 없겠지만
SCPD로 듣고있는 학생들에게 말씀드리는 것입니다.

21
00:01:52,191 --> 00:01:58,178
다음 사항은 바로 과제에 관한 것입니다.
과제 1이 오늘 늦게 업로드 될 것입니다.

22
00:01:58,178 --> 00:02:03,043
아마 오후가 될 것 같습니다만 오늘 자기전에는
꼭 과제가 올라갈 것임을 약속드립니다.

23
00:02:03,043 --> 00:02:07,304
하지만 여러분이 조금 불안하고
지금 당장 과제를 하고싶다면

24
00:02:07,304 --> 00:02:12,684
과제 1의 작년버전을 찾을 수 있을 것입니다.
작년 버전과 아주 유사합니다.

25
00:02:12,684 --> 00:02:19,082
단지 아주 조금만 손 보고 있습니다. 예를 들어 Python
2.7버전에서 Python3으로 업그레이드하는 등이죠

26
00:02:19,082 --> 00:02:24,742
그리고 아주 작은 외관상의 변화만 있고
내용만 보면 작년과 같습니다.

27
00:02:24,742 --> 00:02:30,686
이번 과제에서 여러분은 여러분만의 k-NN을 구현하게
될 것인데요 우리가 이번 강의에서 다룰 것입니다.

28
00:02:30,686 --> 00:02:37,927
여러분은 또한 몇 가지 선형 분류기를 구현할 것인데 SVM과
Softmax 뿐만 아니라 2-layer 신경망도 구현할 것입니다.

29
00:02:37,927 --> 00:02:46,168
그리고 앞으로 몇 강의동안 그 내용을 모두 다룰 것입니다. 그리고
모든 과제는 Python과 Numpy를 사용하고 있습니다.

30
00:02:46,168 --> 00:02:54,312
Python이나 Numpy에 익숙하지 않다면 관련한 tutorial이
있으며, 여러분은 우리 강의 사이트에서 찾아 볼 수 있습니다.

31
00:02:54,312 --> 00:02:59,211
그리고 이것은 사실 엄청나게 중요합니다. NumPy는 여러분들이
vectorized 연산을 아주 효율적으로 할 수 있게 도와주며

32
00:02:59,211 --> 00:03:03,856
방대한 양의 계산을  코드 몇 줄로
간단히 가능하게 해줍니다.

33
00:03:03,856 --> 00:03:13,424
이건 완전 중요한데, 왜냐하면 대부분의 수치계산, 기계학습과
같은 것들이 vectorized 연산을 수행하기 때문입니다.

34
00:03:13,424 --> 00:03:16,964
그리고 여러분들은 첫 과제를 하면서
많은 연습을 할 수 있을 것입니다.

35
00:03:16,964 --> 00:03:23,449
그러니 Matlab, Numpy와 같은 vectorized
tensor 계산이 익숙하지 않은 분들은

36
00:03:23,449 --> 00:03:32,175
일찌감치 과제를 시작하기를 추천드립니다.
그리고 tutorial도 아주 주의 깊게 읽어보시기 바랍니다.

37
00:03:32,175 --> 00:03:40,198
또 한가지 알려드릴 것은 이 수업에서는
공식적으로 Google Cloud가 지원됩니다.

38
00:03:40,198 --> 00:03:46,694
Google Cloud는 Amazon AWS와 유사합니다
여러분은 cloud에서 가상 머신을 사용할 수 있습니다.

39
00:03:46,694 --> 00:03:50,432
가상머신에는 GPU을 포함 시킬 수도 있습니다.

40
00:03:50,432 --> 00:03:55,386
현재 Googld Cloud 사용법과 이를 사용하여 과제를
수행하는 방법에 관한 Tutorial을 작성중입니다.

41
00:03:55,386 --> 00:04:04,723
여러분들이 이미지만 다운받으면 Google Cloud를 통해
과제를 차질없이 진행할 수 있도록 하겠습니다.

42
00:04:04,723 --> 00:04:08,437
그리고 google에서 우리 수업을
아주 잘 지원하고 있기 때문에

43
00:04:08,437 --> 00:04:15,582
여러분들에게 Google Cloud Credits을 무료로 쓸 수
있는 쿠폰을 나누어 드릴 것입니다.

44
00:04:15,582 --> 00:04:24,015
과제나 프로젝트를 수행할 때 필요한 GPU 같은
고사양 컴퓨터 자원들을 자유롭게 사용할 수 있을 것입니다.

45
00:04:24,015 --> 00:04:29,300
아마도 오늘 Pizzza에 세부사항을 포스팅 하도록 하겠습니다.

46
00:04:29,300 --> 00:04:35,566
본인의 Laptop을 사용해도 되는지와 같은 여러 질문을 받았습니다만

47
00:04:35,566 --> 00:04:41,774
제 답변은, 여러분은 Google Cloud를 사용할 수 있습니다.
조만간 쿠폰을 나눠드릴 예정입니다.

48
00:04:43,923 --> 00:04:53,681
자 그럼 지금까지 몇 가지 공지사항을 전달해 드렸고,
수업을 본격적으로 진행하겠습니다.

49
00:04:53,681 --> 00:04:58,125
지난 강의에서 Image Classification에 대해
간략히 말씀드렸습니다.

50
00:04:58,125 --> 00:05:00,450
Image Classification은 컴퓨터비전 분야에서는
Core Task 에 속합니다.

51
00:05:00,450 --> 00:05:04,964
CS231n 은 주로 Image Classification에
관한 문제를 다룹니다.

52
00:05:04,964 --> 00:05:07,832
Image Classification는 어떻게 할까요?

53
00:05:07,832 --> 00:05:09,750
조금 더 구체적으로 설명해 보겠습니다.

54
00:05:09,750 --> 00:05:14,259
Image Classification을 한다고 하면
우선 입력 이미지를 받습니다.

55
00:05:14,259 --> 00:05:16,457
예시로 여기 귀여운 고양이사진이 한 장 있습니다.

56
00:05:16,457 --> 00:05:22,455
그리고 시스템에는 우리가 미리 정해놓은
카테고리 집합이 있습니다.

57
00:05:22,455 --> 00:05:29,134
카테고리에는 가령 개, 고양이, 트럭, 비행기 등이 있을 수 있습니다.

58
00:05:29,134 --> 00:05:36,444
이제 컴퓨터가 해야 할 일은 이미지을 보고 어떤 카테고리에
속할지 고르는 것입니다. 엄청 쉬워보이죠

59
00:05:36,444 --> 00:05:48,232
왜냐면 여러분의 시각체계는 이와 같은 시각인식 테스크에
고도화 되어 있기 때문입니다. 하지만 기계에계는 정말 어려운 일이죠.

60
00:05:48,232 --> 00:05:53,236
컴퓨터가 이 이미지를 볼때 무엇을 보고 있는지를
조금만 더 깊게 생각해 본다면

61
00:05:53,236 --> 00:05:57,428
우리와는 다르게 컴퓨터는 이미지가
고양이라고 생각하지 못할 것입니다.

62
00:05:57,428 --> 00:06:05,922
컴퓨터에게 이미지는 아주 큰 격자 모양의 숫자집합으로밖에
보이지 않습니다. 가령 800x600 이미지 같이 말입니다.

63
00:06:07,371 --> 00:06:13,176
그리고 각 픽셀은 세 개의 숫자로 표현합니다.
각각은 red, green, blue을 의미합니다.

64
00:06:13,176 --> 00:06:15,769
다시 말하자면, 컴퓨터에게 이미지란
단지 거대한 숫자 집합에 불과합니다.

65
00:06:15,769 --> 00:06:24,841
이 거대한 숫자집합에서 "고양이"를 인식하는
것은 상당히 어려워 보입니다.

66
00:06:26,956 --> 00:06:30,186
이것을 바로 의미적론적인 차이라고 합니다.
(Semantic gap)

67
00:06:30,186 --> 00:06:35,462
"고양이"라는 레이블은 우리가 이 이미지에 붙힌 의미상의 레이블입니다.

68
00:06:35,462 --> 00:06:42,897
이 이미지가 고양이사진이라는 사실과 실제 컴퓨터가 보는
픽셀 값과는 큰 차이가 있습니다.

69
00:06:42,897 --> 00:06:51,916
정말 어려운 테스크입니다. 가령 이 이미지에 아주 미묘한 변화만
주더라도 픽셀 값들은 모조리 변하게 될 것입니다.

70
00:06:51,916 --> 00:06:58,782
우리에게 고양이가 한 마리 있는데 그 고양이가 얌전히
앉아만 있으면 아무 일도 일어나진 않을 것입니다.

71
00:06:58,782 --> 00:07:06,921
하지만 카메라를 아주 조금만 옆으로 옮겨도
모든 픽셀 값들이 모조리 달라질 것입니다.

72
00:07:06,921 --> 00:07:09,431
하지만 이 달라진 픽셀 값도 여전히
고양이라는 사실은 변하지 않습니다.

73
00:07:09,431 --> 00:07:12,754
우리가 만든 알고리즘은
이런 것들에 강인해야 합니다.

74
00:07:12,754 --> 00:07:16,260
바라보는 방향 뿐만 아니라
조명 또한 문제가 될 수 있습니다.

75
00:07:16,260 --> 00:07:19,298
어떤 장면이냐에 따라
조명은 각양각생일 것입니다.

76
00:07:19,298 --> 00:07:28,488
고양이가 어두운 곳에 있던 밝은 곳에 있던 고양이는
고양이죠. 우리 알고리즘은 이것에 또한 강인해야 합니다.

77
00:07:28,488 --> 00:07:30,145
또한 객체 자체에 변형이 있을 수 있습니다.

78
00:07:30,145 --> 00:07:38,940
제 생각엔 고양이만큼 다양한 변형이 가능한 동물은 없을 것
같습니다.고양이는 아주 다양한 자세를 취할 수 있습니다.

79
00:07:38,940 --> 00:07:42,441
우리의 알고리즘은 이런 다양한
변형에도 강인해야 합니다.

80
00:07:43,442 --> 00:07:49,762
가려짐(occlusion)도 문제가 될 수 있습니다.고양이의
일부밖에 볼 수 없는 상황이 있을 수도 있습니다.

81
00:07:49,762 --> 00:07:53,686
고양이의 얼굴밖에 볼 수 없다던가, 극단적인 경우에는
소파에 숨어들어간 고양이의 꼬리밖에 볼 수 없을지도 모릅니다.

82
00:07:53,686 --> 00:08:01,532
사람이라면 고양이라는 사실을 단번에 알아챌 수 있습니다.
여전히 이 이미지는 고양이 이미지라는 것을 알 수 있습니다.

83
00:08:01,532 --> 00:08:08,460
우리 알고리즘도 이것에 강인해야 합니다.
제 생각엔 아주 어려운 문제일 것 같네요.

84
00:08:08,460 --> 00:08:10,792
 Background clutter라는 문제도 있습니다.
(배경과 비슷한 경우)

85
00:08:10,792 --> 00:08:20,389
고양이가 배경과 거의 비슷하게 생겼을 수도 있습니다.
이 문제 또한 우리가 다뤄야만 합니다.

86
00:08:20,389 --> 00:08:23,637
또한 하나의 클래스 내에도 다양성이 존재합니다.

87
00:08:23,637 --> 00:08:28,455
"고양이"라는 하나의 개념으로 모든 고양이의 다양한
모습들을 전부 소화해 내야 합니다.

88
00:08:28,455 --> 00:08:32,158
고양이에 따라 생김새, 크기, 색, 나이가 각양 각색일 것입니다.

89
00:08:32,158 --> 00:08:40,033
우리 알고리즘은 이러한 다양성도 다룰 수 있어야 합니다.
사실 문제들은 엄청나게 어렵습니다.

90
00:08:40,033 --> 00:08:47,931
우리 인간의 뇌는 이런 것들을 아주 잘 하기 때문에
컴퓨터가 얼마나 어려워 할지를 망각할 수도 있습니다.

91
00:08:47,931 --> 00:08:54,288
하지만 만일 우리가 앞서 언급한 일련의 문제들을 전부 다룰 수
있는 프로그램을 원한다면, 그리고 그 프로그램이 단연 고양이 뿐만 아니라

92
00:08:54,288 --> 00:08:59,052
우리가 상상할 수 있는 어떤 객체라도 잘 다룰 수 있어야만 한다면,
하지만 이는 아주 어려운 문제입니다.

93
00:08:59,052 --> 00:09:03,078
제 생각에는 그런 일들을 모두 해내는 것은 기적에 가깝다고 봅니다.

94
00:09:03,078 --> 00:09:09,278
하지만 만약 일부 제한된 상황을 가정한다면, 정말 잘 동작 할 뿐만 아니라,
인간의 정확도와도 맞먹는 프로그램이 존재할 수도 있습니다.

95
00:09:09,278 --> 00:09:14,921
수행 시간도 수백ms 밖에 걸리지 않고 말이죠
상당히 놀랍고 대단한 기술이라고 생각합니다.

96
00:09:14,921 --> 00:09:22,241
앞으로의 수업에서 어떤 요소들이 이를
가능하게 만들었는지 살펴 볼 것입니다.

97
00:09:23,492 --> 00:09:27,565
그러면, 만약 여러분이 Image Classifier API 코드를
작성한다고 생각해 봅시다.

98
00:09:27,565 --> 00:09:34,401
아마도 이런식의 Python 메서드를 작성해 보지 않을까 싶습니다.
이미지 입력을 받아서, 어떤 "놀라운 마법"이 일어나고

99
00:09:34,401 --> 00:09:38,180
 이 이미지가 "고양이"이다. 혹은  "강아지이다" 라고 말해주는 것이죠.

100
00:09:38,180 --> 00:09:41,695
하지만 이를 구현하기 위한 적절한 묘안이
딱히 없으실 것입니다. . 그렇죠?

101
00:09:41,695 --> 00:09:49,535
가령 여러분이 알고리즘 수업에서 "숫자 정렬",
"convex hull 계산" 또는 "RSA 암호화" 문제를 풀어야 한다면

102
00:09:49,535 --> 00:09:55,773
여러분은 알고리즘을 하나씩 써 내려가면서 알고리즘이
동작하려면 필요한 모든 과정들을 나열할 것입니다.

103
00:09:55,773 --> 00:10:07,909
하지만 사물 인식의 경우, 가령 고양이를 인식해야 하는 상황이라면 객체를
그런런 직관적이고 명시적인 알고리즘은 존재하지 않습니다.

104
00:10:07,909 --> 00:10:09,874
아주 어려운 문제입니다.

105
00:10:09,874 --> 00:10:18,869
가령 여러분이 맨 처음 이 함수를 작성하려
할 때 대부분이 문제에 봉착 할 것입니다.

106
00:10:18,869 --> 00:10:27,004
지금까지는 사람들이 동물들을 인식하기 위해
고오급 coded rules을 만들고자 하는 시도를 해왔습니다.

107
00:10:27,004 --> 00:10:35,596
마지막 강의에서 다루겠지만, 고양이를 인식하고자 할 때 우선
고양이는 두개의 귀와 하나의 코가 있다는 것을 알 고 있습니다.

108
00:10:35,596 --> 00:10:41,641
또한 Hubel과 Wiesel의 연구 덕분에
Eedges가 대단히 중요하다는 것도 알 고 있습니다.

109
00:10:41,641 --> 00:10:50,983
시도해볼만한 것은 우선 이미지에서 edges를 계산합니다.
그리고 다양한 Corners와 Edges를 각 카테고리로 분류합니다.

110
00:10:50,983 --> 00:10:53,146
가령 세 개의 선이 만나는 지점이면 corner라고 했을 때

111
00:10:53,146 --> 00:10:56,483
귀는 "여기에 corner 하나" "저기에도 corner 하나"
또 "저기에도 코나 하나" 가 있고

112
00:10:56,483 --> 00:11:01,608
이런 방식으로 고양이 인식을 위해
"명시적인 규칙 집합" 을 써내려 가는 방법입니다.

113
00:11:01,608 --> 00:11:06,127
하지만 이런 방식은 잘 동작하지 않는 것으로 알려져 있습니다.
한가지 문제는, 이런 알고리즘은 강인하지 못합니다.

114
00:11:06,127 --> 00:11:17,081
두번째는, 또 다른 객체를 인식해야 한다면, 트럭에 대해서도, 개에
대해서도 별도로 만들어야 할 것입니다. 처음부터 다시 시작해야 하죠

115
00:11:17,081 --> 00:11:19,853
다시 말해, 이런 방법들은 확장성 전혀 없는 방법입니다.

116
00:11:19,853 --> 00:11:29,360
이 세상에 존재하는 다양한 객체들에게 유연하게 적용 가능한
확장성 있는 알고리즘을 만들어야 합니다.

117
00:11:31,311 --> 00:11:38,092
이런 일들을 가능한 하나의 Insight는 바로
데이터 중심 접근방법(Data-Driven Approcach) 입니다.

118
00:11:38,092 --> 00:11:45,766
고양이는 무엇이다, 물고기는 무엇이다 하면서 손으로
직접 어떤 규칙을 써내려 하는 것 대신에

119
00:11:45,766 --> 00:11:55,402
인터넷에 접속해서 엄청 많은
고양이/비행기/사슴 데이터를 수집합니다.

120
00:11:55,402 --> 00:12:03,866
각 카테고리에서 다양한 많은 데이터들을 모으기 위해
Google Image Search와 같은 도구를 이용하면 됩니다.

121
00:12:03,866 --> 00:12:08,338
사실상 방대한 데이터를 수집하려면
상당히 많은 시간과 노력이 필요합니다.

122
00:12:08,338 --> 00:12:14,105
다행이도 우리가 손쉽게 이용할 수 있는
고퀄의 데이터셋들이 있습니다.

123
00:12:14,105 --> 00:12:20,869
이 데이터셋들을 이용해서 Machine Learning Clssifier
를 학습시킬 수 있습니다.

124
00:12:20,869 --> 00:12:28,446
ML 알고리즘은 어떤 식으로든 데이터를 잘 요약해서는
다양한 객체들을 인식할 수 있는 모델을 만들어냅니다.

125
00:12:28,446 --> 00:12:35,930
그리고 학습 모델로 새로운 이미지를 테스트해 보면
고양이나 개를 잘 인식해 낼 것입니다.

126
00:12:35,930 --> 00:12:38,428
이제 우리의 API 가 조금 변경되었습니다.

127
00:12:38,428 --> 00:12:43,621
입력 이미지를 고양이로 인식하려면 이제는
함수 하나가 아니라 두 개 필요합니다.

128
00:12:43,621 --> 00:12:49,115
하나는 Train함수입니다.
입력은 이미지와 레이블이고, 출력은 우리의 모델입니다.

129
00:12:49,115 --> 00:12:55,276
다른 하나는 Predict 함수입니다.
입력이 모델이고, 출력은 이미지의 예측값이죠

130
00:12:55,276 --> 00:13:01,928
Machine Learning의 key insight입니다.
이 key insight은 지난 10-20년간 아주 잘 동작했습니다.

131
00:13:05,784 --> 00:13:11,111
이 수업에서는 주로 Neural Network, CNN,
Deep Learning을 주로 다룰 것입니다.

132
00:13:11,111 --> 00:13:15,819
하지만 Data-driven approach는 Deep Learning
뿐만 아니라 아주 일반적인 개념입니다.

133
00:13:15,819 --> 00:13:23,058
더 복잡한 알고리즘을 배우기 전에 조금 더
심플한 Classifier를 한번 살펴보고 가는 것이 좋을 것 같습니다.

134
00:13:23,058 --> 00:13:28,907
Nearest neighbor 라는
아주 단순한 Classifier가 있습니다.

135
00:13:28,907 --> 00:13:31,243
NN 알고리즘은 상당히 단순합니다.

136
00:13:31,243 --> 00:13:39,108
Train Step에서는 아무 일도 하지 않습니다.
단지 모든 학습 데이터를 기억합니다.

137
00:13:39,108 --> 00:13:47,964
그리고 Pridict Step에서는 새로운 이미지가 들어오면
새로운 이미지와 기존의 학습 데이터를 비교해서

138
00:13:47,964 --> 00:13:53,169
가장 유사한 이미지로 레이블링을 예측합니다.
아주 간단합니다.

139
00:13:53,169 --> 00:13:58,771
하지만 Data-driven Approach 로서
아주 좋은 알고리즘입니다.

140
00:13:59,977 --> 00:14:04,951
좀 더 구체적으로 살펴봅시다.
앞으로 CIFAR-10 데이터셋을 다뤄야 합니다.

141
00:14:04,951 --> 00:14:09,259
Cifar-10은 Machine Learning에서 자주 쓰는
연습용(테스트용) 데이터셋입니다.

142
00:14:09,259 --> 00:14:11,579
여러분의 과제에서도
Cifar-10을 사용합니다.

143
00:14:11,579 --> 00:14:19,920
CIFAR-10에는 10가지 클래스가 있습니다.
비행기, 자동차, 새, 고양이 등이죠

144
00:14:19,920 --> 00:14:24,990
10가지 각 카테고리가 있고
총 50,000여개의 학습용 이미지가 있습니다.

145
00:14:27,173 --> 00:14:30,250
50,000여개의 데이터는 각 카테고리에
균일하게 분포하고 있죠

146
00:14:30,250 --> 00:14:37,565
그리고 알고리즘 테스트용
10,000여개의 테스트 이미지가 있습니다.

147
00:14:38,707 --> 00:14:45,741
자 그러면 CIFAR-10 데이터셋을 이용한
NN예제를 살펴보겠습니다.

148
00:14:45,741 --> 00:14:53,693
오른쪽 칸의 맨 왼쪽 열은
CIFAR-10 테스트 이미지입니다.

149
00:14:53,693 --> 00:15:03,571
그리고 오른쪽 방향으로는 학습 이미지 중
테스트 이미지와 유사한 순으로 정렬했습니다.

150
00:15:03,571 --> 00:15:10,974
테스트 이미지와 학습 이미지를 비교해 보면, 눈으로 보기에는 상당히
비슷해 보입니다. 항상 맞는것은 아니지만 말입니다. 맞죠?

151
00:15:10,974 --> 00:15:21,224
이 이미지들을 너무 작아서 잘 안보이실 수도 있습니다.
32 x 32 이미지들이라 조금 더 유심히 보고 짐작해 보시기 바랍니다.

152
00:15:21,224 --> 00:15:23,850
어찌됐든, 두 번째 행의 이미지는 개 입니다.
그리고 가장 가까운 이미지(1등)도 개 입니다.

153
00:15:23,850 --> 00:15:30,006
하지만 2등, 3등을 살펴보면 "사슴"이나
"말"같아 보이는 이미지들도 보이는군요

154
00:15:30,006 --> 00:15:36,370
개는 아니지만 눈로 보기에는 아주 비슷해 보입니다.
중간이 흰색 물체가 있는 등 말입니다.

155
00:15:36,370 --> 00:15:42,707
이 이미지에 NN 알고리즘을 적용하면
트레이닝 셋에서 "가장 가까운 샘플"을 찾게 됩니다.

156
00:15:42,707 --> 00:15:47,135
그렇게 찾은 "가장 가까운 샘플"의 레이블을 알 수 있겠죠.
이 샘플들은 "학습 데이터" 이니깐요

157
00:15:47,135 --> 00:15:50,875
그리고 두 번째 행의 테스트 이미지도 "개" 입니다.

158
00:15:50,875 --> 00:16:00,018
NN 알고리즘이 잘 동작하지 않을 것 같아 보이지만
그럼에도 해 볼만한 아주 좋은 예제입니다.

159
00:16:00,939 --> 00:16:06,908
여기에서 중요한 점은 이미지 쌍이 있을 때
어떻게 비교를 할 것인지가 관건입니다.

160
00:16:06,908 --> 00:16:12,165
테스트 이미지 하나를 모든 학습 이미지들과 비교할 때
여러가지 비교 방법들이 있습니다.

161
00:16:12,165 --> 00:16:15,640
정확이 말하자면
"어떤 비교 함수를 사용할지"에 달렸습니다.

162
00:16:15,640 --> 00:16:27,448
앞선 예제에서는 L1 Distance를 사용했습니다. Manhattan
distance라고도 합니다. L1 Dist는 아주 간단한 방법입니다.

163
00:16:27,448 --> 00:16:39,346
이미지를  Pixel-wise로 비교합니다.
가령 4x4 테스트 이미지가 있다고 생각해 봅시다.

164
00:16:39,346 --> 00:16:46,242
테스트/트레이닝 이미지의 같은 자리의 픽셀을
서로 빼고 절댓값을 취합니다.

165
00:16:46,242 --> 00:16:51,950
이렇게 픽셀 간의 차이 값을 계산하고 모든 픽셀의 수행 결과를 모두 더합니다.

166
00:16:51,950 --> 00:16:57,963
Image Classification 문제에서 이 방법이 별로인 것 같지만
연습삼아 해보시면 아주 좋습니다.

167
00:16:57,963 --> 00:17:01,991
"두 이미지간의 차이를 어떻게 측정 할 것인가?"
에 구체적인 방법을 제시합니다.

168
00:17:01,991 --> 00:17:07,147
지금 예제의 경우에는 두 이미지간에
"456" 만큼 차이가 나는군요.

169
00:17:08,446 --> 00:17:16,582
NN Classifier를 구현한 Python 코드는
 상당히 짧고 간결합니다.

170
00:17:16,583 --> 00:17:21,348
NumPy에서 제공하는 Vectorizaed operations을
이용했기 때문이죠

171
00:17:21,348 --> 00:17:28,946
여기 앞서 살펴보았던 Train 함수가 있군요.
NN의 경우 Train 함수가 상당히 단순합니다.

172
00:17:28,946 --> 00:17:33,427
단지 학습 데이터를 기억하는 것입니다.
할 일이 크게 없습니다.

173
00:17:33,427 --> 00:17:39,126
Test 함수에서는 이미지를 입력으로 받고
L1 Distance로 비교합니다.

174
00:17:39,126 --> 00:17:45,395
학습 데이터들 중 테스트 이미지와 가장 유사한 이미지들을 찾아냅니다.

175
00:17:45,395 --> 00:17:53,882
Numpy의 vectorized operations을 활용하면
구현은 Python code 한 두 줄이면 충분합니다.

176
00:17:53,882 --> 00:17:57,779
첫 과제에 있으니 연습해 보시기 바랍니다.

177
00:17:58,628 --> 00:18:02,179
이 Simple Classifier에 대한
몇 가지 궁금증이 생길 수 있습니다.

178
00:18:02,179 --> 00:18:09,077
첫째, Trainset의 이미지가 총 N개라면
Train/Test 함수의 속도는 어떻게 될까요?

179
00:18:12,233 --> 00:18:17,878
우선 Train time은 상수시간 O(1) 일테죠
데이터를 기억만 하면 됩니다.

180
00:18:17,878 --> 00:18:22,703
포인터만 잘 사용해서 복사를 하면, 데이터 크기와 상관없이
상수시간으로 끝마칠 수 있습니다.

181
00:18:22,703 --> 00:18:31,099
하지만 Test time에서는 N개의 학습 데이터 전부를
테스트 이미지와 비교해야만 합니다.

182
00:18:31,099 --> 00:18:33,766
상당히 느린 작업입니다.

183
00:18:34,991 --> 00:18:38,641
상당히  "뒤집어진"  것이죠
(Train TIme < Test  TIme)

184
00:18:38,641 --> 00:18:45,326
실제로 우리는 "Train Time은 조금 느려도 되지만"
"Test Time에서는 빠르게 동작"하길 원합니다

185
00:18:45,326 --> 00:18:49,882
가령 여러분이 데이터 센터에서
어떤 Classifier를 학습시키고 있다고 생각해보면

186
00:18:49,882 --> 00:18:54,640
Classifier의 좋은 성능을 보장하기 위해서
Train Time에 많은 시간을 쏟을 수도 있겠죠.

187
00:18:54,640 --> 00:18:57,566
하지만 이 Classifier의 "Test Time" 을 생각해보면

188
00:18:57,566 --> 00:19:02,248
이 모델이 핸드폰이나, 브라우저와 등
Low Power Device에서 동작해야 할 수도 있습니다.

189
00:19:02,248 --> 00:19:07,075
이런 상황에서는 Classifier가 Test Time에서
어느정도 빠른 성능을 보장해야 할 것입니다.

190
00:19:07,075 --> 00:19:11,826
그런 관점에서 NN 알고리즘은
정 반대의 경우입니다.

191
00:19:11,826 --> 00:19:18,286
 CNN 같은 parametic model들은
NN과는 정 반대입니다.

192
00:19:18,286 --> 00:19:24,936
Train TIme은 엄청 오래 걸릴지 모르나
Test Time은 엄청 빠릅니다.

193
00:19:24,936 --> 00:19:30,816
그렇다면 NN알고리즘을 실제로 적용해 본다면 어떻게 생겼을까요?

194
00:19:30,816 --> 00:19:36,130
 NN의 "decision regions" 을 한번 그려 보았습니다.

195
00:19:36,130 --> 00:19:47,547
2차원 평면 상의 각 점은 학습 데이터 입니다. 그리고
점의 색은 클래스 레이블(카테고리) 입니다.

196
00:19:47,547 --> 00:19:53,921
이 예제에서는 클래스가 5개군요. 왼쪽 구석에는
파란색이 있고 오른쪽 구석에는 보라색이 있습니다.

197
00:19:53,921 --> 00:20:02,560
2차원 평면 내의 모든 좌표에서
각 좌표가 어떤 학습 데이터와 가장 가까운지 계산합니다.

198
00:20:02,560 --> 00:20:06,954
그리고 각 좌표를 해당 클래스로 칠했습니다.

199
00:20:06,954 --> 00:20:14,979
NN 분류기는 공간을 나눠서
각 레이블로 분류합니다.

200
00:20:14,979 --> 00:20:18,320
하지만 이 분류기는
그닥 좋지 않습니다.

201
00:20:18,320 --> 00:20:24,676
이 그림을 보면 NN 분류기에서 발생 가능한
문제들을 살펴볼 수 있습니다.

202
00:20:24,676 --> 00:20:31,591
가령 가운데를 보시면, 대부분이 초록색 점들인데
중간에 노란 점이 끼어있습니다.

203
00:20:31,591 --> 00:20:40,087
NN 알고리즘은 "가장 가까운 이웃" 만을 보기 때문에, 녹색
무리 한 가운데 노란색 영역이 생겨 버립니다. 좋은 일은 아니죠

204
00:20:40,087 --> 00:20:44,081
사실은 노란색이 아닌 초록색 영역이어야만 합니다.

205
00:20:44,081 --> 00:20:50,225
그리고 유사하게 초록색 영역이
파란색 영역을 침범하고 있습니다.

206
00:20:50,225 --> 00:20:55,180
이 또한 초록색 점이 끼어들어서 그렇습니다.
아마 이 점은 잡음(noise)이거나 가짜(spurious)일테죠

207
00:20:55,180 --> 00:21:01,606
이러한 문제들이 발생하기 때문에 NN의 조금 더
일반화된 버전인 k-NN 알고리즘이 탄생하였습니다.

208
00:21:01,606 --> 00:21:08,021
단순하게 가장 가까운 이웃만 찾기보다는
조금 더 고급진 방법을 도입할 것입니다.

209
00:21:08,021 --> 00:21:15,057
Distance metric을 이용해서 가까운 이웃을 K개의 만큼
찾고, 이웃끼리 투표를 하는 방법입니다.

210
00:21:15,057 --> 00:21:18,733
그리고 가장 많은 특표수를 획득한 레이블로 예측합니다.

211
00:21:18,733 --> 00:21:24,148
투표를 하는 다양하고 복잡한 방법들이 있을 수 있습니다.
거리별 가중치를 고려하는 등 말이죠.

212
00:21:24,148 --> 00:21:29,810
하지만 가장 잘 동작하면서도 가장 쉬운 방법은
득표수만 고려하는 방법입니다.

213
00:21:29,810 --> 00:21:35,899
보이는 세 예제는 동일한 데이터를 사용한
k-nn 분류기들입니다.

214
00:21:35,899 --> 00:21:43,792
각각 K=1/3/5 에서의 결과입니다.
우선, K=3 의 경우를 살펴봅시다.

215
00:21:43,792 --> 00:21:50,966
앞서 초록색 영역에 자리 잡았던 노란색 점 때문에
생긴 노란 지역이 깔끔하게 사라졌습니다.

216
00:21:50,966 --> 00:21:55,852
이제는 중앙은 초록색이
깔끔하게 점령했습니다.

217
00:21:55,852 --> 00:22:02,454
그리고 왼쪽의 빨강/파랑 사이의 뾰족한 경계들도 점차
부드러워지고 있습니다. 다수결의 힘이죠.

218
00:22:02,454 --> 00:22:12,064
자 그럼 K=5의 경우를 살펴보겠습니다.
파란/빨간 영역의 경계가 이제는 아주 부드럽고 좋아졌습니다.

219
00:22:12,064 --> 00:22:20,771
대게 NN분류기를 사용하면, K는 적어도 1보다는 큰 값으로 사용합니다.

220
00:22:20,771 --> 00:22:26,064
왜냐하면  K가 1보다 커야 결정 경계가 더 부드러워지고
더 좋은 결과를 보이기 때문입니다.

221
00:22:29,252 --> 00:22:30,159
질문있나요?

222
00:22:30,159 --> 00:22:34,279
[질문하는 학생]

223
00:22:34,279 --> 00:22:38,133
질문은 "레이블링이 안된 흰색 지역은 어떻게 처리하는지?" 입니다.

224
00:22:38,133 --> 00:22:45,521
흰색 영역은 k-nn이 "대다수"를 결정할 수 없는 지역입니다.
물론 흰색 영역을 매꿀 수 있는 더 좋은 방법들도 있습니다.

225
00:22:45,521 --> 00:22:50,472
어떤 식으로든 추론을 해보거나, 임의로 정할 수도 있습니다.

226
00:22:50,472 --> 00:22:55,668
하지만 여기에서는 단순한 예제라서 가장 가까운 이웃이
존재하지 않으면 단순하게 흰색으로 칠했습니다.

227
00:23:00,005 --> 00:23:06,616
여러분이 Computer Vision을 공부하는 동안에
다양한 관점을 유연하게 다루는 능력은 매우 유용합니다.

228
00:23:06,616 --> 00:23:13,049
그 중 하나는 바로 이미지를 고차원 공간에 존재하는 하나의 점이라고
생각하는 것입니다. 반대로 이미지를 이미지 자체로 볼 수도 있겠죠

229
00:23:13,049 --> 00:23:23,395
이미지의 픽셀들을 하나의 고차원 벡터로 생각하는 관점이죠.
이 두 관점을 자유롭게 오갈 수 있는 능력은 아주 유용합니다.

230
00:23:23,395 --> 00:23:29,443
이미지를 다루는 문제에서 k-nn을 사용하는
전략은 그닥 좋은 방법이 아닙니다.

231
00:23:29,443 --> 00:23:38,288
잘 분류되었는지 아닌지를 초록/빨간색으로 표기했습니다.
성능이 별로 안 좋습니다.

232
00:23:38,288 --> 00:23:47,264
K값을 높히면 어떻까요? 가장 가까운 이웃 뿐만 아니라
Top-3/Top-5 혹은 모든 행(Row)을 사용하면 어떨까요?

233
00:23:47,264 --> 00:23:55,325
더 많은 이웃들이 투표에 참여하면 각종 잡음들에
조금 더 강인해 질 것임을 추측할 수 있습니다.

234
00:23:57,070 --> 00:24:05,727
k-nn을 사용할 때 결정해야 할 한 가지 사항이 더 있습니다.
바로 서로 다른 점들을 어떻게 비교할 것인지 입니다.

235
00:24:05,727 --> 00:24:15,126
지금까지는 L1 Distance을 이용했습니다.
"픽셀 간 차이 절대값의 합" 입니다.

236
00:24:15,126 --> 00:24:24,491
하지만 L2, 즉 Euclidean distance를 사용해도 되겠죠
"제곱 합의 제곱근"을 거리로 이용하는 방법입니다.

237
00:24:24,491 --> 00:24:28,727
어떤 "거리 척도(distance metric)" 을 선택할지는
아주 흥미로운 주제입니다.

238
00:24:28,727 --> 00:24:35,386
왜냐하면 서로 다른 척도에서는 해당 공간의
근본적인 기하학적 구조 자체가 서로 다르기 떄문입니다.

239
00:24:35,386 --> 00:24:45,020
왼쪽에 보이는 사각형은 사실 L1 Distance의 관점에서는 원입니다.
생긴 모습은 원점을 기준으로 하는 사각형의 모양이죠

240
00:24:45,020 --> 00:24:51,017
 L1의 관점에서는 사각형 위의 점들이 모두
원점으로부터 동일한 거리만큼 떨어져 있습니다.

241
00:24:51,017 --> 00:24:57,241
반면 L2, Euclidean distance 의 관점에서는 원입니다.
우리가 예상했던 바로 그 원입니다.

242
00:24:57,241 --> 00:25:05,135
이 두 가지 거리 척도간에는 아주 흥미로운 차이점이 있습니다.
L1은 어떤 좌표 시스템이냐에 따라 많은 영향을 받습니다.

243
00:25:05,135 --> 00:25:10,201
가령 기존의 좌표계를 회전시키면
L1 distance가 변합니다.

244
00:25:10,201 --> 00:25:18,281
반면 L2 Distance의 경우에는 좌표계와 아무 연관이 없습니다.

245
00:25:18,281 --> 00:25:24,791
만약 특징 벡터의 각각 요소들이 개별적인 의미를
가지고 있다면(e.g. 키 몸무게)

246
00:25:24,791 --> 00:25:27,935
L1 Distance가 더 잘 어울릴 수도 있습니다.

247
00:25:27,935 --> 00:25:37,531
하지만 특징 벡터가 일반적인 벡터이고, 요소들간의 실질적인 의미를 잘 모르는
경우라면, 아마도 L2 Distance가 조금은 더 잘 어울릴 수 있습니다.

248
00:25:37,531 --> 00:25:46,343
여기에서 주목할 점은 k-nn에 다양한 거리 척도를 적용하면
k-nn으로 다양한 종류의 데이터를 다룰 수 있다는 점입니다.

249
00:25:46,343 --> 00:25:51,410
벡터나 이미지 외에도 말이죠. 가령 문장을 분류하는 문제가 있다고 해봅시다.

250
00:25:51,410 --> 00:25:57,716
k-nn 분류기로 이 문제를 다루려면
어떤 거리 척도를 사용할지만 정해주면 됩니다.

251
00:25:57,716 --> 00:26:03,831
두 문장 간의 거리를 측정할 수 있는 어떤 것이든 사용하면 됩니다.

252
00:26:03,831 --> 00:26:12,701
거리 척도만 정해주면 어떤 종류의 데이터도 다룰 수 있습니다.

253
00:26:12,701 --> 00:26:20,283
k-nn은 아주 단순한 알고리즘입니다. 하지만 새로운 문제를
접했을 때  간단히 시도해 볼만한 아주 좋은 알고리즘입니다.

254
00:26:21,805 --> 00:26:28,441
자 그러면 어떤 거리 척도를 사용하는지에 따라서
실제 기하학적으로 어떻게 변하는지 알아봅시다.

255
00:26:28,441 --> 00:26:38,087
양 쪽 모두 동일한 데이터입니다. 다만 왼쪽은 L1 Distance를
오른쪽은 L2 Distance를 사용했습니다.

256
00:26:38,087 --> 00:26:44,093
결과를 보시면 거리 척도에 따라서 결정 경계의 모양
자체가 달라짐을 알 수 있습니다.

257
00:26:44,093 --> 00:26:49,337
왼쪽의 L1 Distance를 살펴보면 결정 경계가
"좌표 축"에 영향을 받는 경향을 알 수 있습니다.

258
00:26:49,337 --> 00:26:53,451
L1 Distance가 좌표 시스템의 영향을 받기 때문입니다.

259
00:26:53,451 --> 00:27:00,294
반면 L2 Distance는 좌표 축의 영향을 받지 않고 결정
경계를 만들기 때문에 조금 더 자연스럽습니다.

260
00:27:04,161 --> 00:27:14,618
지금까지의 모든 예제는 저의 웹 데모사이트에서 가져왔습니다.
여러분은 여기에서 여러분만의 k-nn 분류기를 설계할 수 있습니다.

261
00:27:14,618 --> 00:27:23,854
이 프로텍터 스크린으로 보여드리기가 참 어렵군요
집에가서 한번 해보시기 바랍니다.

262
00:28:07,103 --> 00:28:09,679
좋습니다 말썽이 좀 있었습니다

263
00:28:09,679 --> 00:28:13,496
오늘은 웹 데모를 생략하겠습니다만
꼭 한번 해보시기 바랍니다.

264
00:28:13,496 --> 00:28:26,029
상당히 재미있습니다. K와  리 척도를 바꿔보면서 어떻게
결정 경계가 만들어지는지에 대한 직관을 얻으시길 바랍니다.

265
00:28:30,641 --> 00:28:37,178
여러분이 k-nn을 사용하려고 한다면
반드시 선택해야 하는 몇 가지 항목이 있습니다.

266
00:28:37,178 --> 00:28:41,520
앞서 K에 대해서 이야기했습니다.
L1/L2와 같은 거리 척도 또한 다뤘죠

267
00:28:41,520 --> 00:28:47,286
그렇다면 어떻게 하면 "내 문제"와 "데이터"에
꼭 맞는 모델을 찾을 수 있을까요?

268
00:28:47,286 --> 00:28:53,937
K와 거리척도를 "하이퍼 파라미터" 라고 합니다.

269
00:28:53,937 --> 00:29:01,313
하이퍼파라미터는 Train time에 학습하는 것이 아니므로
여러분이 학습 전 사전에 반드시 선택해야만 합니다.

270
00:29:01,313 --> 00:29:05,623
데이터로 직접 학습시킬 방법이 없습니다.

271
00:29:05,623 --> 00:29:10,260
그렇다면 하이퍼파라미터를 어떻게 정해야 할까요?

272
00:29:10,260 --> 00:29:12,277
하이퍼 파라미터를 정하는 일은
문제의존적(problem-dependent)입니다.

273
00:29:12,277 --> 00:29:20,950
가장 간단한 방법은 데이터에 맞게 다양한
하이퍼파라미터 값을 시도해 보고 가장 좋은 값을 찾습니다.

274
00:29:20,950 --> 00:29:22,404
질문있나요?

275
00:29:22,404 --> 00:29:26,071
[질문하는 학생]

276
00:29:29,589 --> 00:29:34,447
질문은 "어떤 경우에 L1 Distance가
L2 Distance보다 더 좋은지" 입니다.

277
00:29:34,447 --> 00:29:36,800
그것은 문제의존적(problem-dependent)입니다.

278
00:29:36,800 --> 00:29:41,204
어떤 경우에  L1/L2를 써야 하는지
결정하는 것은 어렵겠지만

279
00:29:41,204 --> 00:29:50,185
 L1은 좌표계에 의존적이므로 여러분의 데이터가
좌표계에 의존적인지를 판단하는 것이 판단 기준이 될 수 있습니다.

280
00:29:50,185 --> 00:29:55,513
여러분에게 어떤 특징 벡터가 있고 각 요소가
어떤 특별한 의미를 지니고 있다면

281
00:29:55,513 --> 00:30:03,976
가령 직원들을 분류하는 문제가 있을 때, 데이터의 각 요소가
직원들의 다양한 특징에 영향을 줄 수 있습니다.

282
00:30:03,976 --> 00:30:08,778
가령 봉급, 근속년수와 같은 예가 될 수 있겠습니다.

283
00:30:08,778 --> 00:30:15,850
이처럼 각 요소가 특별한 의미를 가지고 있다면
L1 을 사용하는것이 좀 더 괜찮을 지도 모릅니다.

284
00:30:15,850 --> 00:30:19,989
하지만 일반적으로는 하이퍼파라미터 선택은
어떤 문제와 데이터인지에 의존적입니다.

285
00:30:19,989 --> 00:30:24,381
하이퍼파라미터는 단지 여러가지 시도를 해보고
좋은 것을 선택하는 것이 좋습니다.

286
00:30:28,381 --> 00:30:34,238
하지만 하이퍼파라미터 값들을 실험해 보는 작업도 다양합니다.

287
00:30:34,238 --> 00:30:38,268
"다양한 하이퍼 파라미터를 시도해 보는 것" 과
"그중 최고를 선택하는 것" 이 무슨 뜻일까요?

288
00:30:38,268 --> 00:30:42,911
가장 먼저 떠올릴 수 있는 아이디어는
아주 단순합니다.

289
00:30:42,911 --> 00:30:47,691
"학습데이터의 정확도와 성능"를 최대화하는
하이퍼파라미터를 선택하는 것이죠.

290
00:30:47,691 --> 00:30:52,137
사실 정말 끔찍한 방법입니다.
절대로 이렇게 해서는 안됩니다.

291
00:30:52,137 --> 00:31:00,157
가령 NN 분류기의 경우 K = 1 일 때
학습 데이터를 가장 완벽하게 분류합니다.

292
00:31:01,124 --> 00:31:04,420
앞서 말씀드린 전략대로라면(트레이닝 데이터의 정확도를 올리는)
항상 K = 1 일 때가 최고입니다.

293
00:31:04,420 --> 00:31:13,184
하지만 앞선 예제에서도 보았듯이, 실제로는 K를 더 큰 값으로 선택하는 것이
학습 데이터에서는 몇 개 잘못 분류할 수는 있지만

294
00:31:13,184 --> 00:31:17,915
학습 데이터에 없던 데이터에 대해서는
더 좋은 성능을 보일 수 있습니다.

295
00:31:17,915 --> 00:31:24,463
궁극적으로 기계학습에서는 학습 데이터를 얼마나 잘 맞추는지는
중요하지 않습니다. 우리가 학습시킨 분류기가

296
00:31:24,463 --> 00:31:27,051
한번도 보지 못한 데이터를 얼마나 잘 예측하는지가 중요하죠

297
00:31:27,051 --> 00:31:30,495
그러므로 학습 데이터에만 신경쓰는 것은 최악입니다.
비추입니다.

298
00:31:30,495 --> 00:31:38,390
또 다른 아이디어가 있습니다. 전체 데이터셋 중 학습 데이터를 쪼개서
일부를 테스트 데이터로 사용하는 것입니다.

299
00:31:38,390 --> 00:31:53,716
학습 데이터로 다양한 하이퍼파라미터 값들을 학습을 시키고 테스트
데이터에 적용시켜본 다음, 하이퍼파라미터를 선택하는 방법이죠

300
00:31:54,582 --> 00:32:01,087
이 방법이 조금 더 합리적인 것 같지만 사실은,
이 방법 또한 아주 끔찍한 방법입니다. 절대 하면 안됩니다.

301
00:32:01,087 --> 00:32:06,515
다시한번 기계학습의 궁극적인 목적을 말씀드리자면
한번도 보지 못한 데이터에서 잘 동작해야 합니다.

302
00:32:06,515 --> 00:32:14,523
테스트셋으로는 한번도 보지 못했던 데이터에서의
알고리즘의 성능을 측정할 수 있어야 합니다.

303
00:32:14,523 --> 00:32:23,363
그런데 만약 학습시킨 모델들 중
테스트 데이터에 가장 잘 맞는 모델을 선택한다면

304
00:32:23,363 --> 00:32:31,663
우리는 그저 "테스트 셋에서만"  잘 동작하는
하이퍼파라미터를 고른 것일 수 있습니다.

305
00:32:31,663 --> 00:32:38,280
그렇게 되면,  더이상 테스트 셋에서의 성능은 한번도 보지못한
데이터에서의 성능을 대표할 수는 없습니다.

306
00:32:38,280 --> 00:32:44,672
그러니 이 또한 하지 말아야 합니다. 좋은 생각이 아닙니다.
그렇게 하면 곤경에 빠질 것입니다.

307
00:32:44,672 --> 00:32:49,192
훨씬 더 일반적인 방법은 데이터를 세 개로 나누는 것입니다.

308
00:32:50,185 --> 00:32:57,305
데이터의 대부분은 트레이닝 셋으로 나누고, 일부는
밸리데이션 셋, 그리고 나머지는 테스트 셋으로 나눕니다

309
00:32:57,305 --> 00:33:03,500
그리고 다양한 하이퍼파라미터로 "트레이닝 셋" 을 학습시킵니다.

310
00:33:03,500 --> 00:33:11,621
그리고 "벨리데이션 셋" 으로 검증을 합니다. 그리고 벨리데이션 셋에서
가장 좋았던 하이퍼파라미터를 선택합니다.

311
00:33:11,621 --> 00:33:16,627
그리고 최종적으로 개발/디버깅 등 모든 일들을 다 마친 후에

312
00:33:16,627 --> 00:33:23,401
 벨리데이션 셋에서 가장 좋았던 분류기를 가지고
테스트 셋에서는  "오로지 한번만" 수행합니다.

313
00:33:23,401 --> 00:33:27,139
이 마지막 수치가 여러분의 논문과 보고서에 에 삽입될 것입니다.

314
00:33:27,139 --> 00:33:32,393
그 숫자가 여러분의 알고리즘이 한번도 보지 못한 데이터에
얼마나 잘 동작해 주는지를 실질적으로 말해줄 수 있는 것입니다.

315
00:33:32,393 --> 00:33:38,847
그리고 실제로 벨리데이션 데이터와 테스트 데이터를
엄격하게 나눠놓는 것은 상당히 중요합니다.

316
00:33:38,847 --> 00:33:45,653
가령, 우리는 연구 논문을 작성할때
테스트 셋을 거의 마지막 쯤에야 한번 사용합니다.

317
00:33:45,653 --> 00:33:54,159
저는 논문을 쓸때 마감 일주일 전 부터만
테스트 셋을 사용합니다.

318
00:33:54,159 --> 00:34:00,102
우리는 정직하게 연구를 수행했고 논문의 수치를 공정하게 측정했다는
것을 보장하기 위해서죠. 상당히 중요합니다.

319
00:34:00,102 --> 00:34:03,883
여러분은 여러분의 테스트 데이터를 잘 통제해야만 합니다.

320
00:34:06,468 --> 00:34:10,840
또 다른 하이퍼파라미터 선택 전략은
크로스 벨리데이션(교차 검증) 입니다.

321
00:34:10,840 --> 00:34:17,317
사실 이 방법은 작은 데이터셋일 경우 많이 사용하고
딥러닝에서는 많이 사용하진 않습니다.

322
00:34:17,317 --> 00:34:25,534
이 아이디어는, 우선 테스트 데이터를 정해놓습니다.
테스트 데이터는 아주 마지막에만 사용할 것입니다.

323
00:34:25,534 --> 00:34:35,515
그리고 나머지 데이터는 트레이닝/벨리데이션 으로 딱 나눠 놓는 대신에
트레이닝 데이터를 여러 부분으로 나눠줍니다.

324
00:34:35,516 --> 00:34:41,415
이런 식으로 번갈아가면서 벨리데이션 셋을 지정해 줍니다.

325
00:34:41,415 --> 00:34:45,498
이 예제에서는 5-Fold Cross Validation을 사용하고 있습니다.

326
00:34:45,498 --> 00:34:51,928
처음 4개의 fold에서 하이퍼 파라미터를 학습시키고
남은 한 fold에서 알고리즘을 평가합니다.

327
00:34:51,928 --> 00:35:00,293
그리고 1,2,3,5 fold에서 다시 학습시키고 4 fold로 평가합니다.
이런식으로 계속 순환합니다.

328
00:35:00,293 --> 00:35:07,511
이런 방식으로 최적의 하이퍼파라미터를 확인할 수 있을 것입니다.

329
00:35:07,511 --> 00:35:18,652
이런 방식은 거의 표준이긴 하지만 실제로는 딥러닝같은 큰 모델을 학습시킬
때는 학습 자체가 계산량이 많기 때문에 실제로는 잘 쓰지 않습니다.

330
00:35:18,652 --> 00:35:19,519
질문 있나요?

331
00:35:19,519 --> 00:35:23,186
[질문하는 학생]

332
00:35:29,515 --> 00:35:35,728
질문은 "구체적으로 트레이닝 셋과 벨리데이션 셋의 차이가 무엇인지" 입니다.

333
00:35:35,728 --> 00:35:46,974
k-NN의 예를 들어보자면 트레이닝 셋은
우리가 레이블을 기억하고 있는 이미지들 입니다.

334
00:35:46,974 --> 00:35:52,451
어떤 이미지를 분류하려면 트레이닝 데이터의
모든 이미지들과 비교하게 되겠죠

335
00:35:52,451 --> 00:36:03,184
그리고 가장 근접한 레이블을 선택합니다.
알고리즘은 트레이닝 셋 자체를 기억할 것입니다.

336
00:36:03,184 --> 00:36:08,062
이제는 벨리데이션 셋을 가져와서
트레이닝 셋과 비교합니다.

337
00:36:08,062 --> 00:36:16,815
그리고 이를 통해서 벨리데이션 셋에서는 분류기가
얼마만큼의 정확도가 나오는지 확인합니다.

338
00:36:16,815 --> 00:36:28,473
이것이 바로 트레이닝/벨리데이션 셋의 차이점 입니다. 트레이닝 셋의
레이블을 볼 수 있지만 벨리데이션 셋의 레이블은 볼 수 없습니다.

339
00:36:28,473 --> 00:36:34,043
벨리데이션 셋의 레이블은 알고리즘이
얼마나 잘 동작하는지를 확인할 때만 사용합니다.

340
00:36:34,043 --> 00:36:34,907
질문있나요?

341
00:36:34,907 --> 00:36:38,574
[학생이 질문]

342
00:36:44,373 --> 00:36:55,955
질문은 "테스트 셋이 한번도 보지 못한 데이터를 대표할 수 있는지" 입니다.
이것은 실제도 문제가 될 수 있습니다

343
00:36:55,955 --> 00:37:01,863
기본적인 통계학적 가정이 하나 있는데 여러분의 데이터는 독립적이며,
유일한 하나의 분포에서 나온다는 가정입니다 (i.i.d assumption)

344
00:37:01,863 --> 00:37:12,948
그러니 모든 데이터는 동일한 분포를 따른다고 생각해야 합니다.
물론 실제로는 그렇지 않은 경우가 많습니다.

345
00:37:12,948 --> 00:37:20,951
테스트 셋이 한번도 보지 못한 데이터를 잘 표현하지
못하는 경우를 경험하게 되실 것입니다.

346
00:37:20,951 --> 00:37:25,473
그리고 이런 류의 문제는 datasets crators와
dataset curators가 생각해 볼 문제입니다.

347
00:37:25,473 --> 00:37:28,626
하지만,가령 제가 데이터 셋을 만들때
하는 한가지 일은

348
00:37:28,626 --> 00:37:34,252
데이터를 수집할 때, 일관된 방법론을 가지고
대량의 데이터를 한번에 수집하는 전략을 사용합니다.

349
00:37:34,252 --> 00:37:38,690
그다음에 무작위로 트레이닝 데이터와
테스트 데이터를 나줘줍니다.

350
00:37:38,690 --> 00:37:43,024
한가지 주의해야 할 점은
데이터를 지속적으로 모으고 있는 경우입니다.

351
00:37:43,024 --> 00:37:51,613
먼저 수집한 데이터들을 트레이닝 데이터로 쓰고, 이후에 모은 데이터를 테스트
데이터로 사용한다면 문제가 될 수 있습니다.

352
00:37:51,613 --> 00:37:59,762
대신에 데이터셋 전체를 무작위로 섞어서 데이터셋을 나누는 것이
그 문제를 완화 시킬 수 있는 한가지 방법일 수 있습니다.

353
00:38:04,297 --> 00:38:26,961
크로스 벨리데이션을 수행하고 나면 다음과 같은 그래프를 보실 수 있습니다.
X축은 K-NN의 K입니다. 그리고 Y축은 분류 정확도입니다.

354
00:38:26,961 --> 00:38:31,705
이 경우에는 5-fold 크로스 벨리데이션을 수행하였습니다.

355
00:38:31,705 --> 00:38:38,346
각 K마다 5번의 크로스 벨리데이션을 통해
알고리즘이 얼마나 잘 동작하는지를 알려줍니다.

356
00:38:38,346 --> 00:38:50,276
그리고 "테스트셋이 알고리즘 성능 향상에 미치는 영향" 를 알아보려면
 K fold 크로스벨리데이션이 도움을 줄 수 있습니다.

357
00:38:50,276 --> 00:38:57,606
여러 validation folds 별 성능의 분산(variance)을
고려해 볼 수 있습니다.

358
00:38:57,606 --> 00:39:04,301
분산을 같이 계산하게 되면, 어떤 하이퍼파라미터가 가장
좋은지 뿐만 아니라, 그 성능의 분산도 알 수 있습니다.

359
00:39:04,301 --> 00:39:08,151
여러분이 기계학습 모델을 학습시키는 경우에
보통 이런 모습의 그래프를 그리게 될 것입니다.

360
00:39:08,151 --> 00:39:12,380
하이퍼파라미터에 따라 모댈의 정확도와
성능을 평가할 수 있습니다.

361
00:39:12,380 --> 00:39:19,995
그리고 벨리데이션 셋의 성능이 최대인
하이퍼 파라미터를 선택하게 될 것입니다.

362
00:39:19,995 --> 00:39:25,528
이 예제에서는 아마도 K = 7 일 경우에 가장 좋은 성능을 내는군요.

363
00:39:29,713 --> 00:39:34,458
하지만 실제로는 입력이 이미지인 경우에는
k-NN 분류기를 잘 사용하지 않습니다.

364
00:39:34,458 --> 00:39:38,233
앞서 이야기한 문제들 때문이죠.

365
00:39:38,233 --> 00:39:44,287
우선 한 가지 문제점은 k-nn이 너무 느리다는 것입니다. 우리가
원하는 것과 정 반대이며 이 내용은 앞서 이야기한 적이 있었습니다.

366
00:39:44,287 --> 00:39:54,495
또 하나의 문제는 L1/L2 Distance가 이미지간의 거리를
측정하기에 적절하지 않다는 점입니다.

367
00:39:54,495 --> 00:40:01,254
이 벡터간의 거리 측정 관련 함수들은(L1/L2)
이미지들 간의 "지각적 유사성"을 측정하는 척도로는 적절하지 않습니다.

368
00:40:01,254 --> 00:40:08,796
우리들은 이미지간의 차이를 어떻게 지각하는 것일까요?
가령 여기 왼쪽에 한 여성이 있습니다.

369
00:40:08,796 --> 00:40:11,234
오른쪽에는 세 개의 왜곡된 이미지가 있습니다.

370
00:40:11,234 --> 00:40:18,416
눈과 입을 가려도 보고, 몇 픽셀식 이동도 시켜보고,
전체 이미지에 파란색 색조도 추가시켜 보았습니다.

371
00:40:18,416 --> 00:40:25,425
그리고 각 이미지와 원본의 사이의 
Euclidean Distance를 측정해보면

372
00:40:25,425 --> 00:40:29,469
이들은 모두 동일한 L2 Distance를 가집니다.
좋지 않은 현상이죠

373
00:40:29,469 --> 00:40:39,119
이는 L2 Distance가 이미지들 간의 "지각적 유사도" 를
측정하기에는 적합하지 않다는 의미이기 때문입니다.

374
00:40:40,642 --> 00:40:46,819
K-NN의 또 다른 문제 중 하나는 바로
"차원의 저주" 입니다.

375
00:40:46,819 --> 00:40:51,279
K-NN을 다시한번 살펴보자면

376
00:40:51,279 --> 00:40:57,186
K-NN가 하는 일은 트레이닝 데이터를 이용해서
공간을 분할하는 일이였습니다.

377
00:40:57,186 --> 00:41:05,646
이는 K-NN이 잘 동작하려면 전체 공간을 조밀하게 커버할 만큼의 
충분한 트레이닝 샘플이 필요하다는 것을 의미합니다.

378
00:41:05,646 --> 00:41:15,004
그렇지 않다면 이웃이 사실은 엄청 멀 수도 있고 그렇게 되면 
테스트 이미지을 제대로 분류할 수 없을 것입니다.

379
00:41:16,738 --> 00:41:24,666
공간을 조밀하게 덮으려면 충분한 량의 학습 데이터가 필요하고
그 양은 차원이 증가함에 따라 기하급수 적으로 증가합니다.

380
00:41:24,666 --> 00:41:29,217
아주 좋지 않은 현상입니다. 
기하급수적인 증가는 언제나 옳지 못합니다.

381
00:41:29,217 --> 00:41:35,587
고차원의 이미지라면 모든 공간을 조밀하게 메울만큼의 
데이터를 모으는 일은 현실적으로 불가능합니다.

382
00:41:35,587 --> 00:41:41,300
K-NN을 사용할 시 여러분은 항상 이 점을  염두해야 합니다.

383
00:41:41,300 --> 00:41:45,701
요약을 해보자면 이미지 분류가 무엇인지 설명하기 위해 
K-NN 예제를 들어보았습니다.

384
00:41:45,701 --> 00:41:51,445
"이미지"와 "정답 레이블"이 있는 트레이닝 셋이 있었고
테스트 셋을 예측하는데 이용하였습니다.

385
00:41:51,445 --> 00:41:52,278
질문 있나요?

386
00:41:52,278 --> 00:41:54,519
[질문하는 학생]

387
00:41:54,519 --> 00:41:57,271
오 죄송합니다. 질문은
"이전 슬라이드의 그림이 어떤걸 의미하는지"

388
00:41:57,271 --> 00:41:59,168
"그림의 초록 점과 파란 점은 무엇인지" 입니다.

389
00:41:59,168 --> 00:42:05,596
각 점은 트레이닝 샘플들을 의미합니다.
점 하나하나가 트레이닝 샘플입니다.

390
00:42:05,596 --> 00:42:11,004
그리고 각 점의 색은 트레이닝 샘플이 속한
카테고리를 나타낸다고 보시면 됩니다.

391
00:42:11,004 --> 00:42:17,525
가령 맨 왼쪽의 1차원을 보시면 
이 공간을 조밀하게 덮으려면 트레이닝 샘플 4개면 충분합니다.

392
00:42:17,525 --> 00:42:25,529
2차원 공간을 다 덮으려면 16개가 필요합니다.
1차원의 4배 입니다.

393
00:42:25,529 --> 00:42:28,767
이렇게 3, 4, 5 차원 같이 고차원을 고려해보면

394
00:42:28,767 --> 00:42:35,200
각 공간을 조밀하게 덮기 위해 필요한 트레이닝 샘플의 수는
차원이 늘어남에 따라 기하급수적으로 증가합니다.

395
00:42:35,200 --> 00:42:40,501
그리고 가령 2차원 공간에서는 커브모양이 있습니다.

396
00:42:40,501 --> 00:42:47,641
혹은 더 높은 차원에서는 일종의 
샘플들의 manifolds가 있을 수 있습니다.

397
00:42:47,641 --> 00:42:53,029
하지만 K-NN 알고리즘은 샘플들의 manifolds를
가정하지 않기 때문에

398
00:42:53,029 --> 00:42:58,796
K-NN이 제대로 동작할 수 있는 유일한 방법은 공간을 조밀하게 
덮을 만큼 충분히 많은 트레이닝 샘플을 가지는 것입니다.

399
00:43:01,741 --> 00:43:04,915
지금까지 K-NN을 살펴보았습니다.

400
00:43:04,915 --> 00:43:11,214
첫 과제에 K-NN 실습이 있으니 실제로 
구현해 볼 기회가 있을 것입니다.

401
00:43:11,214 --> 00:43:16,059
K-NN에 관현한 질문을 받고 다음 주제로 넘어 가겠습니다.

402
00:43:16,059 --> 00:43:16,892
질문있나요?

403
00:43:16,892 --> 00:43:21,014
[학생이 질문]

404
00:43:21,014 --> 00:43:22,034
죄송하지만 다시 말해주세요

405
00:43:22,034 --> 00:43:25,951
[학생이 질문]

406
00:43:28,437 --> 00:43:32,033
질문은 "왜 저 이미지들의 L2 Distance가 
같은지" 입니다.

407
00:43:32,033 --> 00:43:35,915
제가 이 이미지들이 같은 L2 Distance를 가지도록
임의로 만들어냈기 때문입니다.

408
00:43:35,915 --> 00:43:38,716
[웃음]

409
00:43:38,716 --> 00:43:45,096
저는 단지 L2 Distance가 이미지간의 유사도을 측정하는데는
좋지 않다는 점을 강조하고 싶었습니다.

410
00:43:45,096 --> 00:43:50,223
실제로 이 이미지들은 저마다 모두 다릅니다.

411
00:43:52,470 --> 00:43:57,649
하지만 여러분이 K-NN을 사용한다면 이미지 간의
유사도를 특정할 수 있는 유일한 방법은

412
00:43:57,649 --> 00:44:00,236
바로 이 단일 거리 성능 척도(L1/L2 등)을 이용하는 수 밖에 없습니다.

413
00:44:00,236 --> 00:44:08,949
이 예시는 여러분들에게 Distance metric이 실제로는
이미지간의 유사도를 잘 포착해 내지 못한다는 것을 알려줍니다.

414
00:44:08,949 --> 00:44:15,384
이 예시에 경우에 이러한 translation과 offset에도
Distance가 일치하도록 제가 임의로 만들어낸 것입니다.

415
00:44:15,384 --> 00:44:16,217
질문 있으십니까?

416
00:44:16,217 --> 00:44:19,884
[학생이 질문]

417
00:44:28,672 --> 00:44:36,615
질문은 "이미지들이 모두 같은 사람이므로 
distance가 같으면 좋은 것 아닌지"입니다.

418
00:44:36,615 --> 00:44:41,810
이 예시에서는 그럴 수도 있습니다. 하지만 반례가 있을 수 있습니다.
가령 서로 다른 두 개의 원본 이미지가 있고

419
00:44:41,810 --> 00:44:48,316
어떤 적절한 위치에 박스를 놓거나, 색을 더하거나 하게 되면
결국 두 이미지의 Distance를 엄청 가깝게 만들 수 있을 것입니다.

420
00:44:48,316 --> 00:44:52,007
반대로 이 예시에서 똑같은 하나의 이미지에
(하나의 이미지이니 Distance가 변하면 안되지만)

421
00:44:52,007 --> 00:44:57,469
임의로 움직이거나(shift) 색을 더하면(tinting)
 Distance는 제멋대로 변할 것입니다.

422
00:44:57,469 --> 00:45:03,172
그러니 다양한 서로다른 이미지들이 같은 Distance를 가지는
경우라면 잘 못될 수도 있는 것입니다.

423
00:45:03,172 --> 00:45:04,039
질문 있으신가요?

424
00:45:04,039 --> 00:45:07,956
[학생이 질문]

425
00:45:15,207 --> 00:45:24,098
질문은 "최적의 하이퍼파라미터를 찾을 때 까지 학습을
다시 시키는 것이 흔한 방법인지" 입니다.

426
00:45:24,098 --> 00:45:30,982
실제로 사람들은 가끔 그렇게 하곤 합니다.
하지만 그때 그때 다르다고 할 수 있습니다.

427
00:45:30,982 --> 00:45:34,430
만약 여러분이 데드라인에 쫒기고 있고
당장 모델을 사용해야 한다면

428
00:45:34,430 --> 00:45:39,875
그런데 데이터 셋 전부를 다시 학습시키는 것이 너무 오래 걸리면
다시 학습시키기 쉽지 않겠죠

429
00:45:39,875 --> 00:45:50,012
하지만 다시 학습시킬 여유가 있고 1%의 성능이라도
짜내고 싶다면 여러분이 할 수있는 하나의 트릭이 될 수 있습니다.

430
00:45:53,288 --> 00:45:56,758
지금까지는 k-NN이 기계학습 알고리즘이라는 점에서
지닌 다양한 특성들에 대해 배웠습니다.

431
00:45:56,758 --> 00:46:03,823
하지만 실제로는 성능이 엄청 좋지는 않습니다.
특히 이미지에는 잘 사용하지 않습니다.

432
00:46:05,258 --> 00:46:08,895
다 그럼 다음은 
Linear Classification 입니다.

433
00:46:08,895 --> 00:46:19,845
Linear Classification 아주 간단한 알고리즘입니다. 하지만 
아주 중요하고 NN과 CNN의 기반 알고리즘이죠

434
00:46:19,845 --> 00:46:26,242
일부 사람들은 Nerural Network를 레고블럭에 비유합니다.

435
00:46:26,242 --> 00:46:30,345
NN을 구축할 때 다양한 컴포넌트들을 사용할 수 있습니다. 
이 컴넌트들을 한데 모아서

436
00:46:30,345 --> 00:46:36,378
CNN이라는 거대한 타워를 지을 수 있는 것입니다.

437
00:46:36,378 --> 00:46:43,215
앞으로 보게될 다양한 종류의 딥러닝 알고리즘들의
가장 기본이 되는 블럭중 하나가 바로 Linear classifier입니다.

438
00:46:43,215 --> 00:46:48,270
때문에 여러분이 Linear classification이 어떻게 동작하는지를
정확히 이해하는것은 아주 중요합니다.

439
00:46:48,270 --> 00:46:52,712
왜냐면 Linear classification이 결국은 전체 NN을
이루게 될 것이기 떄문이지요.

440
00:46:52,712 --> 00:46:56,139
NN의 구조적 특성을 설명하는 또 다른 예시가 있습니다.

441
00:46:56,139 --> 00:47:00,281
저희 연구실에서 진행하고 있는 
Image Captioning과 관련한 것입니다.

442
00:47:00,281 --> 00:47:06,226
Image Captioni 에서는 이미지가 입력이고 
이미지를 설명하는 문장이 출력입니다.

443
00:47:06,226 --> 00:47:14,496
이미지를 인식하기 위해서 CNN을 사용합니다.
그리고 언어를 인식하기 위해서 RNN을 사용합니다.

444
00:47:14,496 --> 00:47:22,767
우리는 그저 두개(CNN + RNN)을 레고 블럭처럼 붙히고 한번에
학습시킵니다. 그러면 이렇게 어려운 문제도 해결할 수 있는 것입니다.

445
00:47:22,767 --> 00:47:26,388
이 모델에 대에서는 앞으로 배우게 되겠지만

446
00:47:26,388 --> 00:47:36,082
여기에서 말하고 싶었던건 NN이 레고블럭과 같다는 것이고.
Linear Classifier가 그것의 기본이 블럭이 된다는 것입니다.

447
00:47:37,096 --> 00:47:41,257
하지만 2강 에서 하기엔 너무 재밌는 내용이니까
우리는 다시 CIFAR-10으로 잠시 돌아가 봐야 겠습니다.

448
00:47:41,257 --> 00:47:42,375
[웃음]

449
00:47:42,375 --> 00:47:45,641
CIFAR-10이 50,000여개의 트레이닝 샘플이 있고

450
00:47:45,641 --> 00:47:49,808
각 이미지는 32x32 픽셀을 가진
3채널 컬러 이미지라는 것을 다시 상기시켜 봅시다.

451
00:47:52,068 --> 00:47:56,696
Linear classification에서는 K-NN과는
조금은 다른 접근 방법을 이용합니다.

452
00:47:56,696 --> 00:48:04,734
Linear classifier는 "parametric model"의
가장 단순한 형태입니다.

453
00:48:04,734 --> 00:48:08,685
parametric model에는 두 개의 요소가 있습니다.

454
00:48:08,685 --> 00:48:12,201
입력 이미지가 있습니다. 
왼쪽에 보이는 고양이 이미지입니다.

455
00:48:12,201 --> 00:48:17,384
이 입력 이미지를 보통 "X" 로 씁니다.

456
00:48:17,384 --> 00:48:24,767
파라미터, 즉 가중치는 문헌에 따라 다르지만 
"W"라고도 하고 세타(theta)라도고 합니다.

457
00:48:24,767 --> 00:48:30,780
이제 우리는 어떤 함수를 작성해야 하는데
이 함수는 data X와 parameter W를 가지고

458
00:48:30,780 --> 00:48:39,991
10개의 숫자를 출력합니다. 이 숫자는
CIFAR-10의 각 10개의 카테고리의 스코어입니다.

459
00:48:39,991 --> 00:48:48,583
이 스코어를 해석해 보자면, "고양이"의 스코어가 높다는 건
입력 X가 "고양이"일 확률이 크다는 것을 의미합니다.

460
00:48:48,583 --> 00:48:49,827
질문 있으십니까?

461
00:48:49,827 --> 00:48:53,494
[학생이 질문]

462
00:48:55,380 --> 00:48:56,717
잘못들었습니다?

463
00:48:56,717 --> 00:48:58,863
[학생이 질문]

464
00:48:58,863 --> 00:49:01,524
질문은 "여기에서 3 무엇인지" 입니다.
(32 x 32 x 3 의 3 이 무엇인지 질문)

465
00:49:01,524 --> 00:49:12,636
3은 Red, Green, Blue 3 채널을 의미합니다. 보통은 컬러 
이미지를 다룹니다. 컬러 정보는 버리기 아까운 유용한 정보입니다.

466
00:49:15,323 --> 00:49:18,999
앞서 K-NN은 파라미터가 없었습니다.

467
00:49:18,999 --> 00:49:24,092
그저 전체 트레이닝 셋을 가지고 있었고
모든 트레이닝 셋을 Test time에 사용했습니다.

468
00:49:24,092 --> 00:49:28,196
하지만 parametric approach 에서는
트레이닝 데이터의 정보를 요약합니다.

469
00:49:28,196 --> 00:49:31,105
그리고 그 요약된 정보를 파라미터 W에 모아줍니다.

470
00:49:31,105 --> 00:49:35,371
이런 방식을 사용하면 Test time에서 더이상 
트레이닝 데이터가 필요하지 않습니다.

471
00:49:35,371 --> 00:49:37,938
Test time에서는 파라미터 W만 있으면 그만입니다.

472
00:49:37,938 --> 00:49:44,684
이 방법은 핸드폰과 같은 작은 디바이스에서 모델을
동작시켜야 할 때 아주 효율적입니다.

473
00:49:44,684 --> 00:49:50,906
그러니 딥러닝은 바로 이 함수 F의 구조를 
적절하게 잘 설계하는 일이라고 할 수 있습니다.

474
00:49:50,906 --> 00:49:55,406
어떤 식으로 가중치 W와 데이터를 조합할지를
여러가지 복잡한 방법으로 고려해 볼 수 있는데

475
00:49:55,406 --> 00:50:01,169
이 과정들이 모두 다양한 NN 아키텍쳐를 설계하는 과정입니다.

476
00:50:01,169 --> 00:50:05,729
가중치 W와 데이터 X를 조합하는 가장 쉬운 방법은
그냥 이 둘을 곱하는 것입니다.

477
00:50:05,729 --> 00:50:08,833
이 방법이 바로 Linear classification 입니다.

478
00:50:08,833 --> 00:50:15,770
F(x,W) = Wx 입니다.
아주 쉬운 식입니다.

479
00:50:15,770 --> 00:50:18,921
그러면 이제 이들의 차원을 한번 알아보겠습니다.

480
00:50:18,921 --> 00:50:34,715
입력 이미지는 32 x 32 x 3 이였습니다. 이 값을 길게
펴서 열 벡터로 만들면 3,072-dim 벡터가 됩니다.

481
00:50:34,715 --> 00:50:38,632
3072-dim열 벡터가 10-classes 스코어가 되어야 합니다.

482
00:50:39,746 --> 00:50:44,236
이는 10개 카테고리에 해당하는 각 스코어를 의미하는
10개의 숫자를 얻고 싶은 것입니다.

483
00:50:44,236 --> 00:50:49,032
따라서 행렬 W는 10 x 3072가 되어야 합니다.

484
00:50:49,032 --> 00:50:57,086
이 둘을 곱하면 10-classes 스코어를 의미하는
10 x 1 짜리 하나의 열 벡터를 얻게 됩니다.

485
00:50:57,086 --> 00:51:01,910
그리고 가끔은 "Bias" 을 보게 될텐데 
가끔 Bias term도 같이 더해주기도 합니다.

486
00:51:01,910 --> 00:51:06,669
Bias term은 10-dim 열 벡터입니다. 
Bias term은 입력과 직접 연결되지 않습니다.

487
00:51:06,669 --> 00:51:12,235
대신에 "데이터와 무관하게" 
특정 클래스에 "우선권"을 부여합니다.

488
00:51:12,235 --> 00:51:16,300
가령 데이터셋이 분균형한 상황을 생각해 볼 수 있습니다.
고양이 데이터가 개 데이터보다 훨씬 더 많은 상황입니다.

489
00:51:16,300 --> 00:51:23,553
이 상황에서는 고양이 클래스에 상응하는 
바이어스가 더 커지게 됩니다.

490
00:51:23,553 --> 00:51:27,778
그러면 이제 이 함수가 어떻게 동작하는지
그림으로 살펴보겠습니다.

491
00:51:28,930 --> 00:51:37,099
이 그림을 보면 왼쪽에 입력 이미지가 있습니다.
2x2 이미지이고 전체 4개의 픽셀입니다.

492
00:51:37,099 --> 00:51:46,577
이 Linear classifier는 2x2 이미지를 입력으로 받고
이미지를 4-dim 열 벡터로 쭉 폅니다.

493
00:51:46,577 --> 00:51:54,030
10개 클래스를 모두 슬라이드에 담을 수 없어서 이 예제에서는
고양이, 개, 배 이렇게 세가지 클래스만 보겠습니다

494
00:51:54,030 --> 00:51:58,197
가중치 행렬 W는 4x3 행렬이 됩니다.

495
00:51:59,890 --> 00:52:02,236
이런 식으로, 입력은 픽셀 4개고 
클래스는 총 3개 입니다.

496
00:52:02,236 --> 00:52:05,567
그리고 추가적으로 3-dim bias 벡터가 있습니다.

497
00:52:05,567 --> 00:52:11,125
bias는 데이터와 독립적으로 
각 카테고리에 연결됩니다.

498
00:52:11,125 --> 00:52:22,814
"고양이 스코어" 는 입력 이미지의 픽셀 값들과 가중치 행렬을
내적한 값에 bias term을 더한 것입니다.

499
00:52:22,814 --> 00:52:30,157
이러한 관점에서 Linear classification은 
템플릿 매칭과 거의 유사합니다.

500
00:52:30,157 --> 00:52:43,183
가중치 행렬 W의 각 행은 각 이미지에 대한 템플릿으로 볼 수  있고
그 행 벡터와 이미지의 열벡터 간의 내적을 계산하는데,

501
00:52:43,183 --> 00:52:50,458
여기에서 내적이란 결국 클래스 간 탬플릿의
유사도를 측정하는 것과 유사함을 알 수 있습니다.

502
00:52:50,458 --> 00:52:57,073
bias는 데이터 독립적으로  각 클래스에 
scailing offsets을 더해주는 것입니다.

503
00:53:00,837 --> 00:53:04,705
템플릿 매칭의 관점에서  
Linear classification 해석해보면

504
00:53:04,705 --> 00:53:12,799
가중치 행렬 W의 한 행을 뽑아서 
이를 이미지로 시각화 시켜 보면

505
00:53:12,799 --> 00:53:18,908
Linear classifier가 이미지 데이터를 인식하기 위해서 
어떤 일을 하는지 짐작할 수 있습니다.

506
00:53:18,908 --> 00:53:23,208
이 예제에서는 Linear classifier가 이미지를 학습합니다.

507
00:53:23,208 --> 00:53:28,974
슬라이드 하단의 이미지는 실제로 
가중치 행렬이 어떻게 학습되는지를 볼 수 있습니다.

508
00:53:28,974 --> 00:53:32,274
CIFAR-10의 각 10개의 카테고리에 해당하는
행 벡터를 시각화시킨 것입니다.

509
00:53:32,274 --> 00:53:35,686
이렇게 시각화된 이미지를 살펴보면 
어떤 일이 일어나는지 알아볼 수 있습니다.

510
00:53:35,686 --> 00:53:40,978
가령 맨 왼쪽의 이미지는 비행기 클래스에 대한 
템플릿 이미지입니다.

511
00:53:40,978 --> 00:53:46,739
이 이미지는 전반적으로 파란 색입니다. 
가운데에는 어떤 물체가 있는 것 같군요

512
00:53:46,739 --> 00:53:51,574
이 이미지를 해석해보면 Linear classifier가 비행기를 
분류할 때 푸르스름한 것들을 찾고 있는 것 같습니다.

513
00:53:51,574 --> 00:53:57,606
이러한 특징들이 이 분류기가 비행기를 더 잘 찾도록 
도와준다고 해석해 볼 수 있습니다.

514
00:53:57,606 --> 00:53:59,444
바로 옆에 있는 자동차의 예시도 한번 보겠습니다.

515
00:53:59,444 --> 00:54:09,654
중앙은 불그스름하고 상단은 푸르스름합니다. 자동차의
앞유리 같군요. 하지만 조금 이상합니다.

516
00:54:09,654 --> 00:54:13,716
실제 자동차 처럼은 보이지가 않습니다.
어떤 자동차도 이렇게 생기진 않았죠.

517
00:54:13,716 --> 00:54:18,317
Linear classifier의 문제 중 하나는 각 클래스에 대해서
단 하나의 템플릿만을 학습하다는 것입니다.

518
00:54:18,317 --> 00:54:24,340
한 클래스 내에 다양한 특징들이 존재할 수 있지만,
모든 것들을 평균화 시키기 때문에

519
00:54:24,340 --> 00:54:29,675
다양한 모습들이 있더라도 각 카테고리를 인식하기
위한 템플릿은 단 하나밖에 없습니다.

520
00:54:29,675 --> 00:54:33,139
이 문제점은 말(馬)을 분류하는 템플릿을 살펴보면 
여실히 드러나는 대목입니다.

521
00:54:33,139 --> 00:54:37,340
바닥은 푸르스름해 보입니다. 보통 말이 풀밭에 서 있으니 
템플릿이 바닥을 푸르스름하게 학습한 것입니다.

522
00:54:37,340 --> 00:54:43,125
그런데 유심히 살펴보면 말의 머리가 두 개 입니다.
각 사이드 마다 하나씩 달려 있습니다.

523
00:54:43,125 --> 00:54:45,855
머리 두개 달린 말은 존재하지 않습니다.

524
00:54:45,855 --> 00:54:52,788
하지만 Linear classifier가 클래스 당 하나의 템플릿밖에 
허용하지 않으므로 이 방법이 최선입니다.

525
00:54:52,788 --> 00:54:59,460
하지만 Neural Network같은 복잡한 모델이라면 
조금 더 정확도 높은 결과를 볼 수 있을 것입니다.

526
00:54:59,460 --> 00:55:05,230
클래스 당 하나의 템플릿만 학습 할 수 있다는 
것과 같은 제약조건이 없다면 말이죠

527
00:55:09,030 --> 00:55:15,649
Linear classifier을 또 다른 관점으로 해석할 수 있습니다.  
이미지를 고차원 공간의 한 점으로 보는 것입니다.

528
00:55:15,649 --> 00:55:23,328
각 이미지을 고차원 공간의 한 점이라고 생각해 봅시다

529
00:55:23,328 --> 00:55:33,125
Linear classifier는 각 클래스를 구분시켜 주는
선형 결정 경계를 그어주는 역할을 합니다.

530
00:55:33,125 --> 00:55:38,897
가령 왼쪽 상단에 비행기의 예를 볼 수 있습니다.

531
00:55:38,897 --> 00:55:49,493
Linear classifier는 파란색 선을 학습해서 
비행기와 다른 클래스를 구분할 수 있습니다.

532
00:55:49,493 --> 00:55:57,318
임의의 값으로 초기화된 모델이 데이터 들을 잘 분류하려고
노력하는 모습을 지켜보면 아주 재밌습니다.

533
00:55:58,709 --> 00:56:04,000
하지만 이미지가 고차원 공간의 하나의 점 이라는 
관점으로 해석해보면

534
00:56:04,000 --> 00:56:09,758
Linear classification이 직면할 수 있는 문제가 있습니다.

535
00:56:09,758 --> 00:56:15,232
이 Linear classifier를 망가뜨릴 수 있는 예제를 
만드는 일은 생각보다 어렵지 않습니다.

536
00:56:15,232 --> 00:56:20,324
맨 왼쪽 그림은 두 개의 클래스를 가진 데이터 셋입니다.

537
00:56:20,324 --> 00:56:26,095
데이터가 조금은 인위적일 수 있지만, 아무튼 데이터셋에는
파랑/빨강 두 개의 카테고리가 있습니다.

538
00:56:26,095 --> 00:56:33,122
파랑색 카테고리는 0보다 큰 픽셀이 홀수 개 인 경우입니다.
(예 : [3,-1] 이면 0보다 큰 수 : 3 (1개, 홀수개) -> 파랑)

539
00:56:33,122 --> 00:56:38,714
반면 0보다 큰 수가 짝수 개면 빨간 카테고리로 분류합니다.

540
00:56:38,714 --> 00:56:53,426
좌표 평면에 이 같은 규칙으로 그려보면
두 개의 사분면에는 파란 클래스,

541
00:56:53,426 --> 00:56:56,063
두 사분면에는 빨간색 클래스를 볼 수 있습니다. 
(예:(1.1):2(짝), (-1,1):1(홀), (-1, -1):0(짝))

542
00:56:56,063 --> 00:57:05,273
이 데이터를 선 하나로 분류할 방법은 없습니다.
 Linear classifie로는 풀기 힘든 문제입니다.

543
00:57:05,273 --> 00:57:09,535
이 예제가 너무 인위적으로 보일 수 있지만 그렇지 않습니다

544
00:57:09,535 --> 00:57:16,424
가령 이렇게 픽셀 갯수를 세는 대신에 영상 내 동물이나 사람의 수가
홀/짝수 인지를 분류하는 문제일 수 있습니다.

545
00:57:16,424 --> 00:57:21,145
홀/짝수를 분류하는 것과 같은 
반전성 문제(parity problem)는

546
00:57:21,145 --> 00:57:25,725
일반적으로 Linear classification으로 풀기 힘든 문제입니다.

547
00:57:28,376 --> 00:57:33,974
Linear classifier로는 풀기 힘든 또 하나는
Multimodal problem입니다. (맨 오른쪽)

548
00:57:33,974 --> 00:57:41,915
맨 오른쪽 이미지를 보면 파란색이 분포하는 
세 개의 섬들이 있습니다.

549
00:57:41,915 --> 00:57:44,791
그 밖의 빨간색은 전부 다른 카테고리에 속합니다.

550
00:57:44,791 --> 00:57:50,959
앞서 소개드린 말(馬)의 예시처럼
Multimodal problem은 언제든 실제로 일어날 수 있습니다.

551
00:57:50,959 --> 00:57:57,268
왼쪽 머리가 하나의 섬이 될 수 있고
오른쪽 머리가 또 하나의 섬이 될 수 있습니다.

552
00:57:57,268 --> 00:58:03,757
섬이 두 개인데 선을  하나만 긋는 것은
그닥 좋은 방법이 아닌 것입니다.

553
00:58:03,757 --> 00:58:10,854
Multimodal data라면 한 클레스가 
다양한 공간에 분포할 수 있으며

554
00:58:10,854 --> 00:58:13,963
이 문제는 Linear classifier로는 풀 수 없습니다.

555
00:58:13,963 --> 00:58:22,259
이처럼 Linear classifier에는 문제점이 일부 있긴 하지만
아주 쉽게 이해하고 해석할 수 있는 알고리즘입니다.

556
00:58:22,259 --> 00:58:27,245
과제1 에서 Linear classifier를 구현하시게 될 것입니다.

557
00:58:28,852 --> 00:58:34,402
이번 시간을 요약해보면 지금까지 Linear classifier의 
수식을 살펴보았습니다.

558
00:58:34,402 --> 00:58:39,185
Linear classifier가 단순히 
행렬과 벡터 곱의 형태라는 것을 배웠고

559
00:58:39,185 --> 00:58:44,922
템플릿 매칭과 관련이 있고, 이 관점에서 해석해 보면 
각 카테고리에 대해 하나의 템플릿을 학습한다는 것을 배웠습니다.

560
00:58:44,922 --> 00:58:55,738
그리고 가중치 행렬 W 를 학습시키고 나면
새로운 학습 데이터에도 스코어를 매길 수 있습니다.

561
00:58:55,738 --> 00:59:01,951
오늘은 어떻게 가중치 행렬 W를 구할 수 있는지는
배우지 않았습니다.

562
00:59:01,951 --> 00:59:06,907
오늘은 Linear classifier가 어떻게 생겼고, 어떻게
동작하는지만 가볍게 알아보았습니다.

563
00:59:06,907 --> 00:59:11,086
지금 보이는 슬라이드가 우리가 다음 시간에 다룰 내용입니다.

564
00:59:11,086 --> 00:59:16,581
다음 강의에서는 적절한 가중치 행렬 W를 고르는 법과 
다양한 알고리즘들에 대해서 다뤄보도록 하겠습니다.

565
00:59:16,581 --> 00:59:21,322
그리고 더 나아가 비용함수, 최적화, 
ConvNet에 대해서 배울 것입니다.

566
00:59:21,322 --> 00:59:25,044
다음 주에 배울 내용들이죠

567
00:59:25,044 --> 00:59:27,044
오늘은 여기까지 하겠습니다.


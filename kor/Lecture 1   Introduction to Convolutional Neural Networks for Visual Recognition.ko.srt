1
00:00:07,641 --> 00:00:10,308
아마 대다수가 지난 학기에 그 수업을 수강하셨을 것 같습니다.

2
00:00:11,762 --> 00:00:15,507
이번 수업을 다시 진행하게 되어 영광입니다.

3
00:00:15,507 --> 00:00:21,523
CS231n 수업은 정말 빠르게 성장하고 있습니다.

4
00:00:21,523 --> 00:00:26,466
이번으로 세 번째 개설되었는데요
처음에는 150명으로 시작했죠

5
00:00:26,466 --> 00:00:29,000
지난해에는 350명이 수강하였습니다. 두 배지요.

6
00:00:29,000 --> 00:00:34,806
올해에는 또 두 배가 늘어서 오늘 아침에 확인했을 때
약 730명의 학생이 수강신청을 해주셨습니다.

7
00:00:34,806 --> 00:00:40,094
안타깝게도 강의실 제한으로 수업에 참여하지 못하는 분들도 계십니다.

8
00:00:40,094 --> 00:00:44,931
하지만 강의 동영상이 두 시간 내로
SCPD 웹 사이트에 게시될 것입니다.

9
00:00:44,931 --> 00:00:50,889
오늘 여기 와서 수업을 듣지 못했어도
몇 시간 후면 바로 확인하실 수 있습니다.

10
00:00:50,889 --> 00:00:55,076
CS231n은 컴퓨터 비전에 관한 수업입니다.

11
00:00:55,076 --> 00:00:57,412
그렇다면 컴퓨터 비전(Computer Vision)이 무엇일까요?

12
00:00:57,412 --> 00:01:02,578
요즘은 워낙 컴퓨터 비전이 유명해서 제가 굳이
이 분야의 중요성을 강조할 필요는 없겠지만

13
00:01:02,578 --> 00:01:10,032
어쨌든 제 분부를 다해 컴퓨터 비전의 중요성을 강조할 것입니다.

14
00:01:10,032 --> 00:01:15,761
최근 몇 년간 엄청나게 많은 시각 데이터가
쏟아져 나오고 있습니다.

15
00:01:15,761 --> 00:01:20,398
이런 수많은 데이터는 전 세계 각처에 퍼져있는
무수한 센서로부터 비롯됩니다.

16
00:01:20,398 --> 00:01:23,064
요즘은 스마트폰이 없는 분들을 더 찾기 힘들죠

17
00:01:23,064 --> 00:01:26,989
여러분의 스마트폰에는 한두 개의 카메라가 내장되어 있습니다.
어떤 기종은 세 개도 있죠

18
00:01:26,989 --> 00:01:31,114
어쩌면 카메라의 수가 전 세계 인구수보다 많을지도 모릅니다.

19
00:01:31,114 --> 00:01:38,508
이런 카메라와 같은 센서들이 전 세계 각지에서
매일매일 데이터를 쏟아내고 있는 실정이죠

20
00:01:38,508 --> 00:01:47,025
CISCO에서 수행한 2015 ~ 2017년도까지의 한 통계자료가
이 사실을 아주 적나라하게 보여줍니다.

21
00:01:48,919 --> 00:01:54,484
통계에 따르면 인터넷 트래픽 중 80%의 지분은 바로
비디오 데이터입니다.

22
00:01:54,484 --> 00:02:00,525
심지어 이 결과는 사진 같은 다른 데이터들을 모두
제외하고 비디오만 추산한 결과입니다.

23
00:02:00,525 --> 00:02:07,476
이 통계는 인터넷의 데이터 대부분이
시각 데이터라는 사실을 보여줍니다.

24
00:02:07,476 --> 00:02:13,157
그러니 시각 데이터들을 잘 활용할 수 있는
알고리즘을 잘 개발하는 것이 무엇보다 중요해졌습니다.

25
00:02:13,157 --> 00:02:17,813
하지만 문제 있습니다. 이런 시각데이터는
해석하기 상당히 까다롭다는 점이죠

26
00:02:17,813 --> 00:02:24,526
일부는 시각 데이터를 암흑물질(dark matter)이라고 합니다.

27
00:02:24,526 --> 00:02:27,437
물리학 수업에서 암흑물질을 들어본 분들도 계실테지만

28
00:02:27,437 --> 00:02:33,377
우주 대부분의 질량을 차지하고
있는 물질이 바로 암흑 물질입니다.

29
00:02:33,377 --> 00:02:38,293
우리는 여러 가지 간접적인 측정실험을 통해서
암흑물질의 "존재" 까지는 알 수 있었지만

30
00:02:38,293 --> 00:02:40,535
암흑물질을 직접 "관측" 할 수는 없습니다.

31
00:02:40,535 --> 00:02:42,838
시각데이터가 사실 그렇습니다

32
00:02:42,838 --> 00:02:55,685
시각 데이터가 대부분이지만 사실상 이들을
이해하고 해석하는 일은 상당히 어렵습니다.

33
00:02:55,685 --> 00:02:58,466
Youtube의 통계자료도 있습니다.

34
00:02:58,466 --> 00:03:07,746
YouTube에는 매초 다섯 시간 분량의
비디오가 업로드된다고 합니다.

35
00:03:07,746 --> 00:03:14,472
우리가 "하나... 둘... 셋..." 세고 나면
YouTube에는 15시간 분량의 비디오가 새로 추가된 것이지요

36
00:03:17,076 --> 00:03:24,146
Google 직원이 아무리 많아도 이 모든 비디오를 직접 보고,
이해하고, 정리한다는 것은 사실상 불가능한 일입니다.

37
00:03:24,146 --> 00:03:32,057
따라서 그들이 비디오들을 잘 정리해서 유저들에게 제공하고
또 비디오에 적절한 광고를 게시하려면

38
00:03:32,057 --> 00:03:37,053
자동으로 시각데이터를 이해, 분석하는 알고리즘을
개발하는 것이 관건인 셈입니다.

39
00:03:38,649 --> 00:03:47,564
컴퓨터 비전이라는 분야 주변에는 상당히 많은 분야가 존재합니다.
따라서 다양한 과학, 공학 분야들과 맞닥뜨리게 됩니다.

40
00:03:47,564 --> 00:03:50,822
컴퓨터 비전이 우주(universe)의 중심이라 할 수 있겠죠

41
00:03:50,822 --> 00:03:56,453
하지만 컴퓨터 비전 뿐만 아니라
물리학을 다뤄야 할 수도 있습니다.

42
00:03:56,453 --> 00:04:01,784
광학, 이미지 구성, 이미지의 물리학적 형성 등을 이해하려면
물리학적인 현상들을 이해할 필요가 있기 때문입니다.

43
00:04:01,784 --> 00:04:03,995
생물학이나 심리학도 알아야 합니다.

44
00:04:03,995 --> 00:04:09,894
동물의 뇌가 어떤 방식으로 시각정보를
물리적으로 "보고 처리하는지"를 이해하려면 말이죠

45
00:04:09,894 --> 00:04:14,305
물론 그 밖에 컴퓨터 과학, 수학, 그리고 공학 등도 다룹니다.

46
00:04:14,305 --> 00:04:19,639
컴퓨터 비전 알고리즘을 구현할 컴퓨터 시스템을
실제로 구축할 때 필요한 분야들이죠

47
00:04:19,640 --> 00:04:25,992
그럼 이제 저를 포함한 이 수업의 교수진과
운영진들을 간단히 설명해 드리겠습니다.

48
00:04:25,992 --> 00:04:33,606
저와 Serena는 Fei-Fei Li 교수님의 지도하에 있는
Stanford Vision Lab의 박사과정 (PhD) 학생입니다.

49
00:04:33,606 --> 00:04:41,184
그리고 저희 Lab은 기계학습과 컴퓨터과학에
관련된 연구들을 진행하고 있습니다.

50
00:04:41,184 --> 00:04:44,900
저 같은 경우에는 Language와 Vision에 좀 더 집중하고 있습니다.
일부 프로젝트도 진행하고 있습니다.

51
00:04:44,900 --> 00:04:49,775
저희 Lab에서는 다양한 연구를 진행하고 있습니다.
그 밖에도 Lab에서는 신경과학과 인지과학과 관련된 분야도 연구합니다.

52
00:04:52,541 --> 00:04:57,557
아마도 여러분은 CS231n이 Stanford의 다른 수업들과
어떤 연관성이 있는지 궁금하실 것입니다.

53
00:04:57,557 --> 00:05:02,848
이 수업은 여러분이 컴퓨터 비전의 기초개론을
알고 있다고 가정하고 수업을 진행합니다.

54
00:05:02,848 --> 00:05:06,926
그러니 만일 여러분이 학부생이거나
혹은 컴퓨터 비전이 처음이시라면

55
00:05:06,926 --> 00:05:14,229
Fei-Fei 와 Juan Carlos Niebles의 CS131을
선수 과목으로 수강하셨어야 합니다.

56
00:05:14,229 --> 00:05:24,925
그리고 Chris Mannin & Richard Socher의
딥러닝 & 자연어처리 수업이 지난 학기에 있었습니다.

57
00:05:24,925 --> 00:05:28,595
아마 대다수가 지난 학기에 그 수업을 수강하셨을 것 같습니다.

58
00:05:31,482 --> 00:05:33,785
CS231n은 그 수업과 일부 겹치는 부분이 있습니다.

59
00:05:33,785 --> 00:05:40,444
하지만 여기에서는 컴퓨터 비전에 초점을 맞춥니다.

60
00:05:41,361 --> 00:05:43,078
또한 Silvio Savarese 교수께서 이번
학기에 CS231a를 강의하십니다.

61
00:05:43,078 --> 00:05:47,378
Silvio Savarese 교수가
가르치는 CS231a입니다.

62
00:05:47,378 --> 00:05:54,010
그리고 CS231a는 컴퓨터 비전을 둘러싼
조금 더 넓은 분야들에 초점을 맞추고 있습니다.

63
00:05:54,010 --> 00:06:03,813
3D reconstruction, 로봇 비전 등
CS231n보다 광범위한 분야를 다루게 됩니다.

64
00:06:03,813 --> 00:06:16,228
여기 CS231n은 신경망(Neural Network), 특히 CNN과
관련된 세부 분야를 중점적으로 배우게 될 것입니다

65
00:06:16,228 --> 00:06:19,178
물론 세미나 수업도 진행할 예정입니다.

66
00:06:19,178 --> 00:06:27,867
세미나 일정이 매년 변동하므로 자세한 사항은
강의계획서와 수업시간표를 확인하시길 바랍니다.

67
00:06:27,867 --> 00:06:34,174
보통 첫 수업은 Fei-Fei Li 교수님이 진행하시지만
안타깝게도 오늘 오실 수 없으셨습니다.

68
00:06:34,174 --> 00:06:38,463
대신 Fei Fei 교수님이 안 계시므로 다른 방법을 고안했습니다.

69
00:06:38,463 --> 00:06:44,772
교수님께서 Computer Vision의 역사를 소개하는
비디오를 녹화하셨습니다.

70
00:06:44,772 --> 00:06:58,000
이 수업은 컴퓨터 비전 수업이기 때문에 오늘날의 CNN을
발전시킨 기존연구의 역사와 흐름을 이해해야만 합니다.

71
00:06:58,500 --> 00:07:00,000
가상의 Fei Fei 교수님을 소개하겠습니다.

72
00:07:00,398 --> 00:07:01,915
[웃음]

73
00:07:01,915 --> 00:07:05,500
여러분께 컴퓨터 비전의 역사를 간단히 소개해 주실 것입니다.

74
00:07:08,610 --> 00:07:20,620
자 우선 오늘의 강의목표를 살펴보겠습니다. 두 가지 주제가 있습니다.
컴퓨터 비전의 역사와 우리 CS231n 수업의 개요입니다.

75
00:07:20,620 --> 00:07:36,100
그렇다면 비전(시각)과 컴퓨터 비전이 언제 어디에서 비롯됐고
현재는 어디쯤 왔는지를 알아보겠습니다.

76
00:07:36,100 --> 00:07:44,770
비전의 역사는 아주 오래전으로 돌아갑니다.
정확하게는 5억 4천만 년 전이죠.

77
00:07:44,770 --> 00:07:50,800
그 시대의 삶은 어땠을까요?
지구 대부분은 물이었고

78
00:07:50,920 --> 00:07:58,300
바다를 부유하는 일부 생물들만 존재했습니다.

79
00:07:58,300 --> 00:08:03,730
이들의 삶은 단조로웠습니다. 그들은 많이 움직이지 않았고
눈(eyes) 같은 건 존재하지 않았습니다.

80
00:08:03,730 --> 00:08:09,640
먹이가 주변에 있으면 잡아먹고 없으면
그저 둥둥 떠 있는 것이 다였습니다.

81
00:08:09,640 --> 00:08:17,140
하지만 5억 4천만 년 전에 아주 놀라운 사건이 벌어졌습니다.

82
00:08:17,140 --> 00:08:33,820
동물학자들은 화석을 연구하면서 천만 년이라는 아주 짧은 시기 동안에
생물의 종이 폭발적으로 늘어났다는 것을 발견했습니다.

83
00:08:33,820 --> 00:08:41,500
얼마 없던 종의 수가 수십만이 된 것입니다.
정말 신기한 일이었습니다. 이유가 무엇이었을까요?

84
00:08:41,500 --> 00:08:47,920
많은 가설이 있었지만, 여전히 수년간 풀지 못한 숙제였습니다.
진화 생물학자들은 이를 "진화의 빅뱅"이라고 불렀습니다.

85
00:08:47,920 --> 00:09:01,299
앤드류 파커 (Andrew Parker)는 이 연구에서
가장 설득력 있는 이론 중 하나를 제안했습니다

86
00:09:01,299 --> 00:09:19,310
그는 약 5억 4천만 년 전 최초의 눈(eyes)이 생겨났다는
것을 발견했습니다. 비젼(시각)의 탄생이

87
00:09:19,310 --> 00:09:26,610
폭발적인 종 분화의 시기를 촉발시킨 것입니다. 생물들은
갑자기 볼 수 있게 되었습니다. 볼 수 있다면 삶은

88
00:09:26,610 --> 00:09:32,580
훨씬 더 능동적이게됩니다. 일부 포식자들은 먹이를 찾아다니고

89
00:09:32,580 --> 00:09:39,980
먹이들은 포식자로부터 달아나야만 했죠.
그래서 비전의 도래는

90
00:09:39,980 --> 00:09:46,860
진화적 군비경쟁을 촉발시켰고 생물들은 하나의 종으로
살아남으려면 빠르게 진화해야만 했습니다.

91
00:09:46,860 --> 00:09:54,870
이것이 바로 비전의 태동입니다. 5억 4천만 년 후(현재)

92
00:09:54,870 --> 00:10:01,380
비전은 거의 모든 동물, 특히 지능을 가진 동물들의

93
00:10:01,380 --> 00:10:09,660
가장 큰 감각 체계로 발전했습니다.
우리 인간은 대뇌 피질의 50%가량의 뉴런이

94
00:10:09,660 --> 00:10:15,450
시각처리에 관여합니다.
비전은 가장 큰 감각체계이며

95
00:10:15,450 --> 00:10:22,590
우리가 생존하고, 일하고,
움직이고, 어떤 것들을 다루고,

96
00:10:22,590 --> 00:10:29,730
의사소통하고, 오락을 즐기는 등 많은 것들을 가능하게 해줍니다.
비전은 동물들에게 중요하며

97
00:10:29,730 --> 00:10:38,930
특히 지능을 가진 동물들에게 정말 중요합니다.
지금까지는 생물학적 비전의

98
00:10:38,930 --> 00:10:48,329
짧은 줄거리였습니다만, 그렇다면 인간이 만든 공학적 비전인
카메라의 역사는 어떨까요?

99
00:10:48,329 --> 00:10:56,450
오늘날 우리가 알고 있는 초창기의 카메라는

100
00:10:56,450 --> 00:11:04,410
1600년대 르네상스 시대의 카메라인 Obscura입니다.

101
00:11:04,410 --> 00:11:13,730
이 카메라는 핀홀 카메라 이론을 기반으로 한 카메라입니다.

102
00:11:13,730 --> 00:11:21,390
Obscura는 생물학적으로 발전한 초기의 눈과 상당히 유사합니다.
빛을 모아주는 구멍이 하나 있고

103
00:11:21,390 --> 00:11:28,020
카메라 뒤편의 평평한 면은 정보를 모으고

104
00:11:28,020 --> 00:11:36,560
이미지를 투영합니다. 카메라 기술이 발전하면서
오늘날 카메라는 어디에나 있습니다.

105
00:11:36,560 --> 00:11:40,910
카메라는 스마트폰 카메라나 다른 여러 기기에 이르기까지
사람들이 사용하는 가장 인기 있는 센서 중 하나가 되었습니다.

106
00:11:40,910 --> 00:11:56,510
그동안에 생물학자들은 비전의 매커니즘을 연구하기 시작했습니다.
인간과 동물의 비전에 연구에 가장 영향력 있었을 뿐만 아니라

107
00:11:56,510 --> 00:12:10,850
컴퓨터 비전에도 영감을 준 한 연구가 있었습니다. 1950/60년대
전기생리학을 이용한 Hubel과 Wiesel의 연구입니다.

108
00:12:10,850 --> 00:12:18,170
그들이 묻고 싶었던 질문은 바로
"포유류의 시각적 처리 메커니즘은 무엇일까?" 였습니다.

109
00:12:18,170 --> 00:12:26,600
그래서 그들은 고양이의 뇌를 연구하기로 합니다.

110
00:12:26,600 --> 00:12:32,090
시각 처리 매커니즘만 보면
고양이와 인간은 비슷합니다.

111
00:12:32,090 --> 00:12:37,490
그들의 고양이 두뇌 뒷면에 전극 몇 개를 꽂았습니다.
"일차 시각 피질"이 있는 곳이었죠

112
00:12:37,490 --> 00:12:52,970
그리고 어떤 자극을 줘야 일차 시각 피질의 뉴런들이
격렬하게 반응하는지 관찰하였습니다.

113
00:12:52,970 --> 00:13:00,380
그들은 일차 시각 피질에는 다양한 종류의
세포가 있다는 것을 알았습니다.

114
00:13:00,380 --> 00:13:12,080
그중 가장 중요한 세포가 있었는데 그 세포들은 아주 단순했습니다.
경계(edges)가 움직이면 이에 반응하는 세포들이었습니다.

115
00:13:12,080 --> 00:13:26,060
물론 더 복잡한 세포들도 있긴 하지만, 주된 발견은
시각 처리가 처음에는 단순한 구조로 시작되며

116
00:13:26,060 --> 00:13:38,560
그 정보가 통로를 거치면서 점점 복잡해진다는 것입니다.

117
00:13:38,560 --> 00:13:46,280
실제 세상을 제대로 인지할 수 있을 때까지 말이죠.

118
00:13:46,280 --> 00:14:00,410
컴퓨터 비전의 역사는 60년대 초반에 태동합니다.
Block World는 Larry Roberts의 연구는

119
00:14:00,410 --> 00:14:07,250
아마도 컴퓨터 비전 분야에서의 최초의 박사 학위 논문입니다.

120
00:14:07,250 --> 00:14:13,850
이 연구에서는 우리 눈에 보이는 사물들을
기하학적 모양으로 단순화시켰습니다.

121
00:14:13,850 --> 00:14:23,419
이 연구의 목표는 우리 눈에 보이는 세상을 인식하고
그 모양을 재구성하는 일이었습니다.

122
00:14:23,419 --> 00:14:31,550
1966년에는 MIT 여름 프로젝트가 하나 진행되었는데, 아주 유명하죠
바로 "The Summer Vision Project" 입니다.

123
00:14:31,550 --> 00:14:38,440
당시 프로젝트의 목표는, 제가 읽어 드리겠습니다.
"시각 시스템의 전반을 구현하기 위해서

124
00:14:38,440 --> 00:14:44,240
프로젝트 참가자들을 효율적으로 이용하는 것."
이였습니다.

125
00:14:44,240 --> 00:14:47,780
다시 말해 그들은 그 여름 안에 대부분의 시각 체계를 구현해
내려는 야심 찬 목표를 세우고 있었던 것입니다.

126
00:14:47,780 --> 00:14:54,590
이는 아주 야심 찬 목표였습니다.
그로부터 50년이 지났습니다.

127
00:14:54,590 --> 00:15:02,240
"컴퓨터 비전"이라는 분야가 한 여름 프로젝트에서 피어나서는

128
00:15:02,240 --> 00:15:13,940
현재 전 세계 수천 명의 연구자들이 아직도 비전의
가장 근본적인 문제들을 연구하고 있습니다.

129
00:15:13,940 --> 00:15:21,380
컴퓨터 비전은 아직 숙제가 많지만, 인공지능 분야에서
가장 중요하고 빠르게 성장하는 분야 중 하나입니다.

130
00:15:21,380 --> 00:15:27,410
언급하지 않을 수 없는 인물이 한 명 더 있습니다.
그는 바로 David Marr입니다.

131
00:15:27,410 --> 00:15:34,550
David Marr은 MIT의 비전 과학자였으며
그는 70년대 후기에 아주 유명한 책을 한 권 저술합니다.

132
00:15:34,550 --> 00:15:48,200
이 책은 그가 비전을 무엇이라 생각하는지, 그리고 어떤 방향으로 컴퓨터
비전이 나아가야 하는지, 그리고 컴퓨터가 비전을 인식하게 하기 위해

133
00:15:48,200 --> 00:16:02,440
어떤 방향으로 알고리즘을 개발해야 하는지를 다룬 책이었습니다.

134
00:16:02,440 --> 00:16:10,639
그는 그의 저서에서, 우리가 눈으로 받아들인 "이미지"를
"최종적인 full 3D 표현"으로 만들려면

135
00:16:10,640 --> 00:16:16,360
몇 단계의 과정을 거쳐야만 한다고 주장했습니다.
첫 단계는, 그가 부르길 "Primal Sketch"라고 하는 단계입니다.

136
00:16:16,360 --> 00:16:23,060
이 과정은 주로 경계(edges), 막대(bars),
끝(ends), 가상의 선(virtual lines),

137
00:16:23,060 --> 00:16:28,970
커브(curves), 경계(boundaries)가 표현되는 과정입니다.
이 과정은 신경과학자들에게 영감을 받은 것들이죠.

138
00:16:28,970 --> 00:16:41,420
Hubel과 Wiesel은 시각처리의 초기 단계는 경계와 같은
단순한 구조와 아주 밀접한 관계가 있다고 했었죠

139
00:16:41,420 --> 00:16:45,860
경계와 커브 이후의 다음 단계는, 그가 부르기로는

140
00:16:45,860 --> 00:16:52,300
"2.5-D sketch"라는 단계이며 이 단계에서는
시각 장면을 구성하는 표면(surfaces) 정보,

141
00:16:52,300 --> 00:16:58,840
깊이 정보, 레이어, 불연속 점과 같은 것들을 종합합니다.

142
00:16:58,850 --> 00:17:04,930
그리고 결국에 그 모든 것을 한데 모아서
surface and volumetric primives의 형태의

143
00:17:04,930 --> 00:17:11,579
계층적으로 조직화된 최종적인 3D 모델을 만들어 냅니다.

144
00:17:11,579 --> 00:17:20,719
그리고 이런 방식은 "비전이 무엇인가"라는 것에 대한 아주
"이상적인" 사고과정이었습니다. 그리고 이런 방식의 사고방식은

145
00:17:20,720 --> 00:17:25,790
실제로 수십 년간 컴퓨터 비전 분야를 지배했으며

146
00:17:25,790 --> 00:17:31,940
학생들이 컴퓨터 비전을 처음 입문하고 나서
"어떻게 시각정보를 분석할 수 있을까"라는 질문을 던졌을 때

147
00:17:31,940 --> 00:17:38,230
직관적인 생각해 볼 수 있는 방법이었습니다.

148
00:17:39,310 --> 00:17:48,380
70년대에는 또 다른 아주 중요한 연구들이 있었습니다.

149
00:17:48,380 --> 00:17:55,160
"우리는 어떻게 해야 장난감 같은 단순한 블록 세계를 뛰어넘어서

150
00:17:55,160 --> 00:18:02,509
실제 세계를 인식하고 표현할 수 있을까?"라는 질문을 하기 시작했습니다.
70 년대를 생각해보면

151
00:18:02,509 --> 00:18:07,910
그 당시에는 사용할 수 있는 데이터가 거의 없었습니다.
컴퓨터도 정말 느렸고

152
00:18:07,910 --> 00:18:20,170
심지어 PC가 보급되기도 전이죠. 이 상황에서 컴퓨터 과학자들은 어떻게
해야 대상을 인식하고 표현할 수 있을지를 고민하기 시작했습니다.

153
00:18:20,170 --> 00:18:26,649
Stanford와 SRI에서 과학자들이
서로 비슷한 아이디어를 제안했습니다.

154
00:18:26,649 --> 00:18:32,740
하나는 "generalized cylinder"이고
하나는 "pictorial structure"입니다.

155
00:18:32,740 --> 00:18:45,510
기본 개념은 "모든 객체는 단순한
기하학적 형태로 표현할 수 있다"라는 것입니다.

156
00:18:45,510 --> 00:18:56,079
가령 사람은 원통 모양을 조합해서 만들 수 있습니다. (왼쪽 그림)
또는 "주요 부위"와 "관절"로 표현할 수도 있을 것입니다. (오른쪽 그림)

157
00:18:56,079 --> 00:19:11,140
두 방법 모두 단순한 모양과 기하학적인 구성을 이용해서
복잡한 객체를 단순화시키는 방법입니다.

158
00:19:11,140 --> 00:19:19,220
이러한 연구들은 수년간 다른 연구에 상당히 많은 영향을 미쳤습니다.

159
00:19:19,220 --> 00:19:33,699
80년대 또 다른 사례로, David Lowe는 어떻게 하면 단순한 구조로
실제 세계를 재구성/인식할 수 있을지 고민했습니다.

160
00:19:33,699 --> 00:19:43,440
David Lowe는 이 연구에서 면도기를 인식하기 위해서 면도기를

161
00:19:43,440 --> 00:19:50,860
선(lines)과 경계(edges) 그리고 직선(straight lines)
그리고 이들의 조합을 이용해서 구성했습니다.

162
00:19:50,860 --> 00:20:10,410
60/70/80년대에는 컴퓨터 비전으로 어떤 일을 할 수 있을까
고민한 시대였습니다. 하지만 너무 어려운 문제였습니다.

163
00:20:10,410 --> 00:20:17,980
지금까지 제가 보여 드린 연구들이 모두 아주 대담했고
큰 야망을 가진 시도였지만

164
00:20:17,980 --> 00:20:24,160
그들은 단순한 수준(toy example)에 불과했습니다.

165
00:20:24,160 --> 00:20:38,019
현실 세계에서 잘 동작할지를 생각해보면
많이 진보하지 못했다고 할 수 있죠. 그래서 컴퓨터 비전 연구자들은

166
00:20:38,019 --> 00:20:43,709
우리가 도대체 무슨 실수를 하고 있을까
고민하다가 한가지 질문을 떠올리게 됩니다.

167
00:20:43,709 --> 00:20:50,200
객체인식이 너무 어렵다면 우선 객체 분할(segmentation)
이 우선이 아니었을까 라고 말이죠

168
00:20:50,200 --> 00:20:58,760
객체분할은 이미지의 각 픽셀을 의미 있는
방향으로 군집화하는 방법입니다.

169
00:20:58,760 --> 00:21:03,880
픽셀을 모아놔도 사람을 정확히
인식할 수 없을지도 모르지만

170
00:21:03,880 --> 00:21:10,140
적어도 배경인 픽셀과 사람이 속해 있을지도
모르는 픽셀을 가려낼 수는 있었습니다.

171
00:21:10,140 --> 00:21:15,339
이를 "영상분할(Image Segmentation)"이라고 합니다.
이 문제를 다룬 아주 중요한 연구가 있었는데

172
00:21:15,339 --> 00:21:21,759
Berkeley 대학의 Jitendra Malik 교수와
그의 제자인 Jianbo Shi의 연구였습니다.

173
00:21:21,760 --> 00:21:29,880
이 연구는 영상분할 문제를 해결하기 위해서
그래프 이론을 도입했습니다.

174
00:21:29,880 --> 00:21:39,600
그리고 컴퓨터 비전에서 유난히
발전 속도가 빨랐던 분야가 있었습니다.

175
00:21:39,610 --> 00:21:45,850
바로 "얼굴인식" 입니다. 인간에게 가장
중요한 부위 중 하나가 바로 얼굴이죠.

176
00:21:45,850 --> 00:21:51,779
어쩌면 얼굴이 가장 중요할 수도 있겠군요.

177
00:21:51,779 --> 00:22:05,220
대략 1999/2000년대에는 "기계학습", 특히나 "통계적 기계학습"
이라는 방법이 점차 탄력을 얻기 시작했습니다.

178
00:22:05,220 --> 00:22:11,620
가령 "Support Vector Machine", "Boosting", "Graphical models"
그리고 초기 "Neural Network" 등이 있었습니다.

179
00:22:11,620 --> 00:22:18,449
그중 가장 큰 기여를 한 연구는 바로

180
00:22:18,449 --> 00:22:24,939
Paul Viola와 Michael Jones이 AdaBoost를
이용해 실시간 얼굴인식에 성공한 것입니다.

181
00:22:24,939 --> 00:22:31,779
이 연구는 당시 아주 대단한 성과였습니다.
연구 당시는 2001년이었고

182
00:22:31,779 --> 00:22:36,730
컴퓨터는 여전히 엄청 느렸습니다.
하지만 그들의 얼굴인식 알고리즘은

183
00:22:36,730 --> 00:22:50,800
실시간과 가깝게(near-real-time) 인식할 수
있었고 논문발표 5년이 지난 -

184
00:22:50,800 --> 00:22:58,960
2006년에 Fujifilm은 실시간 얼굴인식을 지원하는
최초의 디지털카메라를 선보였습니다.

185
00:22:58,960 --> 00:23:05,960
이는 기초 과학 연구의 성과를 실제 응용 제품으로
가장 빠르게 전달한 사례라고 할 수 있습니다.

186
00:23:05,960 --> 00:23:13,920
자! 이제 다시 "어떻게 객체를 잘 인식할 것인가?"
라는 질문으로 다시 한번 돌아가 봅시다.

187
00:23:13,930 --> 00:23:31,300
90년대 후반부터 2010년도까지의 시대를 풍미했던 알고리즘은
"특징기반 객체인식 알고리즘" 이었습니다. 이 시절 나온 -

188
00:23:31,300 --> 00:23:39,670
아주 유명한 알고리즘이 바로 David Lowe의 SIFT
feature입니다. 그의 아이디어는 전체 객체를 -

189
00:23:39,670 --> 00:23:44,860
가령, 여기 정지 표지판이 있습니다.
이 정지 표지판들을 서로 매칭하기는 상당히 어렵죠

190
00:23:44,860 --> 00:23:57,210
카메라 앵글이 변할 수 있고, 겹치거나 화각이 변하고 빛도
변하고 객체 자체도 얼마든지 변할 수 있습니다.

191
00:23:57,210 --> 00:24:15,000
하지만 그들은 객체의 특징 중 일부는 다양한 변화에 조금 더 강인하고
불변하다는 점을 발견했습니다.

192
00:24:15,010 --> 00:24:21,610
그리하여 객체인식은 객체에서 이와 같은
중요한 특징들을 찾아내고

193
00:24:21,610 --> 00:24:28,569
그 특징들을 다른 객체에 매칭시키는 과제가 되었습니다.
이미지 전체를 매칭하는 일보다 훨씬 쉬운 일이었죠.

194
00:24:28,569 --> 00:24:42,060
이 그림은 그 논문에서 가져온 것입니다. 정지표지판
이미지에서 일부 SIFT 특징들을 추출하고

195
00:24:42,060 --> 00:24:49,440
또 다른 정지 표지판에서도 특징을 추출하여
이를 식별하고 매칭합니다.

196
00:24:51,130 --> 00:24:59,330
이미지에 존재하는 "특징"을 사용하게 되면서

197
00:24:59,330 --> 00:25:04,780
컴퓨터 비전은 또 한 번의 도약을 할 수 있었습니다.
그리고 장면 전체를 인식하기에 이르렀습니다.

198
00:25:04,780 --> 00:25:18,620
한 예로, Spatial Pyramid Matching이 있습니다.
기본 아이디어는 우리가 특징들을 잘 뽑아낼 수만 있다면

199
00:25:18,620 --> 00:25:23,750
그 특징들이 일종의 "단서"를 제공해 줄 수 있다는 것이었죠
이미지가 풍경인지, 부엌인지, 또는 고속도로인지 하는 것을 말이죠.

200
00:25:23,750 --> 00:25:37,130
이 연구는 이미지 내의 여러 부분과 여러 해상도에서
추출한 특징을 하나의 특징 기술자로 표현하고

201
00:25:37,130 --> 00:25:44,780
Support Vector Algorithm을 적용합니다.

202
00:25:44,780 --> 00:25:53,930
이런 방식의 연구들은 사람 인식에도 탄력을 주었습니다.

203
00:25:53,930 --> 00:26:02,990
여러 특징들을 잘 조합해 보자는 시도들이었죠
사람인식에 관련된 연구들도 아주 많았습니다.

204
00:26:02,990 --> 00:26:10,490
사람인식과 관련된 연구들은 어떻게 해야 사람의 몸을
현실적으로 모델링할 수 있을지에 관련된 연구였습니다.

205
00:26:10,490 --> 00:26:15,710
그중 하나는 "Histogram Of Gradients"
입니다. , 또 한가지는 -

206
00:26:15,710 --> 00:26:26,770
"Deformable Part Models" 입니다.
그 당시에는 60/70/80년대를 거치고

207
00:26:26,770 --> 00:26:34,160
21세기를 맞이하고 있었고
하나의 변곡점을 마주하게 됩니다.

208
00:26:34,160 --> 00:26:45,680
사진의 품질이 점점 좋아졌습니다. 인터넷과 디지털카메라의 발전은
더더욱 좋은 실험 데이터를 만들어 낼 수 있었습니다.

209
00:26:45,680 --> 00:27:02,840
2000년대 초에 일궈낸 것 중 하나는 바로 컴퓨터 비전이 앞으로
풀어야 할 문제가 무엇인지의 정의를 어느 정도 내렸다는 것입니다.

210
00:27:02,840 --> 00:27:11,120
물론 해결해야 할 다양한 문제가 있겠지만, 이 또한
아주 중요한 문제였습니다. 바로 "객체인식" 입니다.

211
00:27:11,120 --> 00:27:18,950
제가 여태 말씀드린 것들이 객체인식이지만 2000년대 초 -

212
00:27:18,950 --> 00:27:26,600
우리는 Benchmark Dataset를 모으기 시작했습니다.
객체인식 기술의 어디쯤 왔는지 측정해 보기 위해서였죠

213
00:27:26,600 --> 00:27:41,480
그 중 하나는 PASCAL Visual Object Challenge(VOC)
입니다. 이 데이터셋에는 20개의 클래스가 있고

214
00:27:41,480 --> 00:27:57,440
여기 보이는 것들과 같이 기차, 비행기, 사람이 있고 소, 병, 고양이
등도 있는 것으로 기억합니다. 데이터셋은

215
00:27:57,440 --> 00:28:04,280
클래스당 수천 수만 개의 이미지들이 있었으며,
다양한 연구 집단에서

216
00:28:04,280 --> 00:28:11,750
이를 통해 알고리즘의 자신들의 알고리즘을 테스트했고

217
00:28:11,750 --> 00:28:19,870
얼마나 진보했는지를 지켜보았습니다.
여기 2007년부터 2012년도까지의 표가 있습니다.

218
00:28:19,870 --> 00:28:38,680
객체인식 성능은 꾸준히 증가했습니다. 많은 진보가 이루어졌죠

219
00:28:38,680 --> 00:28:53,330
그 무렵, Princeton과 Stanford에 있던 그룹에서 더 어려운
질문을 던졌습니다. 우리는 이 세상의 모든 객체들을

220
00:28:53,330 --> 00:29:00,260
인식할 준비가 되었는가? 였습니다.
이 질문은 한 사실로부터 비롯되었습니다.

221
00:29:00,260 --> 00:29:07,970
거의 대부분의 기계학습 알고리즘에 해당하는 사실이었습니다.

222
00:29:07,970 --> 00:29:20,070
Graphical Model, SVM, AdaBoost 같은 기계학습
알고리즘들이 트레이닝 과정에서 Overfit을 하는 것 같았습니다.

223
00:29:20,070 --> 00:29:25,410
이 문제의 원인 중 하나는 시각 데이터가 너무 복잡하다는 것입니다.

224
00:29:25,410 --> 00:29:37,559
모델의 입력은 복잡한 고차원 데이터였고, 이로 인해
모델을 fit하려면 더 많은 파라미터가 필요했죠

225
00:29:37,559 --> 00:29:44,160
학습 데이터가 부족하면 Overfiting이 훨씬 더 빠르게 발생했고
일반화 능력이 떨어졌습니다.

226
00:29:44,160 --> 00:29:52,440
우리에게는 두 가지 motivation이 있었습니다. 하나는
이 세상의 모든 것들을 인식하고 싶다는 것이고

227
00:29:52,440 --> 00:30:04,620
또 하나는 기계학습의 Overfiting 문제를 극복해보자는 것이었죠

228
00:30:04,620 --> 00:30:17,900
이 동기를 바탕으로 ImageNet 프로젝트를 시작했습니다.
구할 수 있는 모든 이미지를 담은 가장 큰 데이터셋을 만들고 싶었습니다.

229
00:30:17,910 --> 00:30:23,250
이 데이터셋으로 모델을 학습시킬 수 있고 Benchmark도
할 수 있도록 말이죠. 프로젝트는 약 3년 정도 걸렸습니다.

230
00:30:23,250 --> 00:30:37,620
어려운 일도 많았습니다. 우선 인터넷에서 수십억 장의 이미지를
다운받았고 WordNet이라는 Dictionary로 정리했습니다.

231
00:30:37,620 --> 00:30:45,770
WordNet에는 수천 가지의 객체 클래스가 있습니다.

232
00:30:45,770 --> 00:30:52,230
그리고 Clever Crowd Engineering trick을 도입했습니다.
Amazon Mechanical Turk에서 사용하는

233
00:30:52,230 --> 00:31:02,270
이미지의 정렬, 정제, 레이블 등을 제공하는 플랫폼입니다.

234
00:31:02,270 --> 00:31:10,830
그 결과 ImageNet은 대략 15만 장에 달하는 이미지와 22만 가지의
클래스 카테고리를 보유하게 되었습니다.

235
00:31:10,830 --> 00:31:35,759
아마도 당시 AI 분야에서 만든 가장 큰 데이터셋 이었습니다.
ImageNet 덕분에 객체인식은 다른 국면으로 접어들었습니다.

236
00:31:35,759 --> 00:31:41,200
하지만 ImageNet을 Benchmark에 어떻게
활용하는지가 큰 화두였습니다.

237
00:31:41,200 --> 00:31:57,309
그래서 ImageNet 팀은 2009년부터 국제 규모의 대회를
주최했습니다. ILSVRC입니다. 이 대회를 위해서

238
00:31:57,309 --> 00:32:06,190
1000개의 객체에서 140만 개의 test set 이미지를 엄선했습니다.

239
00:32:06,190 --> 00:32:13,629
이 대회의 목적은 이미지 분류 문제를 푸는
알고리즘들을 테스트하기 위함이었습니다.

240
00:32:13,629 --> 00:32:42,909
여기 예제 이미지들이 있습니다. 참가자들은 정답 후보를 총 5가지
고를 수 있습니다. 5개 중에 정답이 있으면 맞춘 것이죠

241
00:32:42,909 --> 00:32:49,720
Image Classification Challenge의 결과입니다.
2010년도부터 2015년도까지의 결과입니다.

242
00:32:49,720 --> 00:33:00,740
x은 연도를, y축은 오류율을 입니다.

243
00:33:00,740 --> 00:33:06,820
좋은 소식으로는 오류율이 점차 감소하고 있습니다.

244
00:33:06,820 --> 00:33:15,369
2012년도의 오류율은 사람보다 낮습니다. 여기에서의 사람은

245
00:33:15,369 --> 00:33:32,470
Stanford의 한 PhD 학생입니다. 이 대회에 참가한 알고리즘처럼
테스트를 수행하며 몇 주를 보내야만 했습니다.

246
00:33:32,470 --> 00:33:43,110
비록 컴퓨터 비전이 아직 객체인식의 모든 문제를
풀지는 못했지만, 진전이 있었다는 것은 사실입니다.

247
00:33:43,110 --> 00:33:56,400
하지만 실생활에 적용하기에는 턱없이 부족했던 낮은 오류율이
인간의 수준으로 오기까지는

248
00:33:56,400 --> 00:34:05,640
불과 몇 년뿐이 걸리지 않았습니다. 그리고 여러분이 이 그래프에서
절대로 놓쳐서 안 되는 특별한 순간이 있습니다.

249
00:34:05,640 --> 00:34:25,649
바로 2012년입니다. 처음 2년 동안은 오류율이 약 25%를 맴돌았습니다.
2012년에는 오류율이 16%로 거의 10%가량 떨어졌고

250
00:34:25,650 --> 00:34:32,969
물론 현재의 오류율이 더 낮지만 2012년도의 감소는 아주 중요합니다.

251
00:34:32,969 --> 00:34:42,569
2012년도에 우승한 알고리즘은
convolutional neural network 모델입니다.

252
00:34:42,570 --> 00:34:49,850
CNN은 그 당시 다른 알고리즘들을 능가하고
ImageNet Challenge에서 우승하였습니다.

253
00:34:49,850 --> 00:34:58,200
우리가 한 학기 동안 배울 내용이 바로
Convolutional neural network에 관한 것입니다.

254
00:34:58,200 --> 00:35:10,370
CNN 모델이 무엇인지 심도 깊게 다룰 것입니다.
CNN을 Deep Learning이라고도 합니다.

255
00:35:10,520 --> 00:35:15,330
Deep Learning이 더 유명한 이름이겠군요

256
00:35:15,330 --> 00:35:26,400
앞으로 CNN이 무엇인지, 어떤 법칙이 있는지, 어떤 선례가 있는지,
이 모델의 최근 동향은 어떠한지를 살펴볼 것입니다. 하지만 이 역사의

257
00:35:26,400 --> 00:35:41,309
시작은 바로 2012년입니다. CNN, Deep learning 모델은
컴퓨터 비전 분야의 진보를 이뤄냄으로써 CNN의 우수성을

258
00:35:41,309 --> 00:35:51,900
입증하였습니다. 자연어 처리나 음성 인식과 같은 다른
관련 분야들도 더불어서 말이죠. 소개는 이쯤 해두고

259
00:35:51,900 --> 00:36:02,500
CS231n 수업 소개를 위해서 나머지 시간은
Justin에게 맡기도록 하겠습니다.

260
00:36:03,000 --> 00:36:08,158
Fei-Fei 교수님 감사합니다.
제가 여기서 이어받겠습니다.

261
00:36:08,189 --> 00:36:14,077
지금부터는 주제를 바꿔서 우리 수업과 관련된
이야기를 해볼까 합니다.

262
00:36:15,436 --> 00:36:22,950
이 수업에서 중점적으로 다룰 문제는
Image Classification입니다.

263
00:36:22,950 --> 00:36:27,037
앞서 ImageNet Challenge 이야기에서
살짝 들어보셨을 것입니다.

264
00:36:27,037 --> 00:36:31,470
Image Classification의 문제 정의를 해보자면
알고리즘이 이미지 한 장을 봅니다.

265
00:36:31,470 --> 00:36:36,443
몇 개의 고정된 카테고리 안에서 정답 하나를 고르는 거죠

266
00:36:36,443 --> 00:36:42,506
이 방법이 다소 한정적이거나 인위적으로 보일 수도
있지만 사실 매우 일반적입니다.

267
00:36:42,506 --> 00:36:49,630
이 문제는 다양한 환경에 적용될 수 있습니다. industry이던
academia이던 말이죠. 다양한 곳에서 적용 가능합니다.

268
00:36:49,630 --> 00:36:58,043
가령 음식, 음식의 칼로리, 미술작품들 등을 인식해야
하는 다양한 제품에 적용할 수 있습니다.

269
00:36:58,043 --> 00:37:08,503
따라서 image classification이라는 간단한 도구가
자체로도 유용할뿐더러 다양한 응용이 될 수도 있습니다.

270
00:37:08,503 --> 00:37:19,660
수업에서는 다양한 문제들을 다룰 것입니다. 하지만 이 문제들 모두
image classification 기반하에 일궈진 것들입니다.

271
00:37:19,660 --> 00:37:24,783
그리고 object detection과 image captioning도
배워 볼 것입니다.

272
00:37:24,783 --> 00:37:28,435
object detection 문제는
classification과 조금 다릅니다.

273
00:37:28,435 --> 00:37:40,351
이 이미지가 고양이다, 개다, 말이다 이렇게 하는 실제로
어디에 있는지 네모박스를 그릴 수 있어야 합니다.

274
00:37:40,351 --> 00:37:44,110
네모박스를 객체의 위치에 정확히 그려 넣어야 합니다.

275
00:37:44,110 --> 00:37:51,475
image captioning 도 배울 것입니다. 이미지가 입력으로 주어지면
이미지를 묘사하는 적절한 문장을 생성해야 합니다.

276
00:37:51,475 --> 00:37:55,599
이 문제가 어렵고 복잡해 보이고 Image classification과도
별로 관련이 없어 보일 수 있지만

277
00:37:55,599 --> 00:38:02,880
image classification 기술을 이런 문제들에서
충분히 재사용할 수 있습니다.

278
00:38:06,482 --> 00:38:14,398
지금까지는 ImageNet Challenge의 맥락에서 말씀드렸습니다만
최근 컴퓨터 비전 분야의 진보를 이끌어낸 주역은 바로

279
00:38:14,398 --> 00:38:20,350
Convolutional neural networks, 즉
CNN입니다. 또는 convnet으로도 불리죠

280
00:38:20,350 --> 00:38:26,827
지난 몇 해 간 ImageNet Challenge의 우승자들을 살펴봅시다.

281
00:38:26,827 --> 00:38:32,631
2011년에서 Lin et al의 알고리즘은 보시면
여전히 계층적(hierarchical)이죠

282
00:38:32,631 --> 00:38:41,211
여러 단계가 있습니다. 특징들을 뽑고, 지역 불변 특징들을 계산하고,
pooling을 거치고 이렇게 여러 단계를 거쳐서

283
00:38:41,211 --> 00:38:46,276
최종적인 특징 기술자를 Linear SVM에 태웁니다.

284
00:38:46,276 --> 00:38:52,583
핵심은 여전히 "계층적" 이라는 점입니다. edges를 뽑고
"불변 특징" 의 개념도 들어있습니다.

285
00:38:52,583 --> 00:38:56,177
그리고 대부분 이러한 직관들은
CNN에도 영향을 미칩니다.

286
00:38:56,177 --> 00:38:59,115
하지만 2012년 가장 획기적인 순간이었습니다.

287
00:38:59,115 --> 00:39:09,225
당시 Toronto에서 Jeff Hinton 교수님의 연구실의
PHD였던 Alex Krizhevsky와 Ilya Sutskever는

288
00:39:09,225 --> 00:39:12,504
7-Layer Convolutional neural network
을 만들었습니다.

289
00:39:12,504 --> 00:39:19,651
AlexNet 또는 Supervision으로도 알려져 있습니다.
AlexNet은 LSVRC'12 에서 아주 좋은 성과를 달성했습니다.

290
00:39:19,651 --> 00:39:24,197
이후 ImageNet의 우승 트로피는
매년 Neural Network의 몫이었습니다.

291
00:39:24,197 --> 00:39:28,096
그리고 이러한 추세로 CNN은 매년 더 깊어져 갔습니다.

292
00:39:28,096 --> 00:39:33,592
AlexNet은 7(8)-Layer Neural Network입니다.
Layer를 세는 방식에 따라 조금 다릅니다.

293
00:39:33,592 --> 00:39:43,172
2015년에 네트워크가 훨씬 더 깊어졌습니다. Google의
GoogleNet 그리고 Oxford의 VGG가 바로 그 주인공이죠.

294
00:39:43,172 --> 00:39:52,373
2015년에는 정말 대박입니다. MSRA의 Residual Network의
Layer 수는 152개에 육박합니다.

295
00:39:52,373 --> 00:39:58,505
이후 Layer 200개까지 쌓으면 성능이 더 좋아진다고는 하지만
아마도 여러분의 GPU 메모리가 감당할 수 없을 것입니다.

296
00:39:58,505 --> 00:40:00,352
나중에 더 다루기로 하죠

297
00:40:00,352 --> 00:40:13,479
오늘 수업에서 알고 가셔야 할 점은 2012년의 CNN의 시대가 도래했고,
이후 CNN을 개선하고 튜닝하려는 많은 시도들이 있었다는 것입니다.

298
00:40:13,479 --> 00:40:19,949
그리고 이 강의 전반에 걸쳐 CNN 모델들이 어떻게
동작하는지는 심도 깊게 살펴볼 것입니다.

299
00:40:22,514 --> 00:40:32,394
하지만 한 가지 명심하셔야 할 점은 CNN이 2012년
ImageNet Challenge에서 빛을 본 것은 사실이지만

300
00:40:32,394 --> 00:40:36,551
CNN이 2012년에 발명된 것은 아닙니다.

301
00:40:36,551 --> 00:40:40,310
사실 CNN은 아주 오래전부터 존재했습니다.

302
00:40:40,310 --> 00:40:53,633
CNN의 기초연구라고 한다면 90년도의 Jan LeCun과
Bell Labs와의 공동 과제를 말씀드릴 수 있습니다.

303
00:40:53,633 --> 00:40:58,829
1998년에 그들은 숫자인식을 위해 CNN을 구축했습니다.

304
00:40:58,829 --> 00:41:07,366
이들은 자필 수표 자동 판독과
우편주소 자동인식에 CNN을 적용하고 싶었습니다.

305
00:41:07,366 --> 00:41:17,237
그들은 이미지를 입력으로 받아서 숫자와 문자를
인식할 수 있는 CNN을 만들었습니다.

306
00:41:17,237 --> 00:41:23,618
CNN의 구조만 보자면 2012년의 AlexNet과 유사합니다.

307
00:41:23,618 --> 00:41:29,080
그림처럼, raw pixel을 입력으로 받아 여러
Convolution Layer Layer를 거치고 Sub-Sampling,

308
00:41:29,080 --> 00:41:31,398
Fully Connected Layer를 거치게 됩니다.

309
00:41:31,398 --> 00:41:34,714
이 모든 건 다음 강의 부터 더 자세히 다루도록 하겠습니다.

310
00:41:34,714 --> 00:41:38,397
하지만, 여러분이 이 두 그림을 보고 있자면
둘이 상당히 비슷해 보일 겁니다.

311
00:41:38,397 --> 00:41:48,420
2012년의 CNN 아키텍쳐들은 서로 비슷비슷했습니다.
90년대의 LeNet 아키텍처를 공유하기 때문입니다.

312
00:41:49,299 --> 00:41:53,377
그럼 이런 질문을 할 수 있겠군요
90년대부터 알고리즘이 있었다면

313
00:41:53,377 --> 00:41:57,454
왜 최근에야 갑자기 유명해진 것일까요?

314
00:41:57,454 --> 00:42:03,277
90년대 이래로 아주 큰 혁신들이 있었습니다.

315
00:42:03,277 --> 00:42:09,217
하나는 바로 계산능력입니다. 무어의 법칙 덕분에
컴퓨터의 계산속도가 매년 빨라졌습니다.

316
00:42:09,217 --> 00:42:18,574
완벽한 척도는 아니지만, CPU의 트랜지스터 개수만 세어봐도
90년대보다 몇십 배 이상 발전했음을 알 수 있죠

317
00:42:18,574 --> 00:42:25,878
또한 graphics processing units의 진보도 한몫했습니다.
GPU는 아주 강력한 병렬처리가 가능한데

318
00:42:25,878 --> 00:42:33,032
계산 집약적인 CNN 모델을 고속으로
처리하는 데 안성맞춤입니다.

319
00:42:33,032 --> 00:42:42,150
단지 더 많은 계산이 가능하다는 것만으로도
연구자들이 더 큰 아키텍쳐를 연구해 볼 수 있었고

320
00:42:42,150 --> 00:42:48,476
경우 따라서는 기존의 고전 알고리즘들의 크기만 키웠음에도
훨씬 더 잘 동작하는 경우도 많았습니다.

321
00:42:48,476 --> 00:42:55,554
연산량의 증가는 딥러닝의 역사에서 아주 중요한 요소입니다.

322
00:42:55,554 --> 00:43:00,559
90년대와 지금은 데이터의 차이도 있었습니다.

323
00:43:00,559 --> 00:43:09,395
CNN 알고리즘이 잘 동작하려면 아주 많은
레이블이 매겨진 이미지가 필요합니다.

324
00:43:09,395 --> 00:43:23,614
90년대에는 레이블이 매겨진 이미지 데이터를 구하기가 쉽지 않았습니다.
아주 크고 다양한 데이터셋을 수집하기가 힘든 시기였습니다.

325
00:43:23,614 --> 00:43:33,176
오늘날은 PASCAL이나 ImageNet 같은 규모가 크고
잘 분류된 레이블들을 가진 데이터셋이 많습니다.

326
00:43:34,228 --> 00:43:38,775
90년대에와 비교하면 사용 가능한 데이터셋이 훨씬 많습니다.

327
00:43:38,775 --> 00:43:43,153
큰 데이터셋들을 잘 활용하면
Higher Capacity Model을 만들 수 있습니다.

328
00:43:43,153 --> 00:43:47,157
그렇게 학습시킨 모델들은 실생활 문제에서도 잘 동작했습니다.

329
00:43:47,157 --> 00:43:56,117
하지만 가장 중요한 것은 CNN이 엄청 좋아 보이고 새로워 보이고
몇 해 전에 갑자기 툭 하고 튀어나온 것처럼 보이지만

330
00:43:56,117 --> 00:43:57,527
그렇지 않다는 것입니다.

331
00:43:57,527 --> 00:44:03,666
CNN스러운 알고리즘들은 이미 아주 오래전부터 있었습니다.

332
00:44:05,015 --> 00:44:12,755
그리고 또 한 가지 중요한 점은 컴퓨터 비전 연구의 목적은
"사람처럼 볼 수 있는" 기계를 만드는 것입니다.

333
00:44:12,755 --> 00:44:16,650
사람들은 시각 체계를 통해 아주 많은 것들을 할 수 있습니다.

334
00:44:16,650 --> 00:44:24,988
여러분은 고양이나 강아지를 찾아서 사각형을
그리는 것 이상의 일들을 할 수 있습니다.

335
00:44:24,988 --> 00:44:27,711
여러분의 시각체계는 컴퓨터 비전보다 훨씬 더 강력합니다.

336
00:44:27,711 --> 00:44:34,047
컴퓨터 비전 분야 이야기를 해드리자면 아직도 우리가 풀어야 할
수많은 도전과제와 미해결 문제가 있습니다.

337
00:44:34,047 --> 00:44:40,220
우리는 더 나은 일을 하고, 더 야심 찬 문제에 도전할 수 있도록
알고리즘을 계속해서 연구해야 합니다.

338
00:44:40,220 --> 00:44:44,043
아직 풀지 못한 문제들의 예를 한번 살펴보겠습니다.
사실 예전부터 연구가 활발히 진행됐습니다.

339
00:44:44,043 --> 00:44:46,923
Semantic Segmentation 즉
Perceptual Grouping 같은 문제들이죠

340
00:44:46,923 --> 00:44:53,866
이미지 전체를 레이블링하는 것 대신
모든 픽셀 하나하나를 이해하는 것입니다.

341
00:44:53,866 --> 00:44:56,846
Semantic Segmentation은 다음에 다시 다루도록 하죠

342
00:44:56,846 --> 00:45:06,127
3D understanding은 실세계를 재구성하는 문제입니다.
제 생각에는 여전히 완벽하게 풀지는 못한 문제이죠

343
00:45:07,498 --> 00:45:10,178
여러분도 엄청나게 많은 것들을 상상해 볼 수 있습니다.

344
00:45:10,178 --> 00:45:11,817
행동 인식의 예를 들어볼까요

345
00:45:11,817 --> 00:45:19,469
가령 어떤 사람이 비디오에서 무언가를 하고 있을 때, 그 행동을 인식할
수 있는 가장 좋은 방법은 무엇일까요? 상당히 도전적인 문제입니다.

346
00:45:19,469 --> 00:45:27,578
그리고 증강현실, 가상현실, 또는 새로운 센서 등을 마주하게 되면

347
00:45:27,578 --> 00:45:32,455
그 자체를 한 분야로 다뤄도 될 만큼 아주 새롭고 흥미롭고
도전적인 문제들을 만나게 될 것입니다.

348
00:45:33,916 --> 00:45:42,228
지금부터 보여 드릴 것은 제가 연구실에서 진행 중인
프로젝트의 일부인데 Visual Genome이라는 데이터셋입니다.

349
00:45:42,228 --> 00:45:47,474
이 프로젝트에서는 실제 세상에서 복잡한 것들을 일부 
포착해 내려고 시도하고 있습니다.

350
00:45:47,474 --> 00:45:57,525
이미지에 박스만 치는 게 아니라 커다란 의미론적 그래프로 
표현하는 것이죠. 이 그래프는 객체를 식별하는 것을 넘어

351
00:45:57,525 --> 00:46:02,590
그 장면에서의 객체 간의 관계, 객체의 성격, 행동 등을 나타낼 수 있습니다.

352
00:46:02,590 --> 00:46:09,527
그리고 이런 방식을 이용한다면 실제 세상을
일부는 포착할 수 있지 않을까 예상합니다.

353
00:46:09,527 --> 00:46:12,889
이런 것들은 우리가 단순하게 Classification만 할 때
활용하지 못했던 것들입니다.

354
00:46:12,889 --> 00:46:15,270
이 프로젝트는 현시점으로서는 표준 접근방식은 아니지만

355
00:46:15,270 --> 00:46:24,840
Image Classification만으로는 포착해 낼 수 없는 훨씬 더 
다양한 일들이 있다는 사실을 알려 드리고 싶었습니다.

356
00:46:28,003 --> 00:46:31,592
그런 관점에서, 정말 재밌는 연구가 하나 있습니다.

357
00:46:31,592 --> 00:46:38,952
Fei-Fei 교수님의 Cal Tech에서의 박사과정 시절의 연구입니다.

358
00:46:38,952 --> 00:46:44,604
연구를 소개해 드리자면 임의의 사람들을 붙잡아서 
이런 사진을 아주 잠시 동안만 보여줬습니다.

359
00:46:44,604 --> 00:46:47,896
사람들에게 아주 짧은 시간 동안만 이미지를 보여준 것입니다.

360
00:46:47,896 --> 00:46:56,473
그런데 사람들은 이미지를 아주 잠깐만 봤음에도 이 같은 
아주 긴 문장을 작성할 수 있었습니다.

361
00:46:56,473 --> 00:47:05,560
주목할만한 결과였습니다. 인간은 이미지를 짧은 시간만
보더라도 이렇게 묘사할 수 있었습니다.

362
00:47:05,560 --> 00:47:10,375
"사람들이 어떤 놀이 또는 싸움을 하고 있고, 두 명씩 짝지어져 있고,
왼쪽 사람은 무언가를 던지고 있고 -

363
00:47:10,375 --> 00:47:14,576
잔디밭인 것 같은 느낌이 드니까 밖인 것 같고.." 등등

364
00:47:14,576 --> 00:47:17,617
사람들이 이미지를 조금만 더 오래 볼 수만 있었다면
어땠을지 상상이 가실 것입니다.

365
00:47:17,617 --> 00:47:22,307
이들이 누구이고 왜 저곳에서 게임을 하는지에 대해서
소설을 한 편 쓸 수 있을지 모르겠습니다.

366
00:47:22,307 --> 00:47:27,187
외부 지식과 경험이 가미된다면 아마 끝도 없을 것입니다.

367
00:47:27,187 --> 00:47:34,663
이미지의 내용을 아주 풍부하고 깊게 이해하는 것은 
이는 컴퓨터 비전 분야가 진정으로 추구하는 방향입니다.

368
00:47:34,663 --> 00:47:44,460
제 생각에는 컴퓨터 비전이라는 분야에는 많은 진보가 
있었지만, 아직 가야 할 길은 멀고도 험난합니다.

369
00:47:44,460 --> 00:47:52,890
다른 예를 하나 더 들어보겠습니다. Andrej Karpathy의
블로그에서 가져온 이미지입니다. 아주 재미있는 이미지입니다.

370
00:47:52,890 --> 00:47:57,696
많은 사람들의 웃음을 자아냈습니다. 제가 보기에도 상당히 
재미있는 이미지입니다. 왜 이 이미지가 웃기죠?

371
00:47:57,696 --> 00:48:04,380
한 남자가 체중계에 서 있습니다. 뭐 보통 사람들이 
체중을 재려고 체중계를 사용하곤 합니다.

372
00:48:04,380 --> 00:48:10,900
그런데 어떤 사람이 뒤에서 몰래 체중계를 밟고 있군요.
우리는 체중계가 어떻게 동작할지 짐작할 수 있습니다.

373
00:48:10,900 --> 00:48:13,867
저 남자는 자신의 "부풀려진" 체중을 보게 될 것이란 것도 알 수 있습니다.

374
00:48:13,867 --> 00:48:16,819
하지만 더 많은 정보가 있습니다. 우리는 저 사람이 
평범한 사람이 아니란 것을 알고 있습니다.

375
00:48:16,819 --> 00:48:24,741
저 사람은 당시의 미국 대통령 Barack Obama입니다. 우리는 
미국의 대통령이라면 존경받는 정치인이어야 한다고 생각합니다.

376
00:48:24,741 --> 00:48:27,045
[웃음]

377
00:48:27,045 --> 00:48:31,304
적어도 동료에게 이런 식의 장난을 치지 말아야 하겠죠

378
00:48:31,304 --> 00:48:34,564
그런데 뒤쪽의 사람들이 이 장면을 보고 웃고 있군요

379
00:48:34,564 --> 00:48:37,912
이를 미루어 우리는 사람들이 이 장면을 어떻게 
받아드리는지 이해할 수 있습니다.

380
00:48:37,912 --> 00:48:42,866
그들도 우리와 같은 생각을 한다는 것을 알 수 있죠

381
00:48:42,866 --> 00:48:45,830
이건 정말 놀라운 것입니다.
이 이미지 한 장에 정말 많은 것들이 있습니다.

382
00:48:45,830 --> 00:48:53,002
컴퓨터 비전 알고리즘이 이런 진정한 깊은 이해를 
하기까지는 아직 갈 길이 멀다고 생각합니다.

383
00:48:53,002 --> 00:49:01,385
이 분야가 큰 진보를 이루긴 했지만 갈 길은 한참 남았습니다.
연구자로서 저에게는 아주 짜릿한 일입니다.

384
00:49:01,385 --> 00:49:06,694
앞으로 더 진보할 수 있는 흥미진진하고 재미있는 문제들이 
우리를 기다리고 있기 때문입니다.

385
00:49:07,913 --> 00:49:13,054
그래서 저는 컴퓨터 비전이 정말 재미있는 분야라는 
것을 여러분이 아셨으면 좋겠습니다.

386
00:49:13,054 --> 00:49:20,043
정말 재밌습니다. 그리고 매우 유용합니다. 
아주 다양한 방법으로 이 세상에 기여할 수 있습니다.

387
00:49:20,043 --> 00:49:28,134
컴퓨터 비전은 의학 진단, 자율주행, 로보틱스 등
어디든 적용할 수 있습니다.

388
00:49:28,134 --> 00:49:33,120
그리고 인간의 지능을 이해하기 위한 여러 핵심 아이디어들을
집대성하는 일종의 실마리가 될지도 모릅니다.

389
00:49:33,120 --> 00:49:37,141
제 생각에는 컴퓨터 비전은 정말 기상천외하고 재밌는 분야입니다.

390
00:49:37,141 --> 00:49:46,234
저는 여러분과 이 수업을 빌어 그런 알고리즘들이 실제로 어떻게 
동작하는지를 심도 깊게 다룰 수 있어서 정말 좋습니다.

391
00:49:46,234 --> 00:49:50,673
여기까지는 컴퓨터 비전의 역사에 대한 
저의 개인적인 견해였습니다.

392
00:49:50,673 --> 00:49:57,055
혹시 궁금한 점이 있으신가요?

393
00:49:57,055 --> 00:50:02,408
그러면 이제는 앞으로의 수업 방향에 대해서 말씀드리겠습니다.

394
00:50:02,408 --> 00:50:06,904
우선 교수진에 대해서 말씀드리겠습니다.
이 수업은 Fei-Fei Li 교수님께서 가르치십니다.

395
00:50:06,904 --> 00:50:11,271
이곳 Stanford의 컴퓨터 과학 교수님이시죠

396
00:50:11,271 --> 00:50:16,852
그리고 제 지도 교수님이시기도 하고
Stanford Vision Lab의 교수님이십니다.

397
00:50:16,852 --> 00:50:22,519
그리고 그 옆에 두 명은 저 Justin Tohnson과
Serena Yeung입니다. 그녀는 여기 앞에 서 있습니다.

398
00:50:22,519 --> 00:50:27,379
우리 둘은 Fei-Fei 교수님의 지도하에
다양한 컴퓨터 비전 과제를 수행하고 있는 PHD 학생입니다.

399
00:50:27,379 --> 00:50:31,920
우리에게 18명의 훌륭한 조교가 있습니다.

400
00:50:31,920 --> 00:50:34,179
대부분은 여기 앞에 앉아 있습니다.

401
00:50:34,179 --> 00:50:40,320
우리가 수업을 잘 진행하고, 모든 것이 잘 돌아갈 수 있도록
도와주는 얼굴 없는 영웅들입니다.

402
00:50:40,320 --> 00:50:42,365
그러니 그들에게 잘해주세요.

403
00:50:42,365 --> 00:50:44,196
[웃음]

404
00:50:44,196 --> 00:50:47,153
말씀드려야 할 점이 있다면, 이 강의는 세 번째 열리는 강의이지만

405
00:50:47,153 --> 00:50:53,050
Andrej Karpathy가 강의를 하지 않는 첫 번째 강의이기도 합니다.

406
00:50:53,050 --> 00:50:58,353
그는 저와 아주 친한 친구입니다. 그 친구는 아직 
안 죽었습니다. 괜찮습니다. 걱정 마세요.

407
00:50:58,353 --> 00:50:59,612
[웃음]

408
00:50:59,612 --> 00:51:11,617
이 수업의 발전과 역사의 대부분은 수년 동안 저와 함께 
일해준 그의 덕분이었습니다.

409
00:51:11,617 --> 00:51:15,398
그 사실은 여러분도 알고 계셨으면 좋겠습니다.

410
00:51:15,398 --> 00:51:22,209
강의에 대해 말해보자면, 조교들과 연락할 수 있는 가장
좋은 방법은 Piazza를 이용하는 것입니다.

411
00:51:22,209 --> 00:51:25,212
당장 가서 가입하세요

412
00:51:25,212 --> 00:51:30,353
Piazza는 이 수업에 대해 의사소통을 할 수 있는
우리가 가장 선호하는 방법입니다.

413
00:51:30,353 --> 00:51:34,313
만약 친구들 앞에서 질문하는 것이 꺼려진다면,

414
00:51:34,313 --> 00:51:40,572
Piazza에 가서 익명으로 질문하세요. private
질문을 올릴 수도 있습니다. 교직원에게 직접 연락하십시오.

415
00:51:40,572 --> 00:51:44,452
기본적으로 여러분이 필요한 것은 그저 Piazza를 경험하는 것입니다.

416
00:51:44,452 --> 00:51:46,445
조교들의 메일링 리스트가 있긴 하지만

417
00:51:46,445 --> 00:51:53,517
Piazza에 올리기 껄끄러운 개인적이거나 비공개적인 것들을 위해서만 쓰고

418
00:51:53,517 --> 00:52:02,125
혹시나 정말 중요한 말 못 할 사항이 있다면, 저나, Serena, Fei-Fei
교수님께 직접 메일을 주셔도 되겠습니다. 이메일로 보내주십시오.

419
00:52:02,125 --> 00:52:06,096
하지만 그 외의 조교와의 커뮤니케이션은 Piazza를
통해서야만 합니다.

420
00:52:06,096 --> 00:52:14,372
올해는 부교재도 있습니다. 필수는 아닙니다. 이 책 없이도 이
강의를 들을 수 있습니다. 이 교재는 스스로 구하셔야 합니다.

421
00:52:14,372 --> 00:52:19,786
개인적으로 아주 기분이 좋습니다. 이 책이 아마도 가장 
처음 출간된 딥러닝 교과서입니다.

422
00:52:19,786 --> 00:52:24,078
올해 초에 출간되었고 E.N. Goodfellow, 
Yoshua Bengio, Aaron Courville이 저자입니다.

423
00:52:24,078 --> 00:52:28,197
슬라이드에 아마존 링크를 추가했습니다.
원한다면 구입하도록 하세요

424
00:52:28,197 --> 00:52:32,943
하지만 온라인에 무료 컨텐츠도 있기 때문에
굳이 살 필요는 없습니다.

425
00:52:32,943 --> 00:52:34,261
다시 한번 말하지만, 이 교재는 완전히 선택 사항입니다.

426
00:52:34,261 --> 00:52:40,614
하지만 아마도 학기 동안 이 책의 일부를 읽으라고 공지할 수도 있습니다.
이 책은 여러분이 추가적인 관점을 얻는 데 많은 도움을 줄 것입니다.

427
00:52:41,697 --> 00:52:48,794
이 수업의 철학은 여러분이 딥러닝에 관한 모든 알고리즘을 
제대로 이해해야 한다는 것입니다.

428
00:52:48,794 --> 00:52:56,097
아주 깊은 수준에서 이해해야 합니다. 어떤 Neural network 모델을
사용했을 때 정확히 어떻게 작동하는지 정확하게 이해해야 합니다.

429
00:52:56,097 --> 00:53:02,314
그리고 그런 아키텍쳐를 선택하면 어떤 영향을 미치는지, 
네트워크가 어떻게 학습되고 테스팅 되는지와 같은 것들을 알아야 합니다.

430
00:53:02,314 --> 00:53:08,757
그리고 여러분은 나만의 CNN을 Python으로 밑바닥부터 구현해야 합니다.

431
00:53:08,757 --> 00:53:16,320
여러분이 전체 foward/backward passes을 직접 구현하다 보면
결국 CNN모델을 완벽히 구현하게 될 것입니다.

432
00:53:16,320 --> 00:53:18,320
저는 이것이 정말 좋다고 생각합니다.

433
00:53:18,320 --> 00:53:25,613
하지만 실제로는 대부분의 사람들이 CNN을 
밑바닥부터 구현하지는 않을 것입니다.

434
00:53:25,613 --> 00:53:31,326
그렇기 때문에 여러분에게도 아주 실용적인 최신 
소프트웨어 도구들 역시 소개해 드릴 것입니다.

435
00:53:31,326 --> 00:53:37,663
Tensor Flow, Torch, PyTorch 와 같은 SOTA 
소프트웨어들도 다룰 것입니다.

436
00:53:37,663 --> 00:53:44,528
아마도 여러분은 그런 툴들을 강의 과제나 프로젝트를
진행하면서 접하실 수 있을 것입니다.

437
00:53:44,528 --> 00:53:49,122
또 한 가지 말씀드릴 점은 이 강의가 아-주 SOTA 하는 것입니다. 
저는 이점이 가장 마음에 듭니다.

438
00:53:49,122 --> 00:53:50,715
이 분야는 아주 빠르게 변합니다.

439
00:53:50,715 --> 00:54:03,749
ImageNet 그래프에도 보았듯이 2012년 이후로 엄청나게 
변하고 있습니다. 제가 여기 있는 동안 일 년 내내 변하고 있죠

440
00:54:03,749 --> 00:54:12,893
어쩌면 우리가 지난해에 다뤘던 수업 내용이 
올해 들어서는 순식간에 사라질 수도 있습니다.

441
00:54:12,893 --> 00:54:16,629
제가 이 과목을 가르칠 때 가장 좋아하는 대목이지요

442
00:54:16,629 --> 00:54:24,041
이 분야는 과학적으로 새롭게 발견된 것들을 모조리 흡수할 수 있고
또 제가 여러분에게 그것들을 알려줄 수 있다는 점이 정말 좋습니다.

443
00:54:24,041 --> 00:54:26,071
재밌는 것들도 있습니다.

444
00:54:26,071 --> 00:54:30,453
심각하지 않은 재미있는 주제에 관해서도 다룰 것입니다.

445
00:54:30,453 --> 00:54:33,122
이미지 캡셔닝과 같은 것인데 매우 재밌습니다.

446
00:54:33,122 --> 00:54:35,349
이미지 캡셔닝은 이미지의 내용을 기술하는 것입니다.

447
00:54:35,349 --> 00:54:39,896
그리고 여기 왼쪽에 보이는 DeepDream과 같은
좀 더 예술적인 것들에 관해서는 다룰 것입니다.

448
00:54:39,896 --> 00:54:44,277
이것은 우리가 Neural Network를 통해
이런 사이키델릭한 이미지를 만들게도 해줍니다.

449
00:54:44,277 --> 00:54:46,877
코스가 끝날쯤이면 어떻게 동작하는지 알 수 있을 겁니다.

450
00:54:46,877 --> 00:54:55,340
오른쪽 그림은 style transfer입니다. 이미지가 주어지면
이미지를 피카소나 반 고흐와 같은 유명화가의 풍으로 바꿔줍니다.

451
00:54:55,340 --> 00:54:59,654
이 또한 코스가 끝날쯤이면
어떻게 동작하는지 알게 되실 것입니다.

452
00:54:59,654 --> 00:55:03,794
학기 동안 세 가지 과제가 있습니다.

453
00:55:03,794 --> 00:55:10,706
첫 번째 과제는 잘하면 일주일 내로 끝낼 수도 있습니다.
그리고 중간고사가 있습니다.

454
00:55:10,706 --> 00:55:17,407
그리고 여러분의 학점에서 가장 큰 비율을 차지하는 것이 바로
최종 코스 프로젝트인데, 3인 1조로 진행할 것이며,

455
00:55:17,407 --> 00:55:20,514
여러분은 모든 사람들의 마음을 날려버릴 만큼
놀라운 프로젝트를 만드시면 됩니다.

456
00:55:20,514 --> 00:55:26,380
제출기한 정책이 있습니다. 도합 7일 정도는 늦을 수 있고 
여러분들의 과제 수행 중 자유롭게 분배할 수 있습니다.

457
00:55:26,380 --> 00:55:34,204
몸이 조금 아프거나, 여행을 가거나, 컨퍼런스에 참가하거나 할 때
유용하게 사용하시면 됩니다.

458
00:55:34,204 --> 00:55:40,880
하지만, 갑자기 학기 말에 와서는 "이번에 컨퍼런스에서
발표를 해야만 해요" 하고 하셔도 소용없습니다

459
00:55:40,880 --> 00:55:42,624
late days를 잘 활용하시기 바랍니다.

460
00:55:42,624 --> 00:55:50,295
하지만 만일 정상참작 할만한 피치 못할 사유가 있다면 
조교들에게 이메일을 보내주시길 바랍니다.

461
00:55:50,295 --> 00:55:54,177
또 한가지 알려 드릴 것은
collaboration policy입니다.

462
00:55:54,177 --> 00:56:00,785
여러분은 Stanford 학생으로서 반드시 윤리규정
(honer code)을 명심해야만 합니다. 아주 중요합니다.

463
00:56:00,785 --> 00:56:03,609
아주 엄격하게 다룰 것입니다.

464
00:56:03,609 --> 00:56:11,037
반드시 윤리규정의 테두리 안에서 협력할 수 있도록
신중하게 행동해 주시기 바랍니다.

465
00:56:12,304 --> 00:56:17,492
Pre-requisites에 대해 말씀드리겠습니다.
Python이 가장 중요할 것 같네요

466
00:56:17,492 --> 00:56:22,339
모든 프로그래밍 과제가 Python으로 진행됩니다.

467
00:56:22,339 --> 00:56:26,066
C나 C++에 익숙해지는 것도
어느 정도는 유용할 것입니다.

468
00:56:26,066 --> 00:56:31,705
이번 코스에서 C나 C++코드를 작성할 일은 
없겠지만 과제를 하다 보면

469
00:56:31,705 --> 00:56:39,879
여러 소프트웨어 패키지의 코드를 살펴볼 것이고, C++ 코드를 알면
패키지들이 어떻게 동작하는지 이해하는데 유용할 것입니다.

470
00:56:39,879 --> 00:56:44,971
그리고 여러분이 미분을 안다고 가정하고 수업을 진행할 것입니다.

471
00:56:44,971 --> 00:56:46,533
또한 일부 선형대수도 안다고 가정할 것입니다.

472
00:56:46,533 --> 00:56:52,072
행렬이 무엇이고, 어떻게 곱하는지 등을 
미리 알아두셔야 합니다.

473
00:56:52,072 --> 00:56:55,691
여기에서 도함수 계산 등을 전부 다 가르칠 순 없습니다.

474
00:56:55,691 --> 00:57:01,238
또한 CS131 또는 CS231a 수준의 컴퓨터 비전 지식이
있다고 가정하고 수업을 진행할 것입니다.

475
00:57:02,367 --> 00:57:05,120
이 과목들을 수강한 적이 있는 분들은 수월할 것입니다.

476
00:57:05,120 --> 00:57:11,550
그렇지 않아도 수업을 듣는 데 큰 지장은 없겠지만 따라잡으려면 좀 
더 노력해야 할 것입니다. 그래도 아마 큰 문제는 없으리라 봅니다.

477
00:57:11,550 --> 00:57:13,704
완전 완전 필수인 prerequisites은 없습니다.

478
00:57:13,704 --> 00:57:20,540
또한 CS229 수준의 기계학습 배경 지식이 있다고 가정합니다.

479
00:57:20,540 --> 00:57:32,416
하지만 기계학습 개념 중에서 정말 중요하다 싶은 것들을 필요할 때 
마다 제가 다시 설명해 드릴 것입니다. 미리 익숙하면 좋겠죠

480
00:57:34,774 --> 00:57:36,950
강의 홈페이지가 있습니다. 가서 확인해 보세요

481
00:57:36,950 --> 00:57:39,742
많은 정보와 링크 강의계획 등 많은 자료가 있습니다.

482
00:57:39,742 --> 00:57:43,656
오늘 다룰 것들은 거의 다 끝낸 것 같군요

483
00:57:43,656 --> 00:57:48,733
그리고 이번 주 목요일부터는 본격적으로
수업을 진행하도록 하겠습니다.

